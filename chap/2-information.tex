\chapter{Information-theoretic model selection}
\label{cha:Information-theoretic model selection}

The information-theoretic approach to model selection from a
decision-theoretic perspective uses the divergence of the chosen model from
the true model as a measure of the loss when using an approximated model in
place of the true data generating mechanism. The Akaike's information
criterion (\aic) is perhaps the most important and widely used model selection
method among these information-theoretic approaches. Its foundation is built
upon the \kl and its relation to the maximum likelihood estimator (\mle).
These are introduced in section~\ref{sec:kl and maximum likelihood}. It is
followed by a discussion of the \aic model selection method.

\subsection{\kl and maximum likelihood}
\label{sub:kl and maximum likelihood}

\kl measures the ``distance'' or ``divergence'' between statistical
populations defined by their underlying probability densities. The divergence
is measured in terms of the information defined by the log-likelihood ratio
\parencite{Kullback:1951va}. The common definition of \kl for continuous
distributions in literature is,
\begin{equation}
  \dkl(g, f(\cdot;\bth)) = \int g(x)\log\frac{g(x)}{f(x;\bth)}\dd x,
  \label{eq:kld}
\end{equation}
where $g$ is the true density of the data generating mechanism, and
$f(\cdot;\bth)$ is the density of a candidate model specified by parameter
vector $\bth$. The integration above is over the support of $g$.

\subsubsection{Properties of \kl and its relation to \protect\mle}
\label{ssub:Properties of kl and its relation to mle}

One important observation is that $\dkl(g,f(\cdot;\bth))\ge0$ with equality if
and only if $g(x) = f(x;\tbth)$ almost everywhere for some value of $\bth =
\tbth$ \parencite{Kullback:1951va}. In terms of equation~\eqref{eq:kld}, this
means that the \kl is always positive unless for some value of $\bth$,
$f(\cdot;\bth) = g(\cdot)$, which is the true data generating density. This
result naturally suggests one to find the estimator $\tbth$ which minimizes
the \kl. Under suitable conditions, rewrite equation~\eqref{eq:kld},
\begin{align}
  \dkl(g,f(\cdot;\bth))
  & = \int g(x)\log g(x)\dd x - \int g(x)\log f(x;\bth)\dd x \notag\\
  & = \int g(x)\log g(x)\dd x - \Exp_g \log f(X;\bth),
  \label{eq:kl exp ll}
\end{align}
where $\Exp_g\log f(X;\bth)$ is the expected log-likelihood with respect to
$g(X)$. Therefore $\tbth$ will maximize the expected log-likelihood. Letting
$\ell_n(\bth;\bfx)$ denote the log-likelihood of an i.i.d. sample $\bfx =
(x_1,\dots,x_n)^T$ from density $g$, evaluated at $\bth$, then by the Strong
Law of Large Numbers,
\begin{equation}
  \frac{\ell_n(\bth;\bfx)}{n}
  \overset{\mathrm{a.s.}}{\longrightarrow}
  \Exp_g\log f(X;\bth), \quad\text{for } n\to\infty\text{ and fixed }\bth,
  \label{eq:kl consistency}
\end{equation}
provided that this expectation exits. Hence \parencite[see][for
details]{Kullback:1951va},
\begin{equation}
  \hbth \overset{\mathrm{a.s}}{\longrightarrow} \tbth,
  \quad\text{for } n\to\infty.
\end{equation}
where $\hbth$ is the \mle. That is, the \mle aims to minimize the \kl from the
candidate model to the true data generating density. This also justifies the
use of \mle from a information-theoretic perspective.

\subsection{Akaike's information criterion}
\label{sub:Akaike's information criterion}

The general formula of the Akaike's information criterion (\aic) for a
candidate parametric model $\calM$ is
\begin{equation}
  \text{\aic} = -2\ell_n(\hbth;\bfx) + 2k
\end{equation}
where $\ell_n(\bth;\bfx)$ is the log-likelihood for a size $n$ sample
$\bfx=(x_1,\dots,x_n)^T$, evaluated at $\bth$. The value $\hbth = \hbth(\bfx)$
is the maximum likelihood estimator (\mle) under model $\calM$.\footnote{More
  precisely it is a class of models whose densities belong to the same
  parametric family.} And $k$ is the dimension of the parameter vector. The
model with the smallest \aic value is selected. It is first introduced by
Akaike in a series of papers \parencite{Akaike:1973uc, Akaike:1974ih,
  Akaike:1977ul}. From a likelihood principle perspective, \aic acts as a
balance between good fit (high log-likelihood) and complexity in the sense
that the dimension of the parameter vector is used as a penalty term in the
formula.  The \aic method aims to find models that have fewer parameters while
fitting the data well. There are other kind of penalized criteria for choosing
models.  And many of them are closely related to the \aic method.


There are precise mathematical reasons behind the \aic's way of penalizing
complexity and measuring the fit of data. These are closely related to the
behavior of the \mle and its relation to the \textcite{Kullback:1951va}'s
measure of the statistical divergence from one probability density to another.
In the next subsection the derivation of \aic is outlined. This shall provide
us a basis for the discussion of the suitability of the \aic and related
methods for model selection.

\subsubsection{Derivation of \protect\aic}
\label{ssub:Derivation of aic}

The \aic method and its formula has been derived and justified by various
authors \parencite[e.g.][]{Stone:1982ck, Sawa:1978tn, Chow:1981te,
  Bozdogan:1987wy, Akaike:1973uc}. To summarize, we start with the same
information-theoretic approach as in \textcite{Akaike:1973uc}, and use Taylor
expansions to approximate the expected loss of using a parametric density with
\mle as the parameter value in place of the true data generating density. In
addition we shall only consider the case in which the sample size $n$ is
sufficient large in the sense that $n\gg k$, where $k$ is the dimension of the
estimated parameter vector. The case in which $n$ is small is considered by
\textcite{Hurvich:1989ev}, and will be reviewed later in
subsection~\ref{sub:Corrected aic}.

As pointed out by \textcite{Bozdogan:1987wy}, though the development of \aic had
its origin in the time series modeling, it is a direct extension to the
information-theoretic interpretation of the \mle (see
subsection~\ref{sub:Properties of kl and its relation to mle}). For a class of
parametric models with density $f(\cdot;\bth)$, we aim to find the ``best'' in
the sense that the \kl is minimized among all possible values of $\bth$, i.e.,
to find the value of $\tbth$, which maximizes the expected log-likelihood.
From a decision-theoretic point of view this model also minimizes the expected
loss measured by the \kl \parencite{Akaike:1973uc}. Ideally, if we can find
$\tbth$, then the next step will be choosing the model with the minimum of
$D_{\mathrm{KL}}(g,f(\cdot;\tbth))$. However, even in the unlikely case that
$g(x) = f(x;\tbth)$, we still have to estimate $\tbth$. As suggested in the
last section, given i.i.d sample $\bfx = (x_1,\dots,x_n)^T$, we shall use the
\mle ($\hbth$) as a consistent estimator for $\tbth$. The \aic method is in
essence used to estimate the expected \kl from $f(\cdot; \hbth)$ to $g$.
Recall equation~\eqref{eq:kl exp ll}, and the additive property of \kl and
log-likelihood, the \aic method aim to estimate the following quantity,
\begin{equation}
  \Exp_{\bfY}\Exp_{\bfX}\log f(\bfX;\hbth(\bfY)),
\end{equation}
where $\bfX = (X_1,\dots,X_n)^T$, $X_i \sim g$, and $\bfY =
(Y_1,\dots,Y_n)^T$, $Y_i \sim g$ is a simple random sample. We emphasize that
$\hbth$, the \mle, is a function of the data without involving $\bfX$.
Therefore, the outer expectation can also be viewed as taken with respect to
$\hbth$. Let $\ell_n(\bth;\bfy)$ be the log-likelihood computed with data
$\bfy = (y_1,\dots,y_n)$. Let $\tbth$ be the value of $\bth$ maximize
$\Exp_{\bfX}\log f(\bfX;\bth)$. Then the key result is that,
\begin{equation}
  \Exp_{\bfY}\Exp_{\bfX}\log f(\bfX;\hbth(\bfY))
  \approx \Exp_{\bfX}\log f(\bfX;\hbth) - \tr(I(\tbth)\Sigma),
  \label{eq:aic general}
\end{equation}
where
\begin{align}
  I(\tbth)
  & = \Exp_{\bfX}\Square[Big]{
    -\frac{\partial^2\log f(\bfX;\bth)}{\partial\bth\partial\bth^T}}
  \Bigm|_{\bth=\tbth} \\
  \Sigma &= \Exp_{\hbth}[(\hbth-\tbth)(\hbth-\tbth)^T].
\end{align}

That is, $\Sigma$ is the theoretical covariance matrix of $\hbth(\bfY)$ when
$\bfY$ follows the true data generating density. This is a more general
results than that found in \textcite{Akaike:1973uc}. In the special case where
$g = f(\cdot;\tbth)$, $I(\tbth)$ becomes the Fisher information matrix, and
$I(\tbth) = \Sigma^{-1}$. In this case equation~\eqref{eq:aic general} leads
to the original \aic by using $\ell_n(\hbth;\bfy)$ as an unbiased consistent
estimator for its own expectation. This ideal situation does not exist in
reality anyway. But as shown in \textcite{Shibata:1989tm}, $k$, the dimension
of the estimated parameter vector is a good estimator of $\tr(I(\tbth)\Sigma)$
when the model is a ``good'' approximation of the true data generating
mechanism. This also leads to the exact formula of \aic. In either case, a
large sample is required to produce a good estimate of the expected
\kl.\footnote{More precisely, it is the relative \kl being estimated, the term
  $\int g(x)\log g(x)\dd x$ in equation~\eqref{eq:kl exp ll} is not and cannot
  be estimated.}

In summary, \aic is fully justified when the candidate models are already good
approximations to the true data generating mechanism and the sample size is
sufficient large. But one can hardly determine if the candidate models are
``good'' approximations while the true model, if it exists at all, is unknown.
There have been many efforts to improve the \aic methods in the situations
that the candidate models are not ``good enough'' approximations or the sample
size is small. One of the information criteria of this kind are reviewed in
the subsection~\ref{sub:Corrected aic}. However, when the model is high
dimensional and the available data is limited, simple information criteria
like \aic can hardly work.

\subsubsection{A Bayesian analysis of \protect\aic}
\label{ssub:A Bayesian analysis of aic}

The \aic method is often compared with the Bayesian model selection as one of
the most important frequentist model selection methods, in particular in
contrast with the \bic method. It is interest to note that
\textcite{Akaike:1978ti} has a Bayesian interpretation of the \aic procedure
for the special case of a multivariate Gaussian model. In that case, \aic can
be viewed as an approximation to the posterior probability. Without going into
the details, \textcite{Akaike:1978ti} considered the situation that the sample
size can be easily increased to a large enough number while two models are
very close in the sense that the difference in their parameters are not quite
visible through the observed data. Under this setting, it was then shown that
the log of posterior model probability from a normal priors for mean vector is
approximated by $(-1/2)$\aic up to an additive constant. However we don't see
this setting as the general cases in data analysis as argued by Akaike.
Instead, in realistic cases the difficulties of model selection problem often
come from the fact that we have very limited data.

On the other hand, as seen in \textcite{Akaike:1980gh}, using likelihood
function and \aic estimates as the source of objectivity of the Bayesian
modeling leads to the \bic procedure. However, those with a subjective
Bayesian perspective are not likely to be convinced by the desire of
objectivity itself. In addition, as noted by \textcite{Kass:1995vb}, the
Bayesian justification of \aic is only valid when the precision of the prior
distribution is comparable to that of the likelihood. This assumption is
hardly ever true in reality.

In both cases, we found that though the motivations of these Akaike's work are
likely purely for the defense of \aic rather than a general assessment of the
connections between \aic and Bayesian modeling, it is still interested to note
that the two frameworks are closely related. In practice different model
selection methods should be considered based on the purposes (inference of the
past or prediction of the future, or otherwise). There is no reason to argue
that one is absolutely superior to another.

\subsubsection{Corrected \protect\aic}
\label{ssub:Corrected aic}

In this subsection we briefly review one of the most widely used extension and
modification of \aic method, the corrected \aic. This and other similar
methods try to improve the model selection procedure in the situations that
\textcite{Akaike:1973uc}'s assumptions are not justified. Therefore, they improve
the robustness of the \aic procedure.

The corrected \aic, usually denoted by \aicc, mainly deals with the situation
that the sample size is not sufficient large in the sense that the dimension
of the parameter vector to be estimated is close to the sample size.
\textcite{Hurvich:1989ev} developed the \aicc for regression and time series
modeling. The \aicc is defined as,
\begin{equation}
  \text{\aicc} = -2\ell_n(\hbth;\bfx) + \frac{2nk}{n-k-1}
\end{equation}
or equivalently,
\begin{equation}
  \text{\aicc} = \text{\aic} + \frac{2k(k+1)}{n-k-1},
\end{equation}
where $n$ is the sample size, other notations are as defined before.

It should be noted that \aicc is just one way to improve \aic for small
samples. In particular, it is derived in the case of a model with linear
structure and normal errors \parencite{Hurvich:1989ev, Burnham:2002wc}. Under
other models, other form of improved \aic can be derived. In general,
\textcite{Hurvich:1989ev}'s form seems to be a good choice and adopted
extensively in the literature even in nonlinear cases
\parencite[e.g.][]{Turkheimer:2003iy}.

However, to illustrate the limitation of the \aicc method, in
\textcite{Zhou:2011uo} which we examined the model selection for compartment
models, the use of \aicc was shown inefficient for high dimension, high noise
and limited size of data. The use of \aic based methods for such models was
first introduced by \textcite{Hawkins:1986ha}. However, their work, and some
recent use of \aic or \aicc mainly focus on low noise data. If the model has a
nonlinear structure and high level noise, it can hardly be approximated by
normal distribution. In such situations, as shown in \textcite{Zhou:2011uo}
for \pet data, \aicc does not perform well. It is possible to derive specific
\aic based information criterion for specific models, like
\textcite{Hurvich:1989ev} did for theirs. Nonetheless this will involve much
more analytical work which may or may not results in improved results.
Instead, in the our work, using Bayesian model selection for \pet data are
approached. The results are encouraging while the implementations still has
limitations.
