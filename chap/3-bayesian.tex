\chapter{Bayesian model selection}
\label{cha:Bayesian model selection}

Bayes' theorem, in its simplest form is stated as below,
\begin{equation}
  \Pr(H|D) = \frac{\Pr(D|H)\Pr(H)}{\Pr(D)}
  \label{eq:bayes}
\end{equation}
where $D$ is the data and $H$ is a hypothesis. Like any other probability
theories, technically the Bayes' theorem merely provides a method of
accounting for the uncertainty. However, what is far more important is the
associated philosophical interpretation of probability, namely the subjective
perspective. Though early development of Bayesian statistics showed interests
of using likelihood functions as objective input to the modeling processes
\parencite{Jeffreys1961, Jeffreys1946}, it is nowadays widely accepted that a
subjective view is more appropriated \parencite[see][chap.~1]{Bernardo2000}.
The subjectivity and objectivity of probability is a more philosophical
problem that we will not elaborate in this report.

However, it shall be emphasized that within the Bayesian framework, we shall
think more in the term of ``beliefs''. It shall be noted that Bayesian
statistics offers a rationalist theory of \emph{personal} beliefs and
decisions in the contexts of uncertainty. So it is natural to question the
relevance of one's personal beliefs to another. Just like the long-run
relevance problem of the frequentist statistics, Bayesian statistics surely
has its own shortcoming. Such philosophical problems are not unimportant, but
are beyond the scope of this report. We will focus on the technical aspects of
the Bayesian statistics, in particular the model comparison and selection
problem, and their implementations.

As stated in chapter~\ref{cha:Introduction}, we insist on the
decision-oriented aspects of statistical inference, in particular, model
selection problems. A throughout treatment of Bayesian modeling from this
perspective can be found in \textcite{Robert:2007tc}. Formal mathematical
representations can also be found in \textcite[][sec.~5.1 and
sec.~6.1]{Bernardo2000}, where the notion of rational decisions in the context
of uncertainty was made precise in the form of axioms
\parencite[see][chap.~2]{Bernardo2000}. It is assumed that a rational decision
cannot be considered separately from rational beliefs. And rational beliefs
shall be built upon available information (the data in the statistical term)
and any personal preference input (the prior information in Bayesian
literatures).

It shall also be noted that logically, one's beliefs do not necessarily lead
to one's decisions. Instead the beliefs can often be interpreted as an useful
summary of the information available. In this case, the beliefs lead to
inference, but the inference itself can be interpreted as the decision to be
made \parencite[][chap.~1]{Lehmann:2005vy}. The foundation of the Bayesian
approach to various statistical problems is prescriptive rather than
descriptive. We are concerned with deriving beliefs from information and
making decisions or preferences based on decisions such that a specified form
of consistency is archived.

\paragraph{A note on notations} To avoid any possible confusions, we clarify
the notations in this chapter and the one follows. We use lowercase Latin or
Greek letters to denote densities and use the same letters for the
corresponding distributions. The same densities are always denoted by the same
letters, unless otherwise specified (table~\ref{tab:notations}). For any other
densities or probabilities, we use the generic notation $\Pr(\cdot)$, for
example $\Pr(D)$ and $\Pr(D|\kappa=k)$. This enables us to use unique letters
to distinguish important densities while not introducing too much notation.
Since there are no integrations or expectations with respect to the data in
the remaining, this use shall not cause problems for understanding. For random
variables denoted by lowercase Greek letters, we use the same letters to
denote their realizations. The upper case Greek letters are reserved for
parameter spaces.

\begin{table}[ht]
  \caption{Notations for densities used in Bayesian model selection}
  \label{tab:notations}
  \begin{tabularx}{\textwidth}{llX}
    \toprule
    Densities & Full expression & Meaning \\
    \midrule
    $f$ & $f(D|\bth_k,\kappa=k)$
    & The likelihood function of data $D$ conditional on the model parameter
    $\bth_k$ and model specified by the
    indicator $\kappa = k$. \\
    $\eta$ & $\eta(D,\bth_k,\kappa=k)$
    & The joint density of the data and the hyperparameter
    $(\bth_{\kappa},\kappa)$. \\
    $\pi$ & $\pi(\bth_k|\kappa=k)$
    & The prior distribution of the model parameter $\bth_k$ conditional on
    the model. \\
    $p$ & $p(\kappa = k)$
    & The prior distribution of the model indicator random variable
    $\kappa$. \\
    $\pi^*$ & $\pi^*(\bth_k|D,\kappa=k)$
    & The posterior distribution of the model parameter conditional on the
    data and the model. \\
    $p^*$ & $p^*(\kappa=k|D)$
    & The posterior distribution of the model indicator conditional on the
    data. \\
    $\mu$ & $\mu(\bth_k,\kappa=k|D)$
    & The full posterior distribution of the hyperparameter
    $(\bth_{\kappa},\kappa)$ conditional on the data. \\
    \bottomrule
  \end{tabularx}
\end{table}

\section{Bayesian model choice problem}
\label{sec:Bayesian model choice problem}

Consider a (possibly infinite) countable set of models $\calM = \{\calM_i,
  k\in I\}$. Our aim is to choose a ``best'' model from this set. There will
usually be actions taken once we have decided which model to use. For example
we may use the chosen model for inference of the past or prediction of the
future. It is the consequences of these actions of interest instead of the
chosen model itself. Therefore, from a decision-theoretic perspective, the
``best'' model should maximize the utility for some quality of the interest.
However, this is a very complicated problem. And in practice, it is common to
ignore the actions following the model selection decision and our interest is
the true model, say $\calM_t$. In this case, it is natural to define a
zero-one utility function, say $u(\calM_i,\calM_t)$, such that
$u(\calM_i,\calM_t) = 1$ for $\calM_t = \calM_i$ and $0$ otherwise.

It is easy to see that the model $\calM_i$ that maximizes the expected utility
given data $D$ is the model with highest posterior probability
$\Pr(\calM_i|D)$ \parencite[see][chap.~6]{Bernardo2000}. This model is thus
our ``best'' model in this setting. It should be noted this argument is only
valid if the true model $\calM_t \in \calM$. Otherwise, the utility is always
zero for all models. However, the precise meaning of a true model is not well
defined. Again, we will not focus on these philosophical problems. In what
follows, we presume that our aim is to find the model with the highest
posterior probability.

Since the set $\calM$ is countable, it is natural to define a discrete random
variable by the injective mapping $\kappa:\calM\to\bbN$, such that
$\kappa(\calM_i) = k_i$, $k_i \in \bbN$. The above problem of choosing the
model with the highest posterior probability is therefore equivalent to
inferencing the posterior density $p^*(\kappa = k|D)$. We will refer to a
model by $\calM_i$, or its corresponding $\kappa = k_i$ or simply $\kappa = k$
in the remaining of this report. The random variable $\kappa$ is often called
the model indicator. In the case of parametric models, assume that conditional
on $\kappa = k$, the parameter vector $\bth_k$ has dimension $n_k$ and
parameter space $\bTh_k\subset\bbR^{n_k}$. In addition, a prior distribution
for both $\bth_k$ conditional on $\kappa = k$ and the model indicator $\kappa$
are specified, say $\pi(\bth_k|\kappa =k)$ and $p(\kappa = k)$, respectively.
Therefore, given the data $D$, the joint density and posterior density are,
respectively,
\begin{align}
  \eta(D,\bth_k,\kappa=k)
  &= f(D|\bth_k,\kappa=k)\pi(\bth_k|\kappa=k)p(\kappa=k)
  \label{eq:full joint}\\
  \mu(\bth_k,\kappa=k|D)
  &= \pi^*(\bth_k|D,\kappa=k)p^*(\kappa=k|D)
  \label{eq:full posterior}
\end{align}
where $f$ is the likelihood function. And $\pi^*$ and $p^*$ are the posterior
densities for $\bth_k$ and $\kappa$, respectively. Therefore the inference of
the model indicator $\kappa$ can be entirely based on the posterior densities
$\mu$ or $p^*$. Two approaches are commonly used in Bayesian model selection
problems. One is to compare the posterior probability $p^*(\kappa=k_i|D)$
pairwise for models $\kappa(\calM_i) = k_i$. This approach leads to the Bayes
factor, reviewed in the next section. The other approach is to inference the
distribution $\mu$ or $p^*$ directly. This approach to Bayesian model
selection can be seen as a specific application of variable dimension models,
which is briefly reviewed in section~\ref{sec:Variable dimension models}.

\section{Bayes factor}
\label{sec:Bayes factor}

When the model set $\calM$ is finite, we can find the model with the highest
posterior probability by compare models pairwise. Rewrite the posterior
density~\eqref{eq:full posterior} as,
\begin{equation}
  \mu(\bth_k,\kappa=k|D) = \frac{\eta(D,\bth_k,\kappa=k)}{\Pr(D)}.
\end{equation}
Substitute equation~\eqref{eq:full joint}, and integrate out the parameter
vector $\bth_k$ over its parameter space, we have the form of the posterior
model probability $p^*(\kappa=k|D)$,
\begin{equation}
  p^*(\kappa=k|D) = \frac{p(\kappa = k)}{\Pr(D)}
  \int f(D|\bth_k,\kappa = k)\pi(\bth_k|\kappa = k)\dd\bth_k.
\end{equation}
Therefore to compare the posterior probabilities of two models, say $\kappa =
k_1$ and $k_2$, one only need to compute their ratio,
\begin{align}
  \frac{p^*(\kappa = k_1|D)}{p^*(\kappa = k_2|D)} &= \frac
  {\int f(D|\bth_{k_1},\kappa=k_1)\pi(\bth_{k_1}|\kappa=k_1)\dd\bth_k}
  {\int f(D|\bth_{k_2},\kappa=k_2)\pi(\bth_{k_2}|\kappa=k_2)\dd\bth_k}
  \frac{p(\kappa=k_1)}{p(\kappa=k_2)} \notag\\
  & = B_{12} \frac{p(\kappa=k_1)}{p(\kappa=k_2)},
  \label{eq:posterior odd}
\end{align}
where
\begin{equation}
  B_{12} = \frac
  {\int f(D|\bth_{k_1},\kappa=k_1)\pi(\bth_{k_1}|\kappa=k_1)\dd\bth_k}
  {\int f(D|\bth_{k_2},\kappa=k_2)\pi(\bth_{k_2}|\kappa=k_2)\dd\bth_k}
  \label{eq:bayes factor}
\end{equation}
is called the Bayes factor. Equation~\eqref{eq:posterior odd} states how the
prior odds ratio is transformed into the posterior odds ratio by the Bayes
factor \parencite{Kass:1995vb}. The Bayes factor is the principle tool for
Bayesian model comparison and model selection. As it is made clear in the
above equation, to compute the Bayes factor, all that needs to be done is the
computation of following quality for each value of $\kappa$,
\begin{equation}
  \Pr(D|\kappa = k)
  = \int f(D|\bth_k,\kappa=k)\pi(\bth_k|\kappa=k)\dd\bth_k,
  \label{eq:marginal likelihood}
\end{equation}
which is called the marginal likelihood of data $D$ under model $\kappa = k$.
It is obvious that $B_{ij} = B_{ik} B_{kj}$, and thus the Bayes factor
approach is equivalent to choose the model with the highest marginal
likelihood, as long as the model set $\calM$ is finite.

It should be noted that the prior distribution $\pi(\bth_k|\kappa=k)$ is
chosen by the statistician in the modeling process. The choice of the prior
distributions is one of the most critical and criticized part of Bayesian
modeling. In principle, the prior distribution shall represent the prior
beliefs. That is, it represents how much is already known about the data
generating mechanism and what is believed about it before the observation of
the data, no more or no less. It shall not only describe all knowledge already
known, but also more importantly preserve all the ignorance. If it contains
more information than actually known, the inference will be biased. In other
words, one can always construct a prior distribution such that inference will
be biased towards one's preference, which is not necessarily rational. It is
often too difficult to elicit a precise distribution from prior information.
Therefore it is necessary to make at least partly arbitrary choice of the
prior distribution \parencites{Robert:2007tc}[][chap.~3]{Kass:1995vb}.

Often the priors need to be considered carefully on a per problem basis.
Nonetheless there are a few classes of prior distributions frequently used in
the Bayesian modeling. In the particular problem of Bayesian model comparison
and selection, it shall be noted that the Bayes factor is more sensitive to
the choice of prior than point or interval estimations \parencite{Kass:1993vy,
  Kass:1995vb}. One of the more important class of prior distribution is the
conjugate prior. See \textcite[][chap.~5]{Bernardo2000} for its justification.
In practice the conjugate prior is often used because of its mathematical
properties. However it is often argued that this is an objective approach as
the subjective input is reduced to the choice of parameters. In addition, when
there is no prior information available, the only justification for conjugate
priors is analytical, their closed form expression. There is no way to justify
the choice of prior distributions on a subjective basis in such situations. In
such settings, there are also efforts to make the prior ``non-informative''.
Among them the most important two are Jeffreys' priors
\parencite{Jeffreys1946} and the reference prior \parencite{Bernardo1979}. See
\textcite{Kass:1996jj} for more detailed analysis of the Jeffreys' priors and
other non-informative approach to Bayesian analysis and
\textcite{Berger:1989vj, Berger:1992kf, Berger:1992wo} for reference priors.

It is also important to evaluate the Bayes factor over a range of possible
priors to assess the sensitivity issues. This is often computational intensive
since many high dimensional integrations are required. When there is enough
information to construct parametric priors, it is possible to alter the values
of parameters and recompute the Bayes factor
\parencite[e.g.,][]{McCulloch:1991hj}. In general situations, a less
computational intensive method is to use the Laplace approximation to compare
the Bayes factor using different priors \parencite[e.g.,][]{Kass:1992tz}. It
is also proposed in the literature to use the maximum of Bayes factor (and
thus the maximal evidence against a model) to evaluate the sensitivity problem
\parencite[e.g.,][]{Berger1987}.

We will not concern much of the choice of priors in the remaining of this
report. It should however be noted that in practice, prior distributions
should always be chosen carefully and the sensitivity should be evaluated. In
what follows, we assume a suitable prior is already chosen such that the Bayes
factor is finite and nonzero, and we concerns the computation of the Bayes
factor. Broadly the computation can be divided into two classes: approximation
based and Monte Carlo methods based. We review one of the most important
information criteria, the Bayesian information criterion (\bic) in the next
subsection. We will review the Monte Carlo methods in
chapter~\ref{cha:Bayesian computation with Monte Carlo methods}.

\subsection{Bayesian information criterion}
\label{sub:Bayesian information criterion}

The Bayesian information criterion (\bic) was developed as an approximation to
the Bayes factor \parencite{Schwarz:1978uv}. The \bic is defined as,
\begin{equation}
  \text{\bic} = -2\log f(D|\hbth) + k\log(n).
\end{equation}
where $f(D|\hbth)$ is the likelihood function evaluated at the \mle ($\hbth$)
and $k$ is the number of parameters to be estimated. The \bic strategy choose
the model with smallest \bic value. To justify its use, we first outline its
derivation, which will provide us some insights of the quality of the
approximation. We start with equation~\eqref{eq:marginal likelihood} while
dropping the model index $k$ and $\kappa=k$, since this computations is for
each model individually. Let $h(\bth) = - \log f(D|\bth)\pi(\bth)$ and let
$\tbth$ denote the value of $\bth$ that minimizes $h(\bth)$. Then,
\begin{equation}
  \Pr(D) = \int f(D|\bth)\pi(\bth)\dd\bth = \int\exp\{-h(\bth)\}\dd\bth
\end{equation}
Applying Taylor expansion to $h(\bth)$ around $\tbth$,
\begin{align}
  h(\bth)
  &= h(\tbth) + (\bth - \tbth)^T h'
  + \frac{1}{2}(\bth - \tbth)^T h''(\tbth) (\bth - \tbth)
  + o(\lVert \bth - \tbth \rVert)^2 \notag\\
  &\approx h(\tbth) + \frac{1}{2}(\bth - \tbth)^T H (\bth - \tbth)
\end{align}
where $H = h''(\tbth)$, the Hessian matrix of $h$ evaluated at $\tbth$ and
$h'$ is the gradient of $h$. Since for $n\to\infty$, the posterior
distribution is concentrated around $\tbth$, and thus only those values close
to $\tbth$ contribute significantly to the integration,
\begin{equation}
  \Pr(D)\approx\exp\{-h(\tbth)\}
  \int\exp\{-\frac{1}{2}(\bth - \tbth)^TH(\bth - \tbth)\}.
\end{equation}
Note the integrand is the kernel of of a multivariate normal distribution with
mean $\tbth$ and covariance matrix $H^{-1}$, which is assumed to be positive
definite and exist. It follows,
\begin{equation}
  \Pr(D) \approx \exp\{-h(\tbth)\} (2\pi)^{k/2} \lvert H \rvert^{-1/2},
\end{equation}
where $k$ is the number of parameters, or the dimension of $\bth$. Thus,
\begin{equation}
  -2\log\Pr(D) \approx
  -2\log f(D|\tbth) - 2\log\pi(\tbth) - k\log(2\pi) + \log\lvert H \rvert.
  \label{eq:bic approx}
\end{equation}
With i.i.d samples, for $n\to\infty$, $H/n \approx i$, where $i$ is the Fisher
information matrix. It follows that $\lvert H \rvert \approx n^k \lvert i
\rvert$ for large $n$. Omitting the $O(1)$ terms,
\begin{equation}
  -2\log\Pr(D) \approx -2\log f(D|\tbth) + k\log(n).
\end{equation}
With $n\to\infty$, $\tbth\approx\hat{\bth}$, where $\hat{\bth}$ is the \mle,
replace $\tbth$ with $\hat{\bth}$, the right hand side of the above equation
becomes the formula for what is commonly known as \bic. The \bic method choose
the model with smallest \bic.

The \bic method is often compared and contrasted with the \aic method, which
we discussed in section~\ref{sec:Akaike's information criterion}. Despite the
similarity in formulas, \aic and \bic have very different assumptions. In
addition to the large sample assumption, which is difficult to assess in
practice, \bic also assumes ``good behavior'' of the likelihood function such
that the use of \mle in place of $\tbth$ is justified. Also, unlike \aic,
whose approximation is often at worst $O(1/n)$, the \bic requires that the
$O(1)$ term in equation~\eqref{eq:bic approx} is negligible compared to other
terms. These assumptions restricted the use of \bic in many situations. See
\textcite{Gelfand1994} for an example where the $O(1)$ approximation failed.
See \textcite{Berger2001} for examples that the irregularity of likelihood
function causes the \bic method failed. Moreover, in non-i.i.d. situations,
just like \aic, the definition of the parameter dimension $k$ is ambiguous
\parencite{Spiegelhalter:1998uc, Kass:1995vb}. See also \textcite[][chap.~5
and chap.~6]{Burnham2002} for more comparison and contrast of \aic and \bic
approaches to model selection. There are other criticism of the \bic strategy.
For example \textcite[][chap.~7]{Robert:2007tc} argued that the \bic strategy
eliminated the subjective input into the Bayes modeling since the value of
\bic does not depend on the prior distribution. However this is equally argued
as an advantage of this strategy in the case that priors, to which Bayes
factors can be very sensitive, are hard to specify.

\section{Variable dimension models}
\label{sec:Variable dimension models}

Recall the end of section~\ref{sec:Bayesian model choice problem}, instead of
using the marginal likelihood for Bayesian model selection, the posterior
density $\mu(\bth_k,\kappa=k|D)$ can also be directly used for model
inference. Since the posterior model probability $p^*(\kappa=k|D)$ is simply
the marginal density of $\mu$. Therefore if we have knowledges of $\mu$,
(usually from simulations, see section~\ref{sec:Simulation for variable
  dimension models}), it is straightforward to make model inference. Formally,
let $\calK$ denote the possible values of $\kappa$ (this is equivalent to the
notion of model set $\calM$). Then the goal is to make inference about the
parameter vector $(\kappa, \bth_{\kappa})$ conditional on the data $D$, whose
parameter space is,
\begin{equation}
  \bTh = \bigcup_{k\in\calK}{k}\times\bTh_k,
  \label{eq:variable dimension parameter space}
\end{equation}
where $\bTh_k$ is the model parameter space of $\bth_k$.

Despite its straightforward interpretation of the inference objective, this
representation has some theoretical difficulties. Limited by the scope of this
report, we will not discuss the details such as the measure-theoretic
difficulty in the notions of prior density for a direct sum space.
Nonetheless, it shall be noted that in the context of Bayesian model
selection, the model parameter spaces $\bTh_k$ are independent of each other.
And thus a prior density $\pi(\bth_k|\kappa =k)$ need to be specified for each
model. However, it is often preferred to use some parameters common to all
models and thus reduce the modeling and computation difficulties. As it will
be discussed in subsection~\ref{sub:Reversible jump mcmc}, the reversible jump
\mcmc techniques rely on such assumptions. The computation of the variable
dimension models are reviewed in section~\ref{sec:Simulation for variable
  dimension models}
