\chapter{Summary and motivations for future research}
\label{cha:Summary and motivations for future research}

\section{Limitations of current methods}
\label{sec:Limitations of current methods}

The limitations of the current methods can be summarized into two kinds. One is the quality of the estimator while
the other is the difficulty of implementations.

The information-theoretic model selection methods aims to estimate the difference between candidate models and the
true model. And the estimators like \aic requires a large sample to perform well. Intuitively, the estimations are
based on the information from data for discrimination between models. Without enough data there can be difficulties
for these information criteria to given sensible model selection results.

Bayesian model selection from a subjective perspective does not require large samples. Beliefs are updated from
priors to posteriors through Bayesian theorem. However, approximation methods for Bayesian model selection like
\bic still requires large samples and regularity of the models to work. Hence they can fail for the same reason as
\aic and related methods.

Monte Carlo methods for simulations from posterior densities overcome the above limitations. However two problems
still remains. The first is the stability and convergence properties of various estimators. The other is the
implementations. In particular newer methods like \rjmcmc still requires much efforts to make it work. It shall be
expected that with more research, these difficulties of implementations will be eases in future. However we can
also seek for alternative ways of doing Bayesian computation.

In the following section, we give a very brief introduction to the sequential Monte Carlo method, which we consider
as an alternative approach to Bayesian model selection problems.

\section{Sequential Monte Carlo sampler}
\label{sec:Sequential Monte Carlo sampler}

The sequential Monte Carlo method (\smc) itself is not new to the literature. It has been extensively used for
particular filtering, which is often the synonym of \smc in literature. It also found applications to sequential
Bayesian inference \parencite{Douc2005}. There is applications of \smc for generalized linear mixed models
\parencite{Fan2008}. However, there are little literature on the use of \smc for Bayesian model selection problems.

In general consider a sequence of probability densities $\{\pi_n\}$ with support $E_n$. The \smc method samples
from these densities \emph{sequentially}. That is first samples come form $\pi_1$, then $\pi_2$, and so on. The
motivation is that if successive densities $\pi_k$ and $\pi_{k+1}$ are not too much different, the moves from
$\pi_k$ to $\pi_{k+1}$ shall be relatively easy. The problem can arise in various areas. A simple example is in the
setting of Bayesian modeling and the goal is to sample the posterior density $\pi^*(\bth|D)$. One may start from
the prior density $\pi(\bth)$, which and be constructed such that it is easy to sample from. And then the sampler
moves ``smoothly'' from the prior to the possibly complex posterior density. In this simple example, the sequence
$\{\pi_n\}$ can be constructed in the form,
\begin{equation}
  \pi_n(\bth) = \pi^*(\bth|D)^{p_n} \pi(\bth)^{1-p_n},
\end{equation}
with $0 = p_1 < p_2 < \dots < p_n < \dots < p_m = 1$.

More generally, consider the case that all $\pi_n$ are defined on the same space $E$. This is applicable in many
Bayesian modeling settings. Let $\pi_n$ be known up to a normalizing constants $Z_n$, that is,
\begin{equation}
  \pi_n(x) = \frac{\gamma_n(x)}{Z_n},
\end{equation}
where $\gamma_n$ is a properly normalized density, the target distribution. Suppose we can sample from some density
$\eta_n$, then using the techniques in importance sampling, we can obtain unbiased and consistent estimator for the
normalizing constant and other qualities. This is no different form the methods in subsection~\ref{sub:Importance
  sampling and harmonic mean based estimators}. However \smc methods aims construct $\eta_n$ iteratively, such that
$\eta_n$ is very close to the target density $\pi_n$. And therefore the estimators does not suffer the same
problems as those methods.

\subsection{Algorithm settings}
\label{sub:Algorithm settings}

At $n = 1$, we start with the target density $\pi_1$, which is assumed to be easy to approximate efficiently by
$\eta_1$ using importance sampling. Often $\pi_1$ itself is easy to sample from already. However we don't assume
$\eta_1 = \pi_1$ for the generality. Then at $n = 2$ with target density $\pi_2$ and we build the importance
density $\eta_2$ by using samples from $\eta_1$. The rationale is that, if $\pi_1$ and $\pi_2$ are close enough,
then it is possible to move previously simulated samples in the high density regions of $\pi_2$ in a sensible way.
Therefore iteratively, at say, $n - 1$, we have $N$ samples $\{X_{n -1}^i\}$ from $\eta_{n-1}$, which is a good
approximation to $\pi_{n-1}$. We move these samples by a kernel $K_n(x,x')$. The new samples $\{X_n^i\}$ obtained
this way has the density,
\begin{equation}
  \eta_n(x') = \int \eta_{n-1}(x)K_n(x,x')\dd x.
\end{equation}
If we can compute $\eta_n$ pointwise, then it is possible to use the importance sampling techniques to estimate
$\pi_n$. A natural choice for $K_n$ is a Markov kernel with invariant distribution $\pi_n$. This is justified in
\textcite{DelMoral2006} and they also presents more formal specifications of the algorithms.

The major drawback of the above settings is that in general it is impossible to compute the importance density
$\eta_n$ pointwise. Though it is possible when the Markov move has an independent kernel, in most cases, whenever a
local move is used, the computation is intractable. This limitation is overcome by introduce an artificial backward
kernel $L_{n-1}(x',x)$ at each stage. See \textcite{DelMoral2006} for details and the optimization of the backward
kernel with regard to the forward kernel $K_n$.

The other drawback is the degeneracy problem. As the discrepancy between $\eta_n$ and $\pi_n$ tends to increase
with $n$, the variance of the unnormalized importance weights tends to increase. Intuitively, it is expected that
the approximation of $\eta_n$ to $\pi_n$ will be worse when $n$ increase since $\eta_n$ is built on top of
$\eta_{n-1}$, which is itself an approximation. The solution is to use resampling techniques. See for example
\textcite{Douc2005} for the use of resampling in \smc.

\subsection{Potential advantages}
\label{sub:Potential advantages and difficulties}

The first potential advantage of \smc is the existence of massive literature on the \mcmc algorithms. These are
good resource for construction the forward kernel $K_n$ in last subsection. However the performance of the
algorithm still depend on the sequence of target distribution $\{\pi_n\}$, the forward kernel $K_n$ and backward
kernel $L_n$. Using \smc for sampling from complex densities and estimating their normalizing constant is
attractive to the Bayesian model selection. This techniques can also be extended to variable dimension model
settings \parencite{Jasra2008}.

The other potential advantages of \smc is its computation cost. Though theoretically it can be even more
computation insensitive than \mcmc. But clearly from the algorithm setting, a large portion of the algorithm can be
parallelized or vectorized with little efforts. In contrast, a considerable portion of the computation in \mcmc
algorithms cannot be easily parallelized and sometime impossible due to the dependency between moves. Though this
is not a mathematical advantage of \smc at all, the author think the advance of computing techniques plays an
equally important role in the development of Monte Carlo methods and therefore should be take into consideration.
Given the fact that most computers today are \textsc{simd} vectorsizable and the advance in parallel computing like
\textsc{gpgpu}, the development of \smc can have important application in real world problems. A preliminary work
was done to take advantage of the \textsc{simd} ability of modern \textsc{cpu} to sample from and compute the
densities, etc., for common distributions\footnote{The work can be found on
  \url{https://github.com/zhouyan/vDist}.  The speed is about four times faster than \textsf{R} on single core
  (parallelization can be enabled easily) while providing similar interface as \textsf{R} in C++.}.

\section{Plans for future work}
\label{sec:Plans for future work}

In the past year we have applied the Bayesian modeling to compartmental models, which is widely used in
neuroscience. Neuroscience will have an important role in our future work and we aim to find applications of
\smc on these kinds of models. However, we will start with more general theoretical development of the use of \smc
in Bayesian model selection problems. Also the planned work is to use methods like \rjmcmc, which is argued to be
one of the most appealing to Bayesian model choice, as a benchmark for comparison our methods. Therefore we will
also implement \rjmcmc for the same kinds of models.
