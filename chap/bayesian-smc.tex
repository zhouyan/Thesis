\chapter{Sequential Monte Carlo for Bayesian Model Comparison}
\label{cha:Sequential Monte Carlo for Bayesian Model Comparison}

As reviewed in chapter~\ref{cha:Monte Carlo Methods}, \mcmc algorithms, though
widely used for the purpose of Bayesian computation, have many limitations.
Algorithms such as \rjmcmc are conceptually appealing, yet often difficult to
design in practice. Other algorithms such as the Metropolis-Hastings
algorithm, though provides a generic framework which can be applied in many
fields, the design of efficient, high performance algorithms still requires
considerable expertise and sometimes extensive experience.

In recent years, there is a tendency of considering population based
algorithms.  A common theme in these algorithms is that, instead of simulating
directly from complex target distribution, related yet simpler distributions
are used to ``help'' the simulation. One of such algorithm, which is
essentially a generalization of the Metropolis-Hastings algorithm, population
\mcmc is reviewed in section~\ref{sub:Population mcmc}, in which the easier to
simulate distribution ``lends'' information to the target and fastens its
mixing.  Another example of population based algorithm is the recent
development of particular \mcmc \cite{Andrieu:2010gc}, which embed \smc
samplers, which will be the central topic of this chapter, inside \mcmc
algorithms.

Sequential Monte Carlo (\smc) samplers, in various forms has been around for
more than a decade and widely used in many fields. Until recent there is
little interest of using it for Bayesian model comparison for a few reasons.
One of the more important one is that, when \mcmc algorithms are available,
\smc could cost more computational resources than a well designed \mcmc
algorithm. However we believe there are at least two important reasons that
\smc can be preferable to \mcmc for the purpose of Bayesian model comparison.

First, it provides a generic and robust framework for simulation from complex
distributions that are previously difficult for \mcmc algorithms, especially
for high dimensional multinode distributions. Though it is not impossible to
design \mcmc algorithms for the same problems, it can be hugely difficult in
practice. The \smc framework provides a vital alternative that is easy to use.
It has the potential to enable statisticians to construct more realistic,
useful models that were previously difficult, if not impossible, to use due to
the computational complexity. Second, most Monte Carlo algorithms has to be
implemented on computers to be useful. Therefore it is unrealistic to not
consider the trend of today's computer technologies. Parallel computing has
become commodity in recent years. And \smc is much more suitable for this kind
of computing than conventional \mcmc. There are many attempts of parallelizing
\mcmc algorithms. However, as we will see later, \smc has certain advantage
over some of them.

In this chapter, we first give a review of \smc algorithms in
section~\ref{sec:Sequential Monte Carlo samplers}. It is followed by a section
details the use of \smc in the context of Bayesian model comparison. Next,
section~\ref{sec:Extensions and refinements} develops some extensions and
refinements of existing practices. Last, we conclude this chapter with a
discussion of how the presented framework lead to an automatic and generic
algorithms.

The application of \smc samplers to the Bayesian model comparison problem is
straightforward. However, it has been overlooked in recent years. In this
section, we outline common strategies of using \smc samplers for Bayesian
model comparison. In the next section, we introduce some innovative refinement
and extensions to existing practices.

\section{Sequential Monte Carlo samplers}
\label{sec:Sequential Monte Carlo samplers}

\smc samplers allow us to obtain, iteratively, collections of weighted samples
from a sequence of distribution $\{\pi_t\}_{t\ge0}$ over essentially any
random variables on some measurable spaces $(E_t,\calE_t)$. It is an extension
of the \emph{sequential importance sampling} (\sis) techniques. It is a
generalization of particle filter algorithms widely used in physics and signal
tracking literature. In the remaining of this section, the sequential
importance sampling and resampling algorithms are introduced. Then how they
are generalized to \smc samplers for the purpose of the current work are
discussed.

\subsection{Sequential importance sampling and resampling}
\label{sub:Sequential importance sampling and resampling}

Sequential importance sampling (\sis) generalizes the importance sampling (see
section~\ref{sec:Classical Monte Carlo and importance sampling}) technique for
a sequence of distributions $\{\pi_t\}_{t\ge0}$ defined on spaces
$\{\prod_{k=0}^tE_k\}_{t\ge0}$. At time $t = 0$, sample
$\{X_0^{(i)}\}_{i=1}^N$ from $\eta_0$ and compute the weights $W_0^{(i)}
\propto \pi_0(X_0^{(i)})/\eta_0(X_0^{(i)})$. At time $t\ge1$, each sample
$X_{0:t-1}^{(i)}$, usually termed \emph{particles} in the literature, is
extended to $X_{0:t}^{(i)}$ by a proposal distribution
$q_t(\cdot|X_{0:t-1}^{(i)})$. And the weights are recalculated by $W_t^{(i)}
\propto \pi_t(X_{0:t}^{(i)})/\eta_t(X_{0:t}^{(i)})$ where
\begin{equation}
  \eta_t(X_{0:t}^{(i)}) =
  \eta_{t-1}(X_{0:t-1}^{(i)})q_t(X_{0:t}^{(i)}|X_{0:t-1}^{(i)})
\end{equation}
and thus
\begin{align}
  W_t^{(i)} \propto \frac{\pi_t(X_{0:t}^{(i)})}{\eta_t(X_{0:t}^{(i)})}
  &= \frac{\pi_t(X_{0:t}^{(i)})\pi_{t-1}(X_{0:t-1}^{(i)})}
  {\eta_{t-1}(X_{0:t-1}^{(i)})q_t(X_{0:t}^{(i)}|X_{0:t-1}^{(i)})
    \pi_{t-1}(X_{0:t-1}^{(i)})} \notag\\
  &= \frac{\pi_t(X_{0:t}^{(i)})}
  {q_t(X_{0:t}^{(i)}|X_{0:t-1}^{(i)})\pi_{t-1}(X_{0:t-1}^{(i)})}W_{t-1}^{(i)}
  \label{eq:si}
\end{align}
and importance sampling estimate of $\Exp_{\pi_t}[\varphi_t(X_{0:t})]$ can be
obtained using $\{W_t^{(i)},X_{0:t}^{(i)}\}_{i=1}^N$.

However this approach fails as $t$ becomes large. The weights tend to become
concentrated on a few particles as the discrepancy between $\eta_t$ and
$\pi_t$ becomes larger. Resampling techniques are applied such that, a new
particle system $\{\bar{W}_t^{(i)},\bar{X}_{0:t}^{(i)}\}_{i=1}^M$ is obtained
with the property,
\begin{equation}
  \Exp\Bigl[\sum_{i=1}^M\bar{W}_t^{(i)}\varphi_t(\bar{X}_{0:t}^{(i)})\Bigr] =
  \Exp\Bigl[\sum_{i=1}^NW_t^{(i)}\varphi_t(X_{0:t}^{(i)})\Bigr]
  \label{eq:resample}
\end{equation}
In practice, the resampling algorithm is usually chosen such that $M = N$ and
$\bar{W}^{(i)} = 1/N$ for $i=1,\dots,N$. Resampling can be performed at each time
$t$ or adaptively based on some criteria of the discrepancy. One popular
quantity used to monitor the discrepancy is \emph{effective sample size}
(\ess), introduced by \cite{Liu:1998iu}, defined as
\begin{equation}
  \ess_t = \frac{1}{\sum_{i=1}^N (W_t^{(i)})^2}
\end{equation}
where $\{W_t^{(i)}\}_{i=1}^N$ are the normalized weights. And resampling can
be performed when $\ess\le \alpha N$ where $\alpha\in[0,1]$.

The common practice of resampling is to replicate particles with large weights
and discard those with small weights. In other words, instead of generating a
random sample $\{\bar{X}_{0:t}^{(i)}\}_{i=1}^N$ directly, a random sample of
integers $\{R^{(i)}\}_{i=1}^N$ is generated, such that $R^{(i)} \ge 0$ for $i
= 1,\dots,N$ and $\sum_{i=1}^N R^{(i)} = N$. And each particle value
$X_{0:t}^{(i)}$ is replicated for $R^{(i)}$ times in the new particle system.
The distribution of $\{R^{(i)}\}_{i=1}^N$ shall fulfill the requirement of
Equation~\ref{eq:resample}. One such distribution is a multinomial
distribution of size $N$ and weights $(W_t^{(i)},\dots,W_t^{(N)})$. See
\cite{Douc:2005wa} for some commonly used resampling algorithms.

\subsection[SMC samplers]{\protect\smc samplers}
\label{sub:SMC Samplers}

\smc samplers allow us to obtain, iteratively, collections of weighted samples
from a sequence of distributions $\{\pi_t\}_{t\ge0}$ over essentially any
random variables on some spaces $\{E_t\}_{t\ge0}$, by constructing a sequence
of auxiliary distributions $\{\tilde\pi_t\}_{t\ge0}$ on spaces of increasing
dimensions, $\tilde\pi_t(x_{0:t})=\pi_t (x_t) \prod_{s=0}^{t-1}
L_s(x_{s+1},x_s)$, where the sequence of Markov kernels $\{L_s\}_{s=0}^{t-1}$,
termed backward kernels, is formally arbitrary but critically influences the
estimator variance. See \cite{DelMoral:2006hc} for further details and
guidance on the selection of these kernels.

Standard sequential importance sampling and resampling algorithms can then be
applied to the sequence of synthetic distributions, $\{\tilde\pi_t\}_{t\ge0}$.
At time $t - 1$, assume that a set of weighted particles
$\{W_{t-1}^{(i)},X_{0:t-1}^{(i)}\}_{i=1}^N$ approximating $\tilde\pi_{t-1}$ is
available, then at time $t$, the path of each particle is extended with a
Markov kernel say, $K_t(x_{t-1}, x_t)$ and the set of particles
$\{X_{0:t}^{(i)}\}_{i=1}^N$ reach the distribution $\eta_t(X_{0:t}^{(i)}) =
\eta_0(X_0^{(i)})\prod_{k=1}^tK_t(X_{t-1}^{(i)}, X_t^{(i)})$, where $\eta_0$
is the initial distribution of the particles. To correct the discrepancy
between $\eta_t$ and $\tilde\pi_t$, Equation~\ref{eq:si} is applied and in
this case,
\begin{equation}
  W_t^{(i)} \propto \frac{\tilde\pi_t(X_{0:t}^{(i)})}{\eta_t(X_{0:t}^{(i)})}
  = \frac{\pi_t(X_t^{(i)})\prod_{s=0}^{t-1}L_s(X_{s+1}^{(i)}, X_s^{(i)})}
  {\eta_0(X_0^{(i)})\prod_{k=1}^tK_t(X_{t-1}^{(i)},X_t^{(i)})}
  \propto \tilde{w}_t(X_{t-1}^{(i)}, X_t^{(i)})W_{t-1}^{(i)}
\end{equation}
where $\tilde{w}_t$, termed the \emph{incremental weights}, are calculated as,
\begin{equation}
  \tilde{w}_t(X_{t-1}^{(i)},X_t^{(i)}) =
  \frac{\pi_t(X_t^{(i)})L_{t-1}(X_t^{(i)}, X_{t-1}^{(i)})}
  {\pi_{t-1}(X_{t-1}^{(i)})K_t(X_{t-1}^{(i)}, X_t^{(i)})}
\end{equation}
If $\pi_t$ is only known up to a normalizing constant, say $\pi_t(x_t) =
\gamma_t(x_t)/Z_t$, then we can use the \emph{unnormalized} incremental
weights
\begin{equation}
  w_t(X_{t-1}^{(i)},X_t^{(i)}) =
  \frac{\gamma_t(X_t^{(i)})L_{t-1}(X_t^{(i)}, X_{t-1}^{(i)})}
  {\gamma_{t-1}(X_{t-1}^{(i)})K_t(X_{t-1}^{(i)}, X_t^{(i)})}
\end{equation}
for importance sampling. Further, with the previously \emph{normalized}
weights $\{W_{t-1}^{(i)}\}_{i=1}^N$, we can estimate the ratio of normalizing
constant $Z_t/Z_{t-1}$ by
\begin{equation}
  \frac{\hat{Z}_t}{Z_{t-1}} =
  \sum_{i=1}^N W_{t-1}^{(i)}w_t(X_{t-1}^{(i)},X_t^{(i)})
\end{equation}
Sequentially, the normalizing constant between initial distribution $\pi_0$
and some target $\pi_T$, $T\ge1$ can be estimated. See \cite{DelMoral:2006hc}
for details on calculating the incremental weights. In practice, when $K_t$ is
invariant to $\pi_t$, and an approximated suboptimal backward kernel
\begin{equation}
  L_{t-1}(x_t, x_{t-1}) = \frac{\pi(x_{t-1})K_t(x_{t-1}, x_t)}{\pi_t(x_t)}
\end{equation}
is used, the unnormalized incremental weights will be
\begin{equation}
  w_t(X_{t-1}^{(i)},X_t^{(i)}) =
  \frac{\gamma_t(X_{t-1}^{(i)})}{\gamma_{t-1}(X_{t-1}^{(i)})}.
  \label{eq:inc_weight_mcmc}
\end{equation}

\section{Application to Bayesian model comparison}
\label{sec:Application to Bayesian model comparison}

The problem of interest is characterizing the posterior distribution over
$\{\Mk\}_{k\in\calK}$, a set of possible models, with model $\Mk$ having
parameter vector $\paramk\in\Paramk$ which must also usually be inferred.
Given prior distributions $\pi(\Mk)$ and $\pi(\paramk|\Mk)$ and likelihood
$p(\data|\paramk,\Mk)$ we seek the posterior distributions $\pi(\Mk|\data)
\propto p(\data|\Mk)$. There are three fundamentally different approaches to
the computations,
\begin{enumerate}
  \item Calculate posterior model probabilities directly.
  \item Calculate the evidence, $p(\data|\Mk)$, of each model.
  \item Calculate pairwise evidence ratios.
\end{enumerate}
Each approach admits a natural \smc strategy.

\subsection{\smc1: An all-in-one approach}
\label{sub:smc1: An all-in-one approach}

One could consider obtaining samples from the same distribution employed in
the \rjmcmc approach to model comparison, namely,
\begin{equation}
  \pi^{(1)}(\Mk,\paramk) \propto \pi(\Mk)\pi(\paramk|\Mk)p(\data|\paramk,\Mk)
\end{equation}
which is defined on the disjoint union space
$\bigcup_{k\in\calK}(\{\Mk\}\times\Paramk)$.

One obvious \smc approach is to define a sequence of distributions
$\{\pi_t^{(1)}\}_{t=0}^T$ such that $\pi^{(1)}_0$ is easy to sample from,
$\pi_{T}^{(1)} = \pi^{(1)}$ and the intermediate distributions move smoothly
between them. In the remainder of this section, we use the notation
$(\Mk[t],\paramk[t])$ to denote a random sample on the space
$\bigcup_{k\in\calK}(\{\Mk\}\times\Paramk)$ at time $t$. One simple approach,
which might be expected to work well, is the use of an annealing scheme such
that,
\begin{equation}
  \pi^{(1)}_t(\Mk[t],\paramk[t]) \propto \pi(\Mk[t])\pi(\paramk[t]|\Mk[t])
  p(\data|\paramk[t],\Mk[t])^{\alpha(t/T)},
  \label{eq:geometry_1}
\end{equation}
for some monotonically increasing $\alpha:[0,1]\to[0,1]$ such that $\alpha(0)
= 0$ and $\alpha(1) = 1$. Other approaches are possible and might prove more
efficient for some problems (such as the ``data tempering'' approach which
\cite{Chopin:2002hg} proposed for parameter estimation which could easily
be incorporated in our framework), but this strategy provides a convenient
generic approach. These choices lead to Algorithm~\ref{alg:smc1}.

% $\alpha$ provides an \emph{annealing schedule} which is intended to allow
% the control of the discrepancy between pairs of adjacent distributions. This
% gives the joint prior on model order and parameters as an initial
% distribution and gradually introduces the influence of the likelihood until
% the full posterior is reached.

% Given a weighted collection of samples $\{W^{(k,i)}_T, X^{(k,i)}_{T} =
% (K_T^{(k,i)},\theta_T^{(k,i)})\}_{i=1}^N$ targeting $\pi^{(1)}$, one may
% approximate the two distributions of interest using the empirical measures,
% \begin{align*}
%   \widehat{\pi}^{(1)}(K = k) &= \sum\limits_{i=1}^N W^{(k,i)} \delta_{k,
%   K^{(k,i)}_T} \\
%   \widehat{\pi}^{(1)}(d\theta|K=k) &= \frac{1}{\widehat{\pi}^{(1)}(K = k)}
%   \sum\limits_{i=1}^N W^{(k,i)} \delta_{k, K_T^{(k,i)}}
%   \delta_{\theta^{(k,i)}_T}(d\theta),
% \end{align*}
% where $\delta_{k,K^{(k,i)}_T}$ denotes the \emph{Kronecker} delta and
% $\delta_{\theta{(k,i)}}$ denotes the Dirac measure concentrated at
% $\theta^{(k,i)}$.

% Thus, the output of such an \smc algorithm may be used as a (weighted)
% equivalent to that from a suitable \rjmcmc algorithm, with the posterior
% probability of each model corresponding to the (weighted) proportion of
% samples from that model.

This approach might outperform \rjmcmc when it is difficult to design
fast-mixing Markov kernels. There are many examples of such an annealed \smc
strategy outperforming \mcmc at a given computational cost --- see, for
example, \cite{Fan:2008tf,Johansen:2008kp,Fearnhead:2010ua}. Such
trans-dimensional \smc has been proposed in several contexts
\cite{Peters:2005wh} and an extension proposed and analysed
by \cite{Jasra:2008bb}.

\begin{algorithm}
\begin{algorithmic}
  \STATE \emph{Initialisation:} Set $t\leftarrow0$.
  \STATE\STATESKIP Sample $X_0^{(i)} = (M_0^{(i)},\theta_0^{(i)})\sim\nu$
  for some proposal distribution $\nu$ (usually the joint prior).
  \STATE\STATESKIP Weight $W_0^{(i)} \propto w_0(X_0^{(i)}) =
  {\pi(M_0^{(i)}) \pi(\theta^{(i)}_0|M_0^{(i)})}/
  {\nu(M_0^{(i)},\theta_0^{(i)})}$.
  \STATE\STATESKIP Apply resampling if necessary (e.g., if \ess
  \cite{Kong:1994ul} less than some threshold).

  \STATE \emph{Iteration:} Set $t\leftarrow t + 1$.
  \STATE\STATESKIP Weight $W_t^{(i)} \propto W_{t-1}^{(i)}
  p(\data|\theta_{t-1}^{(i)},M_{t-1}^{(i)})^{\alpha(t/T) - \alpha([t-1]/T)}$.
  \STATE\STATESKIP Apply resampling if necessary.
  \STATE\STATESKIP Sample $X_t^{(i)} \sim K_t(\cdot|X_{t-1}^{(i)})$, a
  $\pi_t^{(1)}$-invariant kernel.

  \STATE \emph{Repeat} the \emph{Iteration} step \emph{until $t = T$}.
\end{algorithmic}
\caption{\smc1: An All-in-One Approach to Model Comparison.}\label{alg:smc1}
\end{algorithm}

We include this approach for completeness and study it empirically later.
However, the more direct approaches described in the following sections lead
more naturally to easy-to-implement strategies with good performance.

\subsection{\smc2: A direct-evidence-calculation approach}
\label{sub:smc2: A direct-evidence-calculation approach}

An alternative approach would be to estimate explicitly the evidence
associated with each model. We propose to do this by sampling from a sequence
of distributions for each model: starting from the parameter prior and
sweeping through a sequence of distributions to the posterior.

Numerous strategies are possible to construct such a sequence of
distributions, but one option is to use for each model $\Mk$, $k\in\calK$, the
sequence $\{\pi_t^{(2,k)}\}_{t=0}^{T_k}$, defined by
\begin{equation}
  \pi_t^{(2,k)}(\paramk[t]) \propto
  \pi(\paramk[t]|\Mk)p(\data|\paramk[t],\Mk)^{\alpha_k(t/T_k)}.
  \label{eq:geometry_2}
\end{equation}
where the number of distribution $T_k$, and the annealing schedule,
$\alpha_k:[0,1]\to[0,1]$ may be different for each model. This leads to
Algorithm~\ref{alg:smc2}.

The estimator of the posterior model probabilities depends upon the approach
taken to estimate the normalizing constant. Direct estimation of the evidence
can be performed using the output of this \smc algorithm and the standard
unbiased estimator, termed \smc2-\ds below,
\begin{equation}\label{eq:smc2-ds}
  \sum_{i=1}^N \frac{\pi(\theta_0^{(k,i)}|\Mk)}{\nu(\theta_0^{(k,i)})} \times
  \prod_{t=2}^T \sum_{i=1}^N W_{t-1}^{(k,i)}
  p(\data|\theta_{t-1}^{(k,i)}\Mk)^{\alpha_k(t/T_k) - \alpha_k([t-1]/T_k)}
\end{equation}
where $W_{t-1}^{(k,i)}$ is the importance weight of sample $i$,
$\theta_{t-1}^{(k,i)}$, during iteration $t-1$ for model $\Mk$. An alternative
approach to computing the evidence is also worthy of consideration. As has
been suggested, and shown to perform well empirically previously \cite[see,
for example]{Johansen:2006wm}, it is possible to use all of the samples from
every generation of an \smc sampler to approximate the path sampling estimator
and hence to obtain an estimate of the ratio of normalizing constants.
Section~\ref{sub:Path Sampling via smc2/smc3} provides details.

The posterior distribution of the parameters conditional upon a particular
model can also be approximated with,
\begin{equation*}
  \widehat{\pi}_{T_k}^{(2,k)}(\diff\theta) =
  \sum\limits_{i=1}^{N} W_{T_k}^{(k,i)}
  \delta_{\theta^{(k,i)}_{T_k}}(\diff\theta).
\end{equation*}
where $\delta_{\theta^{(k,i)}_{T_k}}$ is the Dirac measure.

% It is straightforward to use different numbers of samples, $N$, for each
% model when appropriate. in settings in which some models can be
% characterised substantially more easily than others.

This approach is appealing for several reasons. One is that it is designed to
estimate directly the quantity of interest: the evidence, producing a sample
from that distribution at the same time. Another advantage of this approach
over \smc1 and the \rjmcmc approach is that it provides as good a
characterisation of each model as is required: it is possible to obtain a good
estimate of the parameters of every model, even those for which the posterior
probability is small. Perhaps most significant is the fact that this approach
does not require the design of proposal distributions or Markov kernels which
move from one model to another: each model is dealt with in isolation. Whilst
this may not be desirable in every situation, there are circumstances in which
efficient moves between models are almost impossible to devise.

This approach also has some disadvantages. In particular, it is necessary to
run a separate simulation for each model --- rendering it impossible to deal
with countable collections of models (although this is not such a substantial
problem in many interesting cases). The ease of implementation may often
offset this limitation.

\begin{algorithm}
\begin{algorithmic}
  \STATE For each model $k \in \calK$ perform the following algorithm.

  \STATE \emph{Initialisation:} Set $t\leftarrow0$.
  \STATE\STATESKIP Sample $\theta_0^{(k,i)}\sim\nu_k$ for some proposal
  distribution $\nu_k$ (usually the parameter prior).
  \STATE\STATESKIP Weight $W_0^{(k,i)} \propto w_0(\theta_0^{(k,i)}) =
  {\pi(\theta_0^{(k,i)}|\Mk)}/{\nu_k(\theta_0^{(k,i)})}$.
  \STATE\STATESKIP Apply resampling if necessary.

  \STATE \emph{Iteration:} Set $t\leftarrow t + 1$.
  \STATE\STATESKIP Weight $W_t^{(k,i)} \propto W_{t-1}^{(k,i)}
  p(\data|\theta_{t-1}^{(k,i)},\Mk)^{\alpha(t/T_k) - \alpha([t-1]/T_k)}$.
  \STATE\STATESKIP Apply resampling if necessary.
  \STATE\STATESKIP Sample $\theta_t^{(k,i)} \sim
  K_t(\cdot|\theta_{t-1}^{(k,i)})$, a $\pi_t^{(k,2)}$-invariant kernel.

  \STATE \emph{Repeat} the \emph{Iteration} step \emph{until $t = T_k$}.
\end{algorithmic}
\caption{\smc2: A Direct-Evidence-Calculation Approach.}\label{alg:smc2}
\end{algorithm}

\subsection{\smc3: A relative-evidence-calculation approach}
\label{sub:smc3: A relative-evidence-calculation approach}

A final approach can be thought of as \emph{sequential model comparison}.
Rather than estimating the evidence associated with any particular model, we
could estimate pairwise evidence ratios directly. The \smc sampler starts with
a initial distribution being the posterior of one model (which could comes
from a separate \smc sampler starting from its prior) and moves towards the
posterior of another related model. Then the sampler can continue towards
another related model.

Given a finite collection of models $\{\Mk\}$, $k\in\calK$, suppose the models
are ordered in a sensible way (e.g., $\Mk[k-1]$ is nested within $\Mk$ or
$\paramk$ is of higher dimension than $\paramk[k-1]$). For each
$k\in\calK$, we consider a sequence of distributions
$\{\pi_t^{(3,k)}\}_{t=0}^{T_k}$, such that $\pi_0^{(3,k)}(\Mk[],\paramk[]) =
\pi(\paramk[]|\data,\Mk) \mathbb{I}_{\{\Mk\}}(\Mk[])$ and $\pi_{T_k}^{(3,k)}(\Mk[],\paramk[]) =
\pi(\paramk[]|\data,\Mk[k+1]) \mathbb{I}_{\{\Mk[k+1]\}}(\Mk[]) = \pi_{0}^{(3,k+1)}(\Mk[],\paramk[])$.
When it is possible to construct a \smc sampler that iterates over this
sequence of distributions, the estimate of the ratio of normalizing constants
is the Bayes factor estimate of model $\Mk[k+1]$ in favour of model $\Mk$.

This approach is conceptually appealing, but requires the construction of a
smooth path between the posterior distributions of interest. The geometric
annealing strategy which has been advocated as a good generic strategy in the
previous sections is only appropriate when the support of successive
distributions is non-increasing. This is unlikely to be the case in
interesting model comparison problems.

In this paper we consider a sequence of distributions on the disjoint union
$\{\Mk,\Paramk\}\cup\{\Mk[k+1],\Paramk[k+1]\},$ with the sequence of
distributions $\{\pi_t^{(3,k)}\}_{t=0}^{T_k}$ defined as the full posterior,
\begin{equation}
  \pi_t^{(3,k)}(\Mk[t],\paramk[t]) \propto
  \pi_t(\Mk[t]) \pi(\paramk[t]|\Mk[t]) p(\data|\paramk[t],\Mk[t])
\end{equation}
where $\Mk[t]\in\{\Mk,\Mk[k+1]\}$ and the prior of models at time $t$,
$\pi_t(\Mk[t])$ is defined by
\begin{equation}
  \pi_t(\Mk[k+1]) = \alpha(t/T_k)
  \label{eq:smc3_prior}
\end{equation}
for some monotonically increasing $\alpha:[0,1]\to[0,1]$ such that $\alpha(0)
= 0$ and $\alpha(1) = 1$. It is clear that the \mcmc moves between iterations
need to be similar to those in the \rjmcmc or \smc1 algorithms. The
difference is that instead of efficient exploration of the whole model space,
only moves between two models are required and the sequence of distributions
employed helps to ensure exploration of both model spaces. The algorithm for
this particular sequence of distribution is outlined in
Algorithm~\ref{alg:smc3}. It can be extended to other possible sequence of
distributions between models.

An advantage of this approach is that it provides direct estimate of the Bayes
factor which is of interest for model comparison purpose while not requiring
exploration of as complicated a space as that employed within \rjmcmc or
\smc1. The estimating of normalizing constant in \smc3 can follows in
exactly the same manner as in the \smc2 case. In \smc3, the same
estimator provides a direct estimate of the Bayes factor.

%Note that the standard \smc unbiased estimator is in fact the
%ratio of normalizing constants between the initial distribution and the final
%distribution. In \smc2, we initialised the system with the prior
%distribution and thus the estimates are directly the evidence for each model.


\begin{algorithm}
\begin{algorithmic}
  \STATE \emph{Initialisation:} Set $k\leftarrow1$.
  \STATE\STATESKIP Use Algorithm~\ref{alg:smc2} to obtain weighted samples
  for $\pi_{T_1}^{(3,1)}$, the parameter posterior for model $\Mk[1]$

  \STATE \emph{Relative Evidence Calculation}
  \STATE\STATESKIP Set $k\leftarrow k + 1$, $t\leftarrow0$.
  \STATE\STATESKIP Denote current weighted samples as
  $\{W_0^{(k,i)},X_0^{(k,i)}\}_{i=1}^N$ where $X_0^{(k,i)} =
  (M_0^{(k,i)},\theta_0^{(k,i)})$
  \STATE\STATESKIP Apply resampling if necessary.

  \STATE\STATESKIP \emph{Iteration:} Set $t\leftarrow t + 1$.
  \STATE\STATESKIP\STATESKIP Weight $W_t^{(k,i)} \propto W_{t-1}^{(k,i)}
  {\pi_t(M_{t-1}^{(k,i)})}/{\pi_{t-1}(M_{t-1}^{(k,i)})}$.
  \STATE\STATESKIP\STATESKIP Apply resampling if necessary.
  \STATE\STATESKIP\STATESKIP Sample $(M_t^{(k,i)},\theta_t^{(ki)}) \sim
  K_t(\cdot|M_{t-1}^{(k,i)}\theta_{t-1}^{(k,i)})$, a $\pi_t^{(3,k)}$-invariant
  kernel.

  \STATE\STATESKIP \emph{Repeat the \emph{Iteration} step up to $t = T_k$}.

  \STATE \emph{Repeat} the \emph{Relative Evidence Calculation} step \emph{until
    sequentially all relative evidences are calculated}.
\end{algorithmic}
\caption{\smc3: A Relative-Evidence-Calculation Approach to Model Comparison.}
\label{alg:smc3}
\end{algorithm}

\subsection{Path Sampling via \smc2/\smc3}
\label{sub:Path Sampling via smc2/smc3}

The estimation of the normalizing constant associated with our sequences of
distributions can be achieved by a Monte Carlo approximation to the \emph{path
  sampling} formulation given by \cite{Gelman:1998ei}. This is similar to the
technique for population \mcmc as described in section~\ref{sec:Bayesian
  computation with Markov chain Monte Carlo}. In the context of \smc, this
approach is also very closely related to the use of \ais for the same purpose
\cite{Neal:2001we} but as will be demonstrated below the incorporation of some
other elements of the more general \smc algorithm family can improve
performance at negligible cost. Recall that, given a parameter $\alpha$ which
defines a family of distributions, $\{p_{\alpha} = q_{\alpha} /
Z_\alpha\}_{\alpha \in [0,1]}$ which move smoothly from $p_0 = q_0 / Z_0$ to
$p_1 = q_1 / Z_1$ as $\alpha$ increases from zero to one, one can estimate the
logarithm of the ratio of their normalizing constants via a simple integral
relationship which holds under very mild regularity conditions,
\begin{equation}
  \log\biggl( \frac{Z_1}{Z_0} \biggr) =
  \int_{0}^{1} \Exp_\alpha \biggl[ \rnd{\log q_{\alpha}(\cdot)}{\alpha}
  \biggr] \intd\alpha, \label{eq:path_identity}
\end{equation}
where $\Exp_\alpha$ denotes expectation under $p_\alpha$. Note that the
sequence of distributions in the \smc2 and \smc3 algorithms above, can both be
interpreted as belonging to such a family of distributions, with $\alpha_t =
\alpha(t/T_k)$, where the mapping $\alpha:[0,1]\to[0,1]$ is again monotonic
with $\alpha(0) = 0$ and $\alpha(1) = 1$.

The \smc sampler provides us with a set of weighted samples obtained from a
sequence of distributions suitable for approximating this integral. At each
$t$ we can obtain an estimate of the expectation within the integral for
$\alpha(t/T)$ via the usual importance sampling estimator, and this integral
can then be approximated via numerical integration. Whenever the sequence
of distributions employed by \smc3 has appropriate differentiability it is
also possible to employ path sampling to estimate, directly, the evidence
ratio via this approach applied to the samples generated by that algorithm. In
general, given an increasing sequence $\{\alpha_t\}_{t=0}^T$ where $\alpha_0 =
0$ and $\alpha_T = 1$, a family of distributions
$\{p_{\alpha}\}_{\alpha\in[0,1]}$ as before, and a \smc sampler that iterates
over the sequence of distribution $\{\pi_t = p_{\alpha_t} =
q_{\alpha_t}/Z_{\alpha_t}\}_{t=0}^T$, then with the weighted samples
$\{W_t^{(j)},X_t^{(j)}\}_{j=1}^N$, and $t = 0,\dots,T$, a path sampling
estimator of the ratio of normalizing constants $\Xi_T = \log(Z_1/Z_0)$
can be approximated (using an elementary trapezoidal scheme) by
\begin{equation}
  \widehat\Xi_{T}^{N} = \sum_{t=1}^T
  \frac{1}{2}(\alpha_t - \alpha_{t - 1})(U_t^N + U_{t-1}^N)
  \label{eq:path_est}
\end{equation}
where
\begin{equation}
  U_t^N = \sum_{j=1}^N
  W_t^{(j)} \rnd{\log q_{\alpha}(X_t^{(j)})}{\alpha}\Bigm|_{\alpha = \alpha_t}
\end{equation}

We term these estimators \smc2-\ps and \smc3-\ps in the followings. The
combination of \smc and path sampling is somewhat natural and has been
proposed before, e.g., \cite{Johansen:2006wm} although not there in a Bayesian
context. Despite the good performance observed in the setting of rare
event simulation, the estimation of normalizing constants by this approach
seems to have received little attention in the literature. We suspect that
this is because of widespread acceptance of the suggestion of
\cite{DelMoral:2006hc}, that \smc doesn't outperform \ais when normalizing
constants are the object of inference or that of \cite{Calderhead:2009bd}
that all simulation-based estimators based around path sampling can be
expected to behave similarly. We will demonstrate below that these
observations, whilst true in certain contexts, do not hold in full generality.

\section{Extensions and refinements}
\label{sec:Extensions and refinements}

The algorithms introduced in the last section can be seen as straightforward
application of the well established \smc algorithms to Bayesian model
comparison. Though by construction, \smc algorithms can be more robust than
many \mcmc and other algorithms. However, as with any Monte Carlo algorithms,
without careful design, the performance can be far from satisfactory for many
realistic applications. In this section, we introduce some extensions and
refinement that can further improve the presented framework. Of course, they
cannot guarantee that the algorithms will perform well for all possible
situations. However, it provide robust and reliable solutions for many
realistic applications with minimal manual tuning of the algorithm. Further,
for more difficult problems, they provide a vital foundation on top of which
higher performance algorithms can be built.

We will use the \pet compartmental model example for illustrative purpose in
this section. More comprehensive performance comparisons is in
section~\ref{sub:Positron Emission Tomography Compartmental Model}. We will
consider both simulated and real world data. The simulated data set is the
same as in chapter~\ref{cha:Introduction}. Due to the large amount of the real
data, it is beyond the resources we have to perform repeated simulations for a
whole \pet scan. Instead, 20 real data sets are sampled from typical region of
interest in a single \pet scan.  Figure~\ref{fig:typical real pet} shows three
of them. As we can see, they varies considerably in characteristics despite
all can be described as ``typical'' \pet data. The purpose of the current work
is to advocate robust and self-tuning algorithms. And the variability in the
data sets provides excellent test examples. In addition, in this chapter, the
models are configured with the non-informative priors and thus the parameters
are exchangeable, similar to that of a mixture model. This creates a
multimodal posterior surface for models with two or more components.

\begin{figure}[t]
  \includegraphics[width=\linewidth]{fig/Typical_PET}
  \caption{Typical real \pet data}
  \label{fig:typical real pet data}
\end{figure}

\subsection{Improved univariate numerical integration}
\label{sub:Improved univariate numerical integration}

As seen in the last section, the path sampling estimator requires evaluation of
the expectation,
\begin{equation*}
  \Exp_\alpha \biggl[ \rnd{\log q_{\alpha}(\cdot)}{\alpha} \biggr]
\end{equation*}
for $\alpha\in[0,1]$, which can be approximated by importance sampling using
samples generated by a \smc sampler operating on the sequence of distributions
$\{\pi_t = p_{\alpha_t} = q_{\alpha_t}/Z_t\}_{t=0}^T$ directly for
$\alpha\in\{\alpha_t\}_{t=0}^T$. For arbitrary $\alpha\in[0,1]$, finding $t$
such that $\alpha\in(\alpha_{t-1},\alpha_t)$, the expectation can be easily
approximated using existing \smc samples --- the quantities required in the
importance weights to obtain such an estimate have already been calculated
during the running of the \smc algorithm and such computations have little
computational cost. %Indeed, in settings in which the \smc importance weights
%for target distribution $\pi_{\alpha}$ depend only upon the sample at time
%$t-1$, as in the proposed algorithms, the computational cost of the new
%weights are often minimal compared to generate new samples.

As noted by \cite{Friel:2012}  we can use more sophisticated numerical
integration strategies to reduce the path sampling estimator bias. For
example, higher order Newton-Cotes rules rather than the Trapezoidal rule can
be implemented straightforwardly. In the case of SMC it is especially
straightforward to estimate the required expectations at arbitrary $\alpha$
and so we cheaply use higher order integration schemes and we can also use
numerical integrations which make use of a finer mesh
$\{\alpha_t'\}_{t=0}^{T'}$ than $\{\alpha_t\}_{t=0}^T$. Since higher order
numerical integrations based on approximations of derivatives obtained from
Monte Carlo methods may potentially be unstable in some situations, the second
approach can be more appealing in some applications. A demonstration of the
bias reduction effect is provided in Section~\ref{sub:Positron Emission
  Tomography Compartmental Model}.

% It is straightforwardly possible to use more sophisticated numerical
% integration techniques in order to evaluate the approximation of the path
% sampling estimator. Even in the presence of adaptive annealing schemes, it
% is reasonably straightforward to implement higher order schemes than the
% simple Trapezoidal approximation. In a related setting, this was shown to
% provide an appreciable reduction in bias by \cite{Friel:2012}. In our
% context we have found that the bias of even the simple Trapezoidal scheme is
% inconsequential in the regime in which the variance of this method is small
% enough to be competitive and so in the interests of parsimony we have not
% investigated such schemes.

% It is possible to use numerical integration methods other than the
% Trapezoidal rules to improve the path sampling estimator. For example, in a
% \smc2 algorithm, if a sequence of distribution indexed by
% $\{\alpha(t/T)\}_{t=0}^T$ are used, where the path sampling integration is
% with respect to $\alpha\in[0,1]$. Then by placing an additional set of
% distributions at middle points
% $\{(\alpha(t/T)+\alpha((t+1)/T))/2\}_{t=0}^{T-1}$, Simpson's rule can be
% applied and can reduce the bias of the estimator given the same total number
% of distributions in some situations. Using similar techniques, other methods
% of numerical integration an be applied easily as well.

% There may or may not be a significant improvement through using these more
% sophisticated numerical integration methods, as other factors of the sampler
% may dominate the bias and variance of the estimates. Nonetheless, with very
% little effort, this kind of refinement is possible in principal within the
% \smc-\ps framework.

\subsection{Adaptive specification of distributions}
\label{sub:Adaptive specification of distributions}

In settings in which the importance weights at time $t$ depend only upon the
sample at time $t-1$, such as that considered here, it is relatively
straightforward to consider sample-dependent, adapative specification of the
sequence of distributions (typically by choosing the value of a tempering
parameter, such as $\alpha_t$ based upon the current sample).  In
\cite{Jasra:2010eh} such a method of adaptive placing the distributions in
\smc algorithms based on controlling the rate at which the effective sample
size (\ess; \cite{Kong:1994ul}) falls was proposed. With very little
computation cost, this provides an automatic method of specifying a tempering
schedule in such a way that the \ess decays in a regular fashion.
\cite[Algorithm 2]{Schafer:2011bx} used a similar technique but by moving the
particle system only when it resamples they are in a setting which would be
equivalent to resampling at every time step (with longer time steps, followed
by multiple applications of the MCMC kernel) in our formulation. We advocate
resampling only adaptively when \ess is smaller than certain preset threshold,
and here we propose a more general adaptive scheme for the selection of the
sequence of distributions which has significantly better properties when
adaptive resampling is employed.

The \ess was designed to assess the loss of efficiency arising from the use a
simple weighted sample (rather than a simple random sample from the
distribution of interest) in the computation of expectations. It's obtained by
considering a sample approximation of a low order Taylor expansion of the
variance of the importance sampling estimator of an arbitrary test function to
that of the simple Monte Carlo estimator; the test function itself vanishes
from the expression as a consequence of this low order expansion.

In our context, allowing $W_{t-1}^{(i)}$ to denote the \emph{normalized
  weights} of particle $i$ at the end of time $t - 1$, and $w_t^{(i)}$ to
denote the \emph{unnormalized} incremental weights of particle $i$ during
iteration $t$, the \ess calculated using the current weight of each particle
is simply,
\begin{equation}
  \ess_t = \left[ {\sum_{j=1}^N\left( \frac{W_{t-1}^{(j)}
          w_t^{(j)}}{\sum_{k=1}^NW_{t-1}^{(k)}w_t^{(k)}}\right)^2}
  \right]^{-1} = \frac{\bigl(\sum_{j=1}^NW_{t-1}^{(j)}w_t^{(j)}\bigr)^2}
  {\sum_{k=1}^N\bigl(W_{t-1}^{(k)}\bigr)^2\bigl(w_t^{(k)}\bigr)^2}
\end{equation}
It's clearly appropriate to use this quantity (which corresponds to the
coefficient of variation of the current normalized importance weights) to
assess weight degeneracy and to make decisions about appropriate resampling
times (cf. \cite{DelMoral:2012jq}) but it is rather less apparent that it's
the correct quantity to consider when adaptively specifying a sequence of
distributions in an \smc sampler.

The \ess of the current sample weights tells us about the accumulated mismatch
between proposal and target distributions (on an extended space including the
full trajectory of the sample paths) since the last resampling time. Fixing
either the relative or absolute reduction in \ess between successive
distributions does \emph{not} lead to a common discrepancy between successive
distributions unless resampling is conducted after every iteration as will be
demonstrated below.

When specifying a sequence of distributions it is natural to aim for a similar
discrepancy between each pair of successive distributions. In the context of
effective sample size, the natural question to ask is consequently, how large
can we make $\alpha_t - \alpha_{t-1}$ whilst ensuring that $\pi_{t}$ remains
sufficiently similar to $\pi_{t-1}$. One way to measure the discrepancy would
be to consider how good an importance sampling proposal $\pi_{t-1}$ would be
for the estimation of expectations under $\pi_t$ and a natural way to measure
this is via the sample approximation of a Taylor expansion of the relative
variance of such an estimator exactly as in the \ess.

Such a procedure leads us to a quantity which we have termed the
\emph{conditional} \ess (\cess),
\begin{equation}
  \cess_t = \left[ {\sum_{j=1}^N N W_{t-1}^{(j)} \left(
        \frac{w_t^{(j)}}{\sum_{k=1}^N
          N W_{t-1}^{(k)}w_t^{(k)}}\right)^2} \right]^{-1}
  = \frac{\bigl(\sum_{j=1}^NW_{t-1}^{(j)}w_t^{(j)}\bigr)^2}
  {\sum_{k=1}^N \frac{1}{N} W_{t-1}^{(k)} \bigl(w_t^{(k)}\bigr)^2}
\end{equation}
which is equal to the \ess only when resampling is conducted during every
iteration. The factor of $1/N$ in the denominator arises from the fact that
$\{W_{t-1}^{(i)}\}$ is normalized to sum to unity rather than to have expectation
unity: the bracketed term coincides with a sample approximation (using the
actual sample which is properly weighted to target $\pi_{t-1}$) of the
expected sum of the unnormalized weights squared divided by the square of a
sample approximation of the expected sum of unnormalized weights when
considering sampling from $\pi_{t-1}$ and targeting $\pi_t$ by simple
importance sampling.

\begin{figure}[t]
  \includegraphics[width=\linewidth]{fig/Adaptive_Dist}
  \caption{A typical plot of $\alpha_t - \alpha_{t-1}$ against $\alpha_t$ (for
    the \pet compartmental model using \smc2 algorithm.) The specifications of
    the adaptive parameter (\ess or \cess) are adjusted such that all four
    samplers use roughly the same number of distributions.}
  \label{fig:adaptive_alpha}
\end{figure}

When using \cess to adaptively place distributions, a value $\cess^* \in (0,
1)$ is chosen, and at each iteration $t$, $\alpha_t$ is chosen such that
$\cess_t = \cess^*$ (with a preset numeric error tolerance.)
Figure~\ref{fig:adaptive_alpha} shows the variation of $\alpha_t -
\alpha_{t-1}$ with $\alpha_t$ when fixed reductions in \ess and \cess are used
to specify the sequence of distributions both when resampling is conducted
during every iteration (or equivalently, when the \ess falls below a threshold
of 1.0) and when resampling is conducted only when the \ess falls below a
threshold of 0.5. It is found that for the simulated \pet data set, the
\cess-based scheme leads to a reduction in estimator variance around 20\%
relative to a manually tuned ($\alpha(t/T_k) = (t/T_k)^5$) schedule
while the \ess-based strategy provides little improvement over the linear case
($\alpha(t/T_k) = t/T_k$) unless resampling is conducted during every
iteration. The effect for the real data sets, which varies considerably from
each as seen in figure~\ref{fig:typical real pet data}, is more prominent. The
variance reduction can more than 50\%.

\subsubsection[cess*, number of distributions, and estimator variance]
{$\cess^*$, number of distributions, and estimator variance}
\label{ssub:cess*, number of distributions, and estimator variance}

It is intuitively to see that, when using a \cess-based adaptive scheme, where
at each iteration $t$, $\cess_t$ is fixed with a value $\cess^*$, the more
close $\cess^*$ is to $1$, the larger number of distributions will be placed
and better estimates can be obtained. Unfortunately it is not trivial to
establish a quantitative relations among these three quantities even
asymptotically. However, it is straightforward to conduct an empirical study
of these relations.

We consider the simulated \pet data set using the \smc2 sampler with 1,000
particles. Figure~\ref{fig:cess iter mean} plots the average number of
distributions (from 100 simulations for each value of $\cess^*$) against the
value of $(1 - \cess^*)$ on log scales. It can be seen that the number of
distributions is proposal to $1 / (1 - \cess^*)$. This relation holds even for
sampler configurations where a very small number of distributions are used.
Similarly, figure~\ref{fig:cess path var} shows the relation between the
variance of path sampling estimator and the value of $\cess^*$.

\begin{figure}[t]
  \includegraphics[width=\linewidth]{fig/CESS_Iter_Mean}
  \caption{Relations between average number of distributions and $\cess^*$.
    The number of distributions ranges from $~30$ to $~1300$.}
  \label{fig:cess iter mean}
\end{figure}

\begin{figure}[t]
  \includegraphics[width=\linewidth]{fig/CESS_Path_Var}
  \caption{Relations between variance of the path sampling estimator and
    $\cess^*$.}
  \label{fig:cess path var}
\end{figure}

Though the coefficient of the proportionality varies for different
applications or data sets, the relation shown in figures~\ref{fig:cess iter
  mean} and~\ref{fig:cess path var} provide a useful guidelines to select the
value of $\cess^*$. A small sample experiment can be conducted before using a
value of $\cess^*$ more close to $1$ to obtain satisfactory estimates or to
utilize given computational resources.

%% \ess only measures the performance of the particle system approximating the
%% target distributions, rather than the variances of the target distributions
%% themselves, which contributes to the variance of incremental weights. As seen
%% in equation~\eqref{eq:smc2-ds} and also for path sampling estimates when using
%% a geometric schedule, the variance of the incremental weights affect the
%% normalizing constants estimates directly. In the case of resampling at every
%% time step, the weights are directly normalized from the incremental weights.
%% In the general situation, as usual, let $W_{n-1}^{(i)}$ denote the
%% \emph{normalized weights} of particle $i$ at the end of time $t = n - 1$, and
%% $w_n^{(i)}$ denote the \emph{unnormalized} incremental weights of particle $i$
%% during the move at $t = n$. In the absence of resampling, we have the
%% recursive relationship,
%% \begin{equation*}
%%   W_n^{(i)} =
%%   \frac{W_{n-1}^{(i)}w_n^{(i)}}{\sum_{j=1}^NW_{n-1}^{(j)}w_n^{(j)}}
%% \end{equation*}
%% and the $\ess_n$,
%% \begin{equation}
%%   \ess_n = \frac
%%   {\bigl(\sum_{j=1}^NW_{n-1}^{(j)}w_n^{(j)}\bigr)^2}
%%   {\sum_{j=1}^N\bigl(W_{n-1}^{(j)}\bigr)^2\bigl(w_n^{(j)}\bigr)^2}
%% \end{equation}
%% If we do resampling after time $t = n - 1$ and before $t = n$, and let
%% $\bar{w}_n^{(i)}$ denote the new incremental weights after resampling, then we
%% have the new $\ess_n$, during move at $t = n$, as,
%% \begin{equation}
%%   \overline{\ess}_n = N \frac
%%   {\bigl(\sum_{j=1}^N\frac{1}{N}\bar{w}_n^{(j)}\bigr)^2}
%%   {\sum_{j=1}^N\frac{1}{N}\bigl(\bar{w}_n^{(j)}\bigr)^2}
%%   \label{eq:ess_before_res}
%% \end{equation}
%% \ess also provides information on ``smoothness'' of the sequence of
%% distributions in the sense that, it measures the discrepancy between sampling
%% distribution and the target distribution and if the target distributions
%% changes little, so shall be \ess. However, this link between \ess and the
%% ``smoothness'' is less direct. Another undesirable aspects of \ess is that its
%% evolution changes when different resampling strategies are used even when the
%% underlying target distributions are the same. Our motivation of extending the
%% concept of \ess is to adaptively place the distributions as if we do
%% resampling at every step without actually carrying out the resampling. Using
%% the unbiasedness of resampling, we can approximate $\overline{\ess}_n$ with
%% \begin{equation}
%%   \cess_n = \frac
%%   {\bigl(\sum_{j=1}^NW_{n-1}^{(j)}w_n^{(j)}\bigr)^2}
%%   {\sum_{j=1}^N\frac{1}{N}W_{n-1}^{(j)}\bigl(w_n^{(j)}\bigr)^2}
%% \end{equation}
%% where by $\cess_n$ we denote the ``conditional \ess'' as it depends upon a
%% consistent approximation of the variance of incremental weights. If we have
%% independent, identically distributed samples from the target distribution at
%% time $t = n - 1$, then $\cess_n$ tells us, essentially, how much variability
%% there would be in those incremental weights. And this variability reflects the
%% difference of the target distribution at time $t = n$ with the previous one.
%% This link is more obvious in the case where
%% equation~\eqref{eq:inc_weight_mcmc} is used to calculate the incremental
%% weights $w_n(x_{n-1},x_{n}$). Intuitively, \cess provides a direct measurement
%% of the ``smoothness'' of the sequence of distributions.

%% When resampling is performed at every step, then $\cess_n = \ess_n$.
%% Asymptotically, as the number of particles goes to infinity, adaptively
%% placing distributions such that $\cess_n = \cess^\star$ for a fixed value
%% $\cess^\star$ will give the same schedule as the resampling-every-time \ess
%% variant.

\subsection{Adaptive specification of proposals}
\label{sub:Adaptive specification of proposals}

The \smc sampler is remarkably robust to the mixing speed of \mcmc kernels
employed as can be seen in the empirical study below. However, as with any
sampling algorithms, faster mixing doesn't harm performance and in some cases
will considerably improve it. In the particular case of Metropolis-Hastings
kernels, the mixing speed relies on adequate proposal scales.

We adopt a simpler approach based on \cite{Jasra:2010eh}. They applied an idea
used within adaptive \mcmc methods \cite{Andrieu:2006tw} to \smc samplers by
using variance of parameters estimated from its particle system approximation
as the proposal scale for the next iteration, suitably scaled with reference
to the dimension of the parameters to be proposed.  Although, in practice we
found that such an automatic approach does not always lead to optimal accept
rates it generally produces satisfactory results and is simple to implement.
In difficult problems alternative approaches to adaptation could be employed;
one approach demonstrated in \cite{Jasra:2010eh} is to simply employ a pair of
accept rate  thresholds and to alter the proposal scale from the simply
estimated value whenever the accept rate falls outside those threshold values.

More sophisticated proposal strategies could undoubtedly improve performance
further and warrant further investigation. One possible approach is using the
Metropolis adjusted Langevin algorithm (\mala; see \cite{Roberts:1996vd}). In
summary, \mala derives a Metropolis-Hastings proposal kernel for a target
$\pi$ which satisfies suitable differentiability and positivity conditions,
from the Langevin diffusion,
\begin{equation*}
  \diff L_t = \frac{1}{2}\nabla\log\pi(L_t)\diff t + \diff B_t
\end{equation*}
where $B_t$ is the standard Brownian motion. Given a state $X_{n-1}$, a new
state is proposed by discrete approximation to the above diffusion. That is,
for a fixed $h>0$,
\begin{equation}
  X_n\sim\calN\Bigl(X_{n-1}+\frac{1}{2}\nabla\log\pi(X_{n-1}), hI_d\Bigr)
\end{equation}
where $I_d$ is the identify matrix and $d$ is the dimension of the state
space. The new proposed state is accepted or rejected through the usual
Metropolis-Hastings algorithm. Compared to a ``vanilla'' random walk, which
often has very robust theoretical properties, \mala is attractive when it is
possible and its convergence conditions \cite{Roberts:1996vd} can be met,
because only one discrete approximation parameter $h$ needs to be tuned
for optimal performance. In addition, results from \cite{Roberts:2001ta}
suggested that \mala can be more efficient than a random walk when using
optimal scalings. We could also use the particle approximation at time index
$t = n - 1$ to estimate the covariance matrix of $\pi_{n}$ and thus tune the
scale $h$ on-line. As these algorithms are known to be somewhat sensitive to
scaling, and we seek approaches robust enough to employ with little user
intervention, we have not investigated this strategy here.

\begin{figure}[t]
  \includegraphics[width=\linewidth]{fig/Adaptive_Proposal}
  \caption{Average random walk accept rates for \pet compartmental models with
    data sets in figure~\ref{fig:typical real pet data} using adaptive
    proposal scales}
  \label{fig:pet adaptive proposal}
\end{figure}

\begin{figure}[t]
  \includegraphics[width=\linewidth]{fig/Fixed_Proposal}
  \caption{Average random walk accept rates for \pet compartmental models with
    data sets in figure~\ref{fig:typical real pet data} using fixed proposal
    scales}
  \label{fig:pet fixed proposal}
\end{figure}

Adaptive specification of proposals is most useful when manual tuning is
difficult or even impossible. We consider the three real \pet data sets shown
in figure~\ref{fig:typical real pet data}. When using adaptive proposal
scales, the average accept rates of the four random walk blocks (for
parameters $\phi_{1:r}$, $\theta_{1:r}$, $\tau$ and $\nu$, respectively), is
shown in figure~\ref{fig:pet adaptive proposal}. Though the results are not
close to the optimal value $0.234$, they are more than acceptable. In
contrast, in figure~\ref{fig:pet fixed proposal} we shown the average accept
rates when using a scheme of proposal scales which is obtained from another
typical real \pet data set. The scheme is tuned such that for all parameters,
the accept rates falls within the range $[0.2, 0.4]$ for $\alpha_t \in [0,
1]$. It can be seen that, not only the results varies considerably due to the
variety of the data sets, which is expected, for one of the data set, the
parameter $\nu$ fails to move at all. Considering that there are about a
quarter million such data sets to be estimated in a single \pet scan,
adaptively specifying the proposal scales is the only vital approach. Note
that, this problem is not unique to the \pet compartmental models at all. In
many realistic applications, there are a large number of data sets varies
considerably from each other.

%A simpler alternative is based on the observation that in many situations,
%especially in Bayesian model comparison context (for example using a \smc2
%sampler for computing model evidences), the sampler starts with a distribution
%(e.g., the prior) whose parameters have large variances and moves towards a
%highly concentrated distribution (e.g., the posterior). Therefore it is
%sensible to have the proposal scales decays sequentially. One scheme to
%construct such a decay is having the proposal scale, say $\sigma$ decays in
%such a form $\sigma = \sigma^\star(b / (a + b\alpha))^p$ where $\sigma^\star$, $a$,
%$b$ and $p$ are constants controlling the speed of and end points of the decay
%and $\alpha$ is the schedule parameter which lies in $[0,1]$ (e.g., the
%$\alpha(t/T)$ in the previous geometric annealing scheme). The constants shall
%be chosen such that the accept rates of the random walk are controlled in an
%acceptable range for all distributions. Practically, one can afford using a
%relatively smaller number of particles and modest number of distributions as a
%pilot run of the algorithm. Using the pilot sampler to tune the constants.
%This simple alternative does indeed require some effort for choosing the
%constants. The benefit is one does not need to tune the proposal scales for
%many iterations, yet avoid the possible situation that with a fully automatic
%adaptation, the accept rates are not always satisfactory.
%
%In the special case of conjugate Normal prior, that is for data $\data$ and a
%parameter of interest $\mu$, $\data|\mu \sim \calN(\mu,\lambda^{-1})$ and
%$\mu \sim \calN(\xi, \kappa^{-1})$ and a geometric annealing scheme, namely
%$\pi_t(\mu)\propto\pi(\mu)[p(\data|\mu)]^{\alpha(t/T)}$, it is well known that
%at time $t$, $\pi_t(\mu)$ is a normal density with variance $(\kappa +
%n\lambda\alpha)^{-1}$ where $n$ is the number of data points. Therefore if we
%follow \cite{Jasra:2010eh} and use the parameter variance as proposal scale,
%the above scheme indeed produce the ``optimal'' scales with the constants
%properly chosen. In more general Bayesian modeling cases, with large amount of
%data, the posterior distribution are asymptotically Normal. This observation
%suggest more general usefulness of the above deterministic scheme.  Other
%specification of the decay is also possible. In section~\ref{sec:Illustrative
%  Applications} we demonstrate the effectiveness of this scheme empirically.
%
%Adaptive schemes for either a random walk or an \mala algorithm are
%conceptually appealing and require little computational cost and manual
%tuning. However in practice the results may not be satisfactory and the
%performance can depend on the type of the random walks. In contrast the
%deterministic automatic schemes give users more control of the tuning though
%the choice of the constants may not be obvious in interesting problems.

\section{An automatic, generic framework}
\label{sec:An automatic, generic framework}

With the above refinements, we are ready to implement the \smc2 algorithm
with minimal tuning and application-specific effort while providing robust and
accurate estimates of the model evidence $p(\data|\Mk)$. First the geometric
annealing scheme that connects the prior $\pi(\paramk|\Mk)$ and the posterior
$\pi(\paramk|\data,\Mk)$, provides a smooth path for a wide range of problems.

Second, the actual annealing schedule under this scheme can be determined
through the adaptive schedule as described above. The advantage of the
adaptive schedule will be shown empirically later.

Third, we can adaptively specify the Metropolis random walk (or \mala) scales
through the estimation of their scaling parameters as the sampler iterates. In
contrast to the \mcmc setting, where such adaptive algorithms will usually
require a burn-in period, which will not be used for further estimation, in
\smc, the variance and covariance estimates come at almost no cost, as all the
samples will later be used for marginal likelihood estimation. Additionally,
adaptation within \smc does not require separate theoretical justification --
something which can significantly complicate the development of effective,
theoretically justified schemes in the \mcmc setting. Alternatively, we can
also specify the proposal scales in a deterministic, but sensible way. Since
\smc algorithms are relatively robust to the change of scales, such
deterministic scales will not require the same degree of tuning as is required
to obtain good performance in \mcmc algorithms.

The adaptive strategy can be applied to both algorithms directly. The
applicability to the \smc3 algorithm depends on the nature of the sequence
of distributions. We outline the strategy in Algorithm~\ref{alg:adaptive}.

\begin{algorithm}
\begin{algorithmic}
  \STATE \emph{Accuracy control}
  \STATE\STATESKIP Set constant $\cess^\star\in(0,1)$, using a small pilot
  simulation if necessary.

  \STATE \emph{Initialization:} Set $t\leftarrow0$.
  \STATE\STATESKIP Perform the \emph{Initialization} step
    as in Algorithm~\ref{alg:smc1} or~\ref{alg:smc2}

  \STATE \emph{Iteration:} Set $t\leftarrow t + 1$

  \STATE\STATESKIP \emph{Step size selection}
  \STATE\STATESKIP\STATESKIP Use a binary search %(or other search algorithms)
  to find $\alpha^\star$ such that $\cess_{\alpha^\star} = \cess^\star$
  \STATE\STATESKIP\STATESKIP Set $\alpha_t \leftarrow\alpha^\star$ if
    $\alpha^\star \le 1$, otherwise set $\alpha_t\leftarrow1$

  \STATE\STATESKIP \emph{Proposal scale calibration}
  \STATE\STATESKIP\STATESKIP
  Computing the importance sampling estimates of first two moments of
  parameters.
  \STATE\STATESKIP\STATESKIP
  Set the proposal scale of the Markov proposal $K_t$ with the estimated
  parameter variances.

  \STATE\STATESKIP Perform the \emph{Iteration} step as in
  Algorithm~\ref{alg:smc1} or~\ref{alg:smc2} with the found $\alpha_t$
  and proposal scales.

  \STATE \emph{Repeat} the \emph{Iteration} step %up to $t = T$ (or $T_k$ in the
    %case of Algorithm~\ref{alg:smc2})}
    \emph{until $\alpha_t = 1$} then set $T=t$.
\end{algorithmic}
\caption{An Automatic, Generic Algorithm for Bayesian Model Comparison}
\label{alg:adaptive}
\end{algorithm}

As laid out above, the algorithms require minimal tuning. Its robustness,
accuracy and efficiency will be shown in section~\ref{sec:Illustrative
  applications} through comprehensive empirical studies. Here we shown some
interesting yet intuitively expected results. We consider the simulated \pet
data set, using a \smc2 sampler with $1000$ particles. It is already shown
that using either the adaptive specification of distribution placement or the
\mcmc proposal scales can give better results. The combination of the two can
lead to even better results. The use \cess-based schedule scheme will not only
place more distributions where the target distributions changes more rapidly,
but also it will place more distributions where the \mcmc algorithm mixes
slower. To illustrate the idea, we consider four configurations of the
sampler. The proposal scales are specified either adaptively or using a fixed
scheme which is manually tuned. The placement of the distributions is either
\cess-based or using a schedule such that $\alpha(t/T_k) = (t/T_k)^5$.

\begin{table}[t]
  \begin{tabularx}{\linewidth}{lXX}
    \toprule
    & \multicolumn{2}{c}{Proposal scales} \\
    \cmidrule(lr){2-3}
    Schedule & Fixed & Adaptive \\
    \midrule
    Quadratic & $1.6\pm0.27$ & $1.6\pm0.22$ \\
    \cess     & $1.6\pm0.19$ & $1.6\pm0.15$ \\
    \bottomrule
  \end{tabularx}
  \caption{Bayes factor estimates ($\log B_{1,2}\pm\sd$)
    for the simulated \pet data set using \smc2 algorithm. All four samplers
    are configured such that about $200$ distributions are used.}
  \label{tab:pet four sampler same dist}
\end{table}

The results are shown in table~\ref{tab:pet four sampler same dist}. More
comprehensive results can be found in section~\ref{sec:Illustrative
  applications}. However, from this particular example, we can see that the
combination of the two adaptive schemes provides superior performance at very
little computation cost. Table~\ref{tab:pet four sampler same iter} shows
actually equivalent results. Instead of fixing the number of distributions, we
configured the samplers such that they all give roughly the same precision of
the estimates. It is rather obvious to see that the use of adaptive methods
actually give a considerable reduction of computational cost for a given
desired precision of the estimates.

\begin{table}[t]
  \begin{tabularx}{\linewidth}{lXX}
    \toprule
    & \multicolumn{2}{c}{Proposal scales} \\
    \cmidrule(lr){2-3}
    Schedule & Fixed & Adaptive \\
    \midrule
    Quadratic & $200$ & $182$ \\
    \cess     & $175$ & $157$ \\
    \bottomrule
  \end{tabularx}
  \caption{Number of distributions used for the simulated \pet data set using
    \smc2 algorithm. All four samplers are configured such that the Bayes
    factor estimates ($\log B_{1,2}$) have a standard deviation $0.27$.}
  \label{tab:pet four sampler same iter}
\end{table}

We shall also note that \smc1 is less straightforward as the between model
moves still require effort to design and implement. In \smc3, the
specification of the sequences between posterior distributions are less
generic compared to the geometric annealing scheme in \smc2. However, the
adaptive schedule and automatic tuning of \mcmc proposal scales, both can be
applied in these two algorithms in principal.

Although further enhancements and refinements are clearly possible, we focus
in the remainder of this article on this simple, generic algorithm which can
be easily implemented in any application and has proved sufficiently powerful
to provide good estimation in the examples we have encountered thus far.

\section{Illustrative applications}
\label{sec:Illustrative applications}

In this section, we will use three examples to illustrate the algorithms. The
Gaussian mixture model is discussed first, with implementations for all three
\smc algorithms with comparison to \rjmcmc and \pmcmc. It will be shown that
all five algorithms agree on the results while the performance in
terms of Monte Carlo variance varies considerably. It will also be
demonstrated how the adaptive refinements of the algorithms behaves in
practice. We will reach the conclusion that considering ease of
implementation, performance and generality, the \smc2 algorithm is most
promising among all three strategies.

Then two more realistic examples, a nonlinear \ode model and a Positron
Emission Tomography compartmental model are used to study the performance and
robustness of algorithm \smc2 compared to \ais and \pmcmc. Various
configurations of the algorithms are considered including both sequential and
parallelized implementations.

The \texttt{C++} implementations, which make use of the vSMC library of
\cite{software:VSMC}, of all examples can be found at
\url{https://github.com/zhouyan/vSMC}.

\subsection{Gaussian Mixture Model}
\label{sub:Gaussian Mixture Model}

Since \citet{Richardson:1997ea}, the Gaussian mixture model (\gmm) has
provided a canonical example of a model-order-determination problem. We use
the model formulation of \citet{DelMoral:2006hc} to illustrate the efficiency
and robustness of the methods proposed in this paper compared to other
approaches.  The model is as follows; data $\data = (y_1,\dots,y_n)$ are
independently and identically distributed as
\begin{equation*}
  y_i|\theta_r \sim \sum_{j=1}^r \omega_j\calN(\mu_j,\lambda_j^{-1})
\end{equation*}
where $\calN(\mu_j,\lambda_j^{-1})$ denotes the Normal distribution with
mean $\mu_j$ and precision $\lambda_j$; $\theta_r =
(\mu_{1:r},\lambda_{1:r},\omega_{1:r})$ and $r$ is the number of components
in each model. The parameter space is thus $\Real^r \times (\Real^{+})^r\times
\Delta_r$ where $\Delta_r = \{\omega_{1:r}:0\le\omega_j\le1;
\sum_{j=1}^r\omega_j=1\}$ is the standard $r$-simplex. The priors which are
the same for each component are taken to be
$\mu_j\sim\calN(\xi,\kappa^{-1})$, $\lambda_j\sim\calGa(\nu,\chi)$ and
$\omega_{1:r}\sim\calD(\rho)$ where $\calD(\rho)$ is the symmetric Dirichlet
distribution with parameter $\rho$ and $\calGa(\nu,\chi)$ is the Gamma
distribution with shape $\nu$ and scale $\chi$. The prior parameters are set
in the same manner as in \citet{Richardson:1997ea}. Specifically, let $\ymin$
and $\ymax$ be the minimum and maximum of data $\data$, the prior parameters
are set such that
\begin{equation*}
  \xi = (\ymax + \ymin) / 2, \quad \kappa = (\ymax - \ymin)^{-2}, \quad
  \nu = 2, \quad \chi = 50\kappa, \quad \rho = 1
\end{equation*}
The data is simulated from a four components model with $\mu_{1:4} = (-3, 0,3,
6)$, and $\lambda_j =2$, $\omega_j = 0.25$, $j = 1,\dots,4$.

We consider several algorithms. First the \rjmcmc algorithm as in
\citet{Richardson:1997ea}, and second an implementation of the \smc1
algorithm. Next \ais, \pmcmc and \smc2 are used for within-model
simulations. The last is an implementation of the \smc3 algorithm. In all
the algorithms, the local move which does not change the dimension of the
model is constructed as a composition of Metropolis-Hastings random walk
kernels:
\begin{enumerate}
  \item Update $\mu_{1:r}$ using a multivariate Normal random walk proposal.
  \item Update $\lambda_{1:r}$ using a multivariate Normal random walk on
    logarithmic scale, i.e., on $\log\lambda_{j}$, $j = 1, \dots, r$.
  \item Update $\omega_{1:r}$ using a multivariate Normal random walk on logit
    scale, i.e., on $\omega_{j}/\omega_r$, $j = 1,\dots,r-1$.
\end{enumerate}
The \rjmcmc, \smc1 and \smc3 algorithms use two additional
reversible jump moves. The first is a combine and split move; the second is a
birth and death move. Both are constructed in the same manner as in
\citet{Richardson:1997ea}. Also in these implementations, an adjacency
condition was imposed on the means $\mu_{1:r}$, such that $\mu_1 < \mu_2 <
\dots < \mu_r$. No such restriction was used for other algorithms.

In \smc1, \smc2 and \pmcmc implementations, the distributions are chosen
with a geometric schedule, i.e., as in Equation~\eqref{eq:geometry_1} for
\smc1 and Equation~\eqref{eq:geometry_2} for the other two. This annealing
scheme has been used in \citet{DelMoral:2006hc,Jasra:2007in} and many other
works. The geometric scheme can also be seen in \citet{Calderhead:2009bd} for
\pmcmc tempering. A schedule $\alpha(t/T) = (t/T)^p$, with $p = 2$ was used.
The rationale behind this particular schedule can be seen in
\citet{Calderhead:2009bd} and other values of $p$ were also tried while
$p\approx2$ performs best in this particular example. The adaptive schedule
was also implemented for \smc2 and \ais algorithms.

The proposal scales for each block of the random walks are specified
dynamically according to values of $\alpha(t/T)$ for the \smc2 and \ais
algorithms and also manually tuned for other algorithms such that the
acceptance rates fall in $[0.2, 0.5]$. Later for the \smc2 and \ais
algorithms, we also consider adaptive schedule of the distribution
specification parameter $\alpha(t/T)$ and the proposal scales of the random
walks.

For \smc2, \smc3 and \ais we consider both the direct estimator and the
path sampling estimator. For \pmcmc we consider the path sampling estimator.

\subsubsection{Results}\label{sec:gmm_res}

The \smc1 implementation uses $10^4$ particles and $500$ distributions.
The \rjmcmc implementation uses $5\times10^6$ iterations in addition to
$10^6$ iterations of burn-in period. The resulting estimates of model
probabilities are shown in Table~\ref{tab:gmm-rj}.

\begin{table}
  \begingroup\small\begin{tabularx}{\linewidth}{XXccccccc}
    \toprule
    & & \multicolumn{7}{c}{Number of components} \\
    \cmidrule(lr){3-9}
    Algorithm & Quantity & $\le2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $\ge8$ \\
    \midrule
    \smc1  & $\Prob(M = \Mk)$
    & $0$ & $0.00257$ & $0.886$ & $0.103$ & $0.00715$ & $0.00128$ & $0$ \\
    & $\log B_{4,k}$
    & $\infty$ & $5.84$ & $0$ & $2.15$ & $4.82$ & $6.54$ & $\infty$ \\
    \rjmcmc  & $\Prob(M = \Mk)$
    & $0$ & $0.000526$ & $0.887$ & $0.103$ & $0.00623$ & $0.00324$ & $0$ \\
    & $\log B_{4,k}$
    & $\infty$ & $6.56$ & $0$ & $2.15$ & $4.96$ & $5.61$ & $\infty$ \\
    \bottomrule
  \end{tabularx}\endgroup
  \caption{Gaussian mixture model estimates obtained via \smc1 and \rjmcmc}
  \label{tab:gmm-rj}
\end{table}

The \smc2, \smc3 and \ais implementations use $1,000$ particles and
$500$ iterations. The \pmcmc implementation uses $50$ chains and $10^4$
iterations in addition to $10^4$ iterations of burn-in period --- these
implementations have approximately equal computational costs. From the
results obtained under the \smc1 and \rjmcmc algorithms it is clear that, in this
particular example, simulations for models with fewer than ten components are
adequate to characterize the model space. Therefore, under this
configuration, the cost is roughly the same in terms of computational
resources as that of the \smc1 and \rjmcmc algorithms. From the results of
\rjmcmc and \smc1, we consider four and five components models (i.e., the
true model and the most competitive amongst the others). The estimates are shown in
Table~\ref{tab:gmm-pair}  which, like all of the other tables in this section,
summarises the Monte Carlo variability of 100 replicate runs of each algorithm..

\begin{table}
  \begingroup\small\begin{tabularx}{\linewidth}{lXXXXXXX}
    \toprule
    & \multicolumn{7}{c}{Algorithms} \\
    \cmidrule(lr){2-8}
    Quantity
    & \smc2-\ds & \smc2-\ps & \smc3-\ds & \smc3-\ps & \ais-\ds &
    \ais-\ps & \pmcmc \\
    \midrule
    $\log B_{4,5}$
    & $2.15$ & $2.15$ & $2.16$ & $2.21$ & $2.16$ & $2.17$ & $2.63$ \\
    \sd
    & $0.25$&$\color{red}{\bf 0.22}$ & $0.61$&$0.62$ & $1.12$&$1.10$ & $0.41$
    \\
    \bottomrule
  \end{tabularx}\endgroup
  \caption{Gaussian mixture model estimates obtained via \smc2, \smc3, \ais and \pmcmc}
  \label{tab:gmm-pair}
\end{table}

From Tables~\ref{tab:gmm-rj} and~\ref{tab:gmm-pair}, it can be seen that the
standard estimators (\rjmcmc, \smc1, \smc2-\ds, \smc3-\ds and
\ais-\ds) agree with each other. Among the path sampling estimators,
\smc2-\ps and \ais-\ps have little bias. \smc3-\ps shows a little more
bias. The \pmcmc algorithm has a considerable larger bias as the number of
distributions is relatively small (as noted previously, a larger number will
negatively affect the mixing speed).

In terms of Monte Carlo variance, in Table~\ref{tab:gmm-pair}, \smc2
clearly has an advantage compared to its no-resampling variant, \ais. The
differences of Monte Carlo \sd between \smc2, \smc3 and \pmcmc,
although they do not affect model selection in this particular example, are
considerable.

\paragraph{Effects of resampling} It is clear from these results that
resampling (when required) can substantially improve the estimation of
normalising constants within an \smc framework. This doesn't contradict the
statement in \cite{DelMoral:2006hc} which suggests that resampling may not
much help when the normalising constant is the object of interest: the
theoretical argument which supports this relies upon the assumption that the
Markov kernel used to mutate the particles mixes extremely rapidly and the
result is obtained under the assumption that resampling is performed after
every iteration. When the Markov kernel is not so rapidly mixing, the
additional stability provided by the resampling operation can outweight the
attendant increase in Monte Carlo variance and that is what we observed here
(and in the case of the other examples considered below; results not shown).

\paragraph{Effects of adaptive schedules} To assess the evolution of
distributions with an adaptive schedule, we consider the relation between
$\alpha_t - \alpha_{t-1}$ and $\alpha_t$. As stated before, one of the
motivations of using \cess for adaptive placement of distribution is to
ensure that $\alpha_t$ evolves the same path regardless the resampling
strategies. Figure~\ref{fig:adaptive_alpha} shows the evolution of $\alpha_t$
when fixing \ess or \cess and resampling every iteration or only when $\ess <
N/2$. As shown in the plot, when fixing \cess, the evolution of the
distributions is not affected by the resampling strategy. In contrast, fixing
\ess yields a sequence of distributions which depends strongly upon the
resampling strategy.

% Recall that, the purpose of using an adaptive schedule is to construct a
% ``smooth'' sequence of distributions. Though the precise meaning of
% ``smoothness'' may vary from one application to another and both changes in
% \ess and \cess can be reasonably used as measures of it, it shall note that
% at very least, this is a property of the sequence of the distributions
% regardless of the resampling strategies. From this perspective, using is
% \cess is a more plausible approach.

% In terms of the actual performance when using the \cess adaptive strategy
% in \smc2 algorithm, a reduction of standard deviation of 20\% was
% observed comparing to $\alpha(t/T) = t/T$ and very close to the carefully
% hand-chosen $\alpha(t/T) = (t/T)^2$. When applied to the \smc3
% algorithm, 50\% reduction was observed. If the \ess adaptive strategy is
% used instead, similar standard deviation reduction is observed when
% resampling is performed every iteration but no significant effect was
% observed when resampling was only performed when $\ess < N/2$.

In terms of the actual performance when using the \cess adaptive strategy in
the \smc2 and \ais algorithms, a reduction of standard deviation of 20\% was
observed comparing to $\alpha(t/T) = (t/T)^2$, the one shown in
Table~\ref{tab:gmm-pair}. When applied to the \smc3 algorithm, 50\%
reduction was observed. If the \ess adaptive strategy is used instead,
similar standard deviation reduction is observed when resampling is performed
every iteration but no significant effect was observed when resampling
was only performed when $\ess < N/2$ (i.e., using \ess rather than \cess
entirely eliminated the benefit).

\paragraph{Effects of adaptive proposal scales} When using the \smc2
algorithm, if the adaptive strategy of \citet{Andrieu:2006tw} is applied,
where the important sampling estimates of the variance of parameters are
included in the adaptation, the acceptance rates fall within $[0.2, 0.5]$
dynamically without manual tuning as for the results in
Table~\ref{tab:gmm-pair}. It should be noted that in this particular example,
it is the variance of $\log\lambda_i$ being estimated as the corresponding
random walk block operates on the log scale.  The same principle applies to
the weight parameters, which have random walks on logit scale.
Approximately 20\% reduction in standard deviation was observed.

% However, there are no significant improvement in terms of reduction of
% estimator variances. This can be viewed as evidence that \smc algorithms
% are more robust to the change of proposal scales.

\subsection{Nonlinear Ordinary Differential Equations}

The example from the previous section suggests that \smc2 performs well
relative to the other \smc possibilities. Given the wide range of settings
in which it can be easily deployed, we will now concentrate
further on this method. It also suggests that in the simple case of Gaussian
mixtures, a complete adaptive strategy for both distribution specification
and proposal scales works well. In this section, this will now be further
explored in a more complex model, a nonlinear ordinary differential equations
system.  This model, which was studied in \cite{Calderhead:2009bd}, is known
as the Goodwin model. The \ode system, for an $m$-component model, is:
\begin{align*}
  \frac{\diff X_1(t)}{\diff t} &= \frac{a_1}{1 + a_2 X_m(t)^{\rho}}
  - \alpha X_1(t)  & \\
  \frac{\diff X_i(t)}{\diff t} &= k_{i-1}X_{i-1}(t) - \alpha X_i(t)
  & i = 2,\dots,m \\
  X_i(0) &= 0 & i = 1,\dots,m
\end{align*}
The parameters $\{\alpha,a_1,a_2,k_{1:m-1}\}$ have common prior distribution
$\calGa(0.1, 0.1)$.  Under this setting, $X_{1:m}(t)$ can exhibit either
unstable oscillation or a constant steady state. The data are simulated for
$m=\{3,5\}$ at equally spaced time points from $0$ to $60$, with time step
$0.5$. The last $80$ data points of $(X_1(t), X_2(t))$ are used for inference.
Normally-distributed noise with standard  deviation $\sigma=0.2$ is added to
the simulated data. Following \cite{Calderhead:2009bd}, the variance of the
additive measurement error is assumed to be known. Therefore, the posterior
distribution has $m+2$ parameters for an $m$-component model.

As shown in \cite{Calderhead:2009bd}, when $\rho > 8$, due to the possible
instability of the \ode system, the posterior can have a considerable number
of local modes. In this example, we set $\rho = 10$. Also, as the solution to
the \ode system is somewhat unstable, slightly different data can result in
very different posterior distributions.

\subsubsection{Results}

We compare results from the \smc2 and \pmcmc algorithms. For the \smc
implementation, $1,000$ particles and $500$ iterations were used, with the
distributions specified specified by Equation~\eqref{eq:geometry_2}, with
$\alpha(t/T) = (t/T)^5$, or via the completely adaptive specification.  For
the \pmcmc algorithm, $50,000$ iterations are performed for burn-in and
another $10,000$ iterations are used for inference. The same tempering as was
used for \smc is used here. Note that, in a sequential implementation of \pmcmc, with each
iteration updating one local chain and attempting a global exchange, the
computational cost of after burn-in iterations is roughly the same as the
entire \smc algorithm. In addition, changing $T$ within the range of the
number of cores available does not substantially change the
computational cost of a generic parallel implementation of the \pmcmc
algorithm. We compare results from $T = 10,30,100$.

The results for data generated from the simple model ($m = 3$) and complex
model ($m = 5$), again summarising variability amongst 100 runs of each
algorithm, are shown in Table~\ref{tab:node-s-all} and~\ref{tab:node-c-all},
respectively.

\input{tab/node-s}
\input{tab/node-c}

As shown in both cases, the number of distributions can affect the
performance of \pmcmc algorithms considerably. When using $10$ distributions,
large bias from numerical integration for path sampling estimator was
observed, as expected. With $30$ distributions, the performance is comparable
to the \smc2 sampler, though some bias is still observable. With $100$
distributions, there is a much larger variance because, with more chains, the
information travels more slowly from rapidly mixing chains to slowly mixing ones and
consequently the mixing of the overall system is inhibited.

The \smc algorithm provides results comparable to the best of
three \pmcmc implementations in essentially all settings, including one in
which both the annealing schedule and proposal scaling were fully
automatic. In fact, the completely adaptive strategy was the most successful.

It can be seen that increasing the number of distributions not only reduces
the bias of numerical integration for path sampling estimator, but also
reduces the variance considerably. On the other hand increasing the number of
particles can only reduce the variance of the estimates, in accordance with
the central limit theorem (see \citet{DelMoral:2006hc} for the standard
estimator and extensions for path sampling estimator,
Proposition~\ref{prop:path_clt}) (as the bias arises from the numerical
integration scheme). Therefore, though there exists the trade-off between the
number of particles and the number distributions, increasing either of them
can always benefit the accuracy of the estimators.

\subsection{Positron Emission Tomography Compartmental Model}
\label{sub:Positron Emission Tomography Compartmental Model}

It is now interesting to compare the proposed algorithm with other
state-of-art algorithms using the more realistic \pet compartmental model
example.

Real neuroscience data sets involve a very large number ($\sim200,000$ per
brain) of time series, which are typically somewhat heterogeneous.
Figure~\ref{fig:petplot} shows estimates of $V_D =
\sum_{j=1}^m\phi_j/\theta_j$ from a typical \pet scan (generated using \smc2
as will be discussed later). Robustness is therefore especially important. An
application-specific \mcmc algorithm was developed for this problem in
\citet{Zhou2013}. A significant amount of tuning of the algorithms was
required to obtain good results.The results shown in Figure~\ref{fig:petplot}
are very close to those of \citet{Zhou2013} but, as is shown later, they were
obtained with almost no manual tuning effort and at similar computational
cost.

For \smc and \pmcmc algorithms, the requirement of robustness means that the
algorithm must be able to calibrate itself automatically to different data
(and thus different posterior surfaces). A sequence of distributions which
performs well for one time series may not perform even adequately for another
series.  Specification of proposal scales that produces fast-mixing kernels
for one data series may lead to slow mixing for another. In the following
experiment, we will use a single simulated time series, and choose schedules
that performs both well and poorly for this particular time series. The
objective is to see if the algorithm can recover from a relatively poorly
specified schedule and obtain reasonably accurate results.

\subsubsection{Results}

In this example we focus on the comparison between \smc2 and \pmcmc. We also
consider parallelized implementations of algorithms. In this case, due to its
relatively small number of chains, \pmcmc can be parallelized completely (and
often cannot fully utilize the hardware capability if a na\"\i ve approach to
parallelization is taken; while we appreciate that more sophisticated
parallelization strategies are possible, these depend instrinsicially upon the
model under investigation and the hardware employed and given our focus on
automatic and general algorithms, we don't consider such strategies here). The
\pmcmc algorithm under this setting is implemented such that each chain is
updated at each iteration. Further, for the \smc algorithms, we consider two
cases. In the first we can parallelize the algorithm completely (in the sense
that each core has a single particle associated with it). In this setting we
use a relatively small number of particles and a larger number of time steps.
In the second, we need a few passes to process a large number of particles at
each time step, and accordingly we use fewer time steps to maintain the same
total computation time. These two settings allow us to investigate the
trade-off between the number of particles and time steps. In both
implementations, we consider three schedules, $\alpha(t/T) = t/T$ (linear),
$\alpha(t/T) = (t/T)^5$ (prior), and $\alpha(t/T) = 1 - (1 - t/T)^5$
(posterior). In addition, the adaptive schedule based upon \cess is also
implemented for the \smc2 algorithm.

Results from 100 replicate runs of the two algorithms under various regimes can be found in
Table~\ref{tab:pet-py-par-sel} and~\ref{tab:pet-bf-par-sel} for the marginal
likelihood and Bayes factor estimates, respectively. The \smc algorithms
consistently outperforms the \pmcmc algorithms in the parallel settings. The
Monte Carlo \sd of \smc algorithms is typically of the order of one fifth of
the corresponding estimates from \pmcmc in most scenarios. In some settings
with the smaller number of samples, the two algorithms can be comparable.
Also at the lowest computational costs, the samplers with more time steps and
fewer particles outperform those with the converse configuration by a fairly
large margin in terms of estimator variance. It shows that with limited
resources, ensuring the similarity of consecutive distributions, and thus good
mixing, can be more beneficial than a larger number of particles. However,
when the computational budget is increased, the difference becomes negligible.
The robustness of \smc to the change of schedules is again apparent.

\input{tab/pet-py-par-sel-ja}
\input{tab/pet-bf-par-sel-ja}

\paragraph{Effects of adaptive schedule}

A set of samplers with adaptive schedules are also used. Due to the nature of
the schedule, it cannot be controlled to have exactly the same number of time
steps as non-adaptive procedures.  However, the \cess was controlled such that
the average number of time steps are comparable with the fixed schedules and
in most cases slightly less than the fixed numbers.

It is found that, with little computational overhead, adaptive schedules do
provide the best results (or very nearly so) and do so without user
intervention. The reduction of Monte Carlo \sd varies among different
configurations. For moderate or larger number of distributions, a reduction
about 50\% was observed. In addition, it shall be noted that, in this example,
the bias of the path sampling estimates are much more sensitive to the
schedules than the previous Gaussian mixture model example. A vanilla linear
schedule does not provide a low bias estimator at all even when the number of
distributions is increased to a considerably larger number. The prior schedule
though provides a nearly unbiased estimator, there is no clear theoretical
evidence showing that this shall work for other situations. %Even it has more
%general usage, as suggested in \citet{Calderhead:2009bd}, the power still has
%to be chosen (in the previous \gmm example, $p = 2$ was the best choice while
%in this \pet example $p = 5$ is more suitable). In contrast,
The adaptive schedule, without any manual calibration, can provide a nearly unbiased
estimator, even when path-sampling is employed, in addition to potential
variance reduction.

\paragraph{Bias reduction for path sampling estimator}

As seen in Table~\ref{tab:pet-py-par-sel} and~\ref{tab:pet-bf-par-sel}, a bad
choice of schedule $\alpha(t/T)$ can results in considerable bias for the
basic path sampling estimator, here for \smc2-\ps but the problem is
independent of the mechanism by which the samples are obtained. Increasing the number of iterations can reduce
this bias but at the cost of additional computation time. As outlined in
Section~\ref{ssub:Improved Univariate Numerical Integration}, in the case of
the \smc algorithms discussed here, it is possible
to reduce the bias without increasing computational cost significantly. To
demonstrate the bias reduction effect, we constructed \smc sampler for the
above \pet example with only $1,000$ particles and about $20$ iterations
specified using the \cess based adaptive strategy. The path sampling estimator was
approximated using Equation~\eqref{eq:path_est} as well as other higher order
numerical integration or by integrating over a grid that contains
$\{\alpha_t\}$ at which the samples was generated. The results are shown in
Table~\ref{tab:pet-py-bias-reduction}

\begin{table}
  \begin{tabularx}{\linewidth}{lXXXX}
    \toprule
    & \multicolumn{4}{c}{Number of grid points (compared to sampled iterations)} \\
    \cmidrule(lr){2-5}
    Integration rule & $\times1$ & $\times2$ & $\times4$ & $\times8$ \\
    \midrule
    Trapezoid
    & $-52.2\pm5.01$ & $-45.5\pm1.93$ & $-42.1\pm1.21$ & $-40.5\pm1.06$ \\
    Simpson
    & $-43.2\pm1.39$ & $-41.0\pm1.10$ & $-40.0\pm1.04$ & $-39.4\pm1.04$ \\
    Simpson $3/8$
    & $-42.1\pm1.21$ & $-40.5\pm1.06$ & $-39.7\pm1.04$ & $-39.3\pm1.04$ \\
    Boole
    & $-40.9\pm1.09$ & $-39.9\pm1.04$ & $-39.4\pm1.04$ & $-39.2\pm1.05$ \\
    \bottomrule
  \end{tabularx}
  \caption{Path sampling estimator of marginal likelihood of two components
    \pet model. The estimator was approximated using samples from \smc2
    algorithm with $1,000$ particles and $20$ iterations, with different
    numerical integration strategies. Large sample result (see
    Table~\ref{tab:pet-py-par-sel}) provide an estimate of
    $-39.2$.}
  \label{tab:pet-py-bias-reduction}
\end{table}

\paragraph{Real data results}

Finally, the methodology of \smc2-\ps was applied to measured positron
emission tomography data using the same compartmental setup as in the
simulations. The data shown in Figure~\ref{fig:petplot} comes from a study
into opioid receptor density in Epilepsy, with the data being described in
detail in \cite{Jiang:2009kf}.  It is expected that there will be considerable
spatial smoothness to the estimates of the volume of distribution, as this is
in line with the biology of the system being somewhat regional. Some regions
will have much higher receptor density while others will be much lower,
yielding higher and lower values of the volume of distribution, respectively.
While we did not impose any spatial smoothness but rather estimated the
parameters independently for each time series at each spatial location, as can
be seen, smooth spatial estimates of the volume of distribution consistent
with neurological understanding were found using the approach. This method is
computationally feasible for the entire brain on a voxel-by-voxel basis, due
to the ease of parallelization of the \smc algorithm. In the analysis
performed here, 1000 particles were used, along with an adaptive schedule
using a constant $\cess^\star = 0.999$, resulting in about 180 to 200
intermediate distributions. The model selection results are very close to
those obtained by a  previous study of the same data \citep{Zhou2013},
although the present approach requires much less implementation effort and has
roughly the same computational cost.

\subsection{Summary}

These three illustrative applications have essentially shown three aspects of
using \smc as a generic tool for Bayesian model selection. Firstly, as seen in
the Gaussian mixture model example, all the different variants of \smc
proposed, including both direct and path sampling versions, produce results
which are competitive with other model selection methods such as \rjmcmc and
\pmcmc. In addition, in this somewhat simple example, \smc2 performs well,
and leads to low variance estimates with no appreciable bias. The effect
of adaptation was studied more carefully in the nonlinear \ode example, and it
was shown that using both adaptive selection of distributions as well as
adaptive proposal variances leads to very competitive algorithms, even against
those with significant manual tuning. This suggests that an automatic process
of model selection using \smc2 is possible. In the final example,
considering the easy parallelization of algorithms such as \smc2 suggests
that great gains in variance estimation can be made using settings such as
\gpu computing for application where computational resources are of
particular importance (such as in image analysis as in the PET example). It is
also clear that the negligible cost of the bias reduction techniques described
means that one should always consider using these to reduce the bias inherent
in path sampling estimation.

\section{Discussion}
\label{sec:Bayesian SMC discussion}

It has been shown that \smc is an effective Monte Carlo method for Bayesian
inference for the purpose of model comparison. Three approaches have been
outlined and investigated in several illustrative applications including the
challenging scenarios of nonlinear \ode models and \pet compartmental
systems. The proposed strategy is always competitive and often substantially
outperforms the state of the art in this area.

It has been demonstrated that it is possible to use the \smc algorithms to
estimate the model probabilities directly (\smc1), or through individual
model evidence (\smc2), or pair-wise relative evidence (\smc3). In
addition, both \smc2 and \smc3 algorithms can be coupled with the path
sampling estimator.

Among the three approaches, \smc1 is applicable to very general settings. It
can provide a robust alternative to \rjmcmc when inference on a countable
collection of models is required (and could be readily combined with the
approach of \cite{Jasra:2008bb} at the expense of a little additional
implementation effort). However, like all Monte Carlo methods involving
between model moves, it can be difficult to design efficient algorithms in
practice.  The \smc3 algorithm is conceptually appealing. However, the
existence of a suitable sequence of distributions between two posterior
distributions may not be obvious.

The \smc2 algorithm, which only involves within-model simulation, is most
straightforward to implement in many interesting problems. It has been shown
to be exceedingly robust in many settings. As it depends largely upon a
collection of within-model \mcmc moves, any existing \mcmc algorithms can be reused in the
\smc2 framework. However, much less tuning is required because the algorithm
is fundamentally less sensitive to the mixing of the Markov kernel and it is
possible to implement effective adaptive strategies at little computational
cost. With adaptive placement of the intermediate distributions and
specification of the \mcmc kernel proposals, it provides a robust and essentially
automatic model comparison method.

Compared to the \pmcmc algorithm, \smc2 has greater flexibility in the
specification of distributions. Unlike \pmcmc, where the number and placement
of distributions can affect the mixing speed and hence performance
considerably, increasing the number of distributions will always benefit a \smc
sampler given the same number of particles. When coupled with a path
sampling estimator, this leads to less bias and variance. Compared to its
no-resampling variant, it has been shown that \smc samplers with resampling
can reduce the variance of normalizing constant estimates considerably.

Even after three decades of intensive development, no Monte Carlo method can
solve the Bayesian model comparison problem completely automatically without
any manual tuning. However, \smc algorithms and the adaptive strategies
demonstrated in this paper show that even for realistic, interesting problems,
these samplers can provide good results with very minimal tuning and few
design difficulties. For many applications, they could already be used as
near automatic, robust solutions. For more challenge problems, the robustness
of the algorithms can serve as solid foundation for specific algorithm
designs.
