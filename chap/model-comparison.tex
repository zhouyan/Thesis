\ifx\inthesis\undefined % In Thesis

\documentclass[11pt, hyper, bib, fontset=Minion]{marticle}
\input{../macros}
\addbibresource{../thesis.bib}

\title{Bayesian Model Comparison}
\author{Yan Zhou}
\date{\today}

\begin{document}
\maketitle

\else % In Thesis

\chapter{Bayesian Model Comparison}
\label{cha:Bayesian Model Comparison}

\fi % In Thesis

Model comparison for the purposes of selection, averaging and validation is a
problem found through out statistics and related disciplines. There are two
popular approaches to this problem among others. One is the main subject of
this thesis, Bayesian model comparison. The other is information-theoretic
based and in particular Akaike's information criterion (\aic). Though not fit
into the Bayesian framework, \aic is used as a baseline for comparison in
chapter~\note{Bayesian Model Comparison for Positron Emission Tomography
  Compartmental Models} and is reviewed briefly in the next section. The rest
of this chapter review Bayesian model comparison techniques in detail.

\section{Akaike's information criterion}
\label{sec:Akaike's information criterion}

The \aic uses the \kl (see section~\ref{sub:kl and maximum likelihood
  estimator}) as a measure of the loss when using an approximated model in
place of the true data generating mechanism. It is built upon the relation
between \kl and the maximum likelihood estimator (\mle). The general formula
of \aic for a candidate parametric model $\calM$ is
\begin{equation}
  \text{\aic} = -2\ell_n(\hbth;\bfx) + 2k
\end{equation}
where $\ell_n(\bth;\bfx)$ is the log-likelihood for a size $n$ sample
$\bfx=(x_1,\dots,x_n)^T$, evaluated at $\bth$. The value $\hbth = \hbth(\bfx)$
is the \mle under model $\calM$.\footnote{More precisely it is a class of
  models whose densities belong to the same parametric family.} And $k$ is the
dimension of the parameter vector. The model with the smallest \aic value is
selected. It is first introduced by Akaike in a series of papers
\parencite{Akaike:1973uc, Akaike:1974ih, Akaike:1977ul}. From a likelihood
principle perspective, \aic acts as a balance between good fit (high
log-likelihood) and complexity in the sense that the dimension of the
parameter vector is used as a penalty term in the formula.  The \aic method
aims to find models that have fewer parameters while fitting the data well.
There are other kind of penalized criteria for choosing models.  And many of
them are closely related to the \aic method.

In the remaining of this section, the relation between \kl and \mle is
introduced first. Then a derivation of \aic is provided, as a basis for
further discussions. A Bayesian analysis and corrected \aic are also reviewed.

\subsection{\protect\kl and maximum likelihood estimator}
\label{sub:kl and maximum likelihood estimator}

\kl measures the ``distance'' or ``divergence'' between statistical
populations defined by their underlying probability densities. The common
definition of \kl for continuous distributions in literature is,
\begin{equation}
  \dkl(g, f(\cdot;\bth)) = \int g(x)\log\frac{g(x)}{f(x;\bth)}\intd x,
  \label{eq:kld}
\end{equation}
where $g$ is the true density of the data generating mechanism, and
$f(\cdot;\bth)$ is the density of a candidate model specified by parameter
vector $\bth$. The integration above is over the support of $g$.

One important observation is that $\dkl(g,f(\cdot;\bth))\ge0$ with equality if
and only if $g(x) = f(x;\tbth)$ almost everywhere for some value of $\bth =
\tbth$ \parencite{Kullback:1951va}. This result naturally suggests one to find
the estimator $\tbth$ which minimizes the \kl. Under suitable conditions,
rewrite equation~\eqref{eq:kld},
\begin{align}
  \dkl(g,f(\cdot;\bth))
  & = \int g(x)\log g(x)\intd x - \int g(x)\log f(x;\bth)\intd x \notag\\
  & = \int g(x)\log g(x)\intd x - \Exp_g \log f(X;\bth),
  \label{eq:kl exp ll}
\end{align}
where $\Exp_g\log f(X;\bth)$ is the expected log-likelihood with respect to
$g(X)$. Therefore $\tbth$ will maximize the expected log-likelihood. Letting
$\ell_n(\bth;\bfx)$ denote the log-likelihood of an \iid sample $\bfx =
(x_1,\dots,x_n)^T$ from density $g$, evaluated at $\bth$, then by the Strong
Law of Large Numbers,
\begin{equation}
  \frac{\ell_n(\bth;\bfx)}{n}
  \overset{\mathrm{a.s.}}{\longrightarrow}
  \Exp_g\log f(X;\bth), \quad\text{for } n\to\infty\text{ and fixed }\bth,
  \label{eq:kl consistency}
\end{equation}
provided that this expectation exits. Hence \parencite[see][for
details]{Kullback:1951va},
\begin{equation}
  \hbth \overset{\mathrm{a.s}}{\longrightarrow} \tbth,
  \quad\text{for } n\to\infty.
\end{equation}
where $\hbth$ is the \mle. That is, the \mle aims to minimize the \kl from the
candidate model to the true data generating density. This also justifies the
use of \mle from an information-theoretic perspective.

\subsection{Derivation of \protect\aic}
\label{sub:Derivation of aic}

The \aic method and its formula has been derived and justified by various
authors \parencite[e.g.,][]{Stone:1982ck, Sawa:1978tn, Chow:1981te,
  Bozdogan:1987wy, Akaike:1973uc}. To summarize, we start with the same
information-theoretic approach as in \textcite{Akaike:1973uc}, and use Taylor
expansions to approximate the expected loss of using a parametric density with
\mle as the parameter value in place of the true data generating density. In
addition we shall only consider the case in which the sample size $n$ is
sufficient large in the sense that $n\gg k$, where $k$ is the dimension of the
estimated parameter vector. The case in which $n$ is small is considered by
\textcite{Hurvich:1989ev}, and will be reviewed later in
section~\ref{sub:Corrected aic}.

As pointed out by \textcite{Bozdogan:1987wy}, though the development of \aic
had its origin in the time series modeling, it is a direct extension to the
information-theoretic interpretation of the \mle. For a class of parametric
models with density $f(\cdot;\bth)$, we aim to find the ``best'' in the sense
that the \kl is minimized among all possible values of $\bth$, i.e., to find
the value of $\tbth$, which maximizes the expected log-likelihood.  From a
decision-theoretic point of view this model also minimizes the expected loss
measured by the \kl \parencite{Akaike:1973uc}. Ideally, if we can find
$\tbth$, then the next step will be choosing the model with the minimum of
$D_{\mathrm{KL}}(g,f(\cdot;\tbth))$. However, even in the unlikely case that
$g(x) = f(x;\tbth)$, we still have to estimate $\tbth$. As suggested in the
last section, given \iid sample $\bfx = (x_1,\dots,x_n)^T$, we shall use the
\mle ($\hbth$) as a consistent estimator for $\tbth$. The \aic method is in
essence used to estimate the expected \kl from $f(\cdot; \hbth)$ to $g$.
Recall equation~\eqref{eq:kl exp ll}, and the additive property of \kl and
log-likelihood, the \aic method aim to estimate the following quantity,
\begin{equation}
  \Exp_{\bfY}\Exp_{\bfX}\log f(\bfX;\hbth(\bfY)),
\end{equation}
where $\bfX = (X_1,\dots,X_n)^T$, $X_i \sim g$, and $\bfY =
(Y_1,\dots,Y_n)^T$, $Y_i \sim g$ is a simple random sample. We emphasize that
$\hbth$, the \mle, is a function of the data without involving $\bfX$.
Therefore, the outer expectation can also be viewed as taken with respect to
$\hbth$. Let $\ell_n(\bth;\bfy)$ be the log-likelihood computed with data
$\bfy = (y_1,\dots,y_n)$. Let $\tbth$ be the value of $\bth$ maximize
$\Exp_{\bfX}\log f(\bfX;\bth)$. Then the key result is that,
\begin{equation}
  \Exp_{\bfY}\Exp_{\bfX}\log f(\bfX;\hbth(\bfY))
  \approx \Exp_{\bfX}\log f(\bfX;\hbth) - \tr(I(\tbth)\Sigma),
  \label{eq:aic general}
\end{equation}
where
\begin{align}
  I(\tbth)
  & = \Exp_{\bfX}\Square[Big]{
    -\frac{\partial^2\log f(\bfX;\bth)}{\partial\bth\partial\bth^T}}
  \Bigm|_{\bth=\tbth} \\
  \Sigma &= \Exp_{\hbth}[(\hbth-\tbth)(\hbth-\tbth)^T].
\end{align}

That is, $\Sigma$ is the theoretical covariance matrix of $\hbth(\bfY)$ when
$\bfY$ follows the true data generating density. This is a more general
results than that found in \textcite{Akaike:1973uc}. In the special case where
$g = f(\cdot;\tbth)$, $I(\tbth)$ becomes the Fisher information matrix, and
$I(\tbth) = \Sigma^{-1}$. In this case equation~\eqref{eq:aic general} leads
to the original \aic by using $\ell_n(\hbth;\bfy)$ as an unbiased consistent
estimator for its own expectation. This ideal situation does not exist in
reality anyway. But as shown in \textcite{Shibata:1989tm}, $k$, the dimension
of the estimated parameter vector is a good estimator of $\tr(I(\tbth)\Sigma)$
when the model is a ``good'' approximation of the true data generating
mechanism. This also leads to the exact formula of \aic. In either case, a
large sample is required to produce a good estimate of the expected
\kl.\footnote{More precisely, it is the relative \kl being estimated, the term
  $\int g(x)\log g(x)\intd x$ in equation~\eqref{eq:kl exp ll} is not and cannot
  be estimated.}

In summary, \aic is fully justified when the candidate models are already good
approximations to the true data generating mechanism and the sample size is
sufficient large. But one can hardly determine if the candidate models are
``good'' approximations while the true model, if it exists at all, is unknown.
There have been many efforts to improve the \aic methods in the situations
that the candidate models are not ``good enough'' approximations or the sample
size is small. One of the information criteria of this kind are reviewed in
the section~\ref{sub:Corrected aic}. However, when the model is high
dimensional and the available data is limited, simple information criteria
like \aic can hardly work.

\subsection{A Bayesian analysis of \protect\aic}
\label{sub:A Bayesian analysis of aic}

The \aic method is often compared with Bayesian model selection as one of the
most important frequentist model selection methods, in particular in contrast
with the \bic method. It is interesting to note that \textcite{Akaike:1978ti}
has a Bayesian interpretation of the \aic procedure for the special case of a
multivariate Gaussian model. In that case, \aic can be viewed as an
approximation to the posterior probability. Without going into the details,
\textcite{Akaike:1978ti} considered the situation that the sample size can be
easily increased to a large enough number while two models are very close in
the sense that the difference in their parameters are not quite visible
through the observed data. Under this setting, it was then shown that the log
of posterior model probability from a normal priors for mean vector is
approximated by $(-1/2)$\aic up to an additive constant. However we don't see
this setting as the general cases in data analysis as argued by Akaike.
Instead, in realistic cases the difficulties of model selection problem often
come from the fact that we have very limited data.

On the other hand, as seen in \textcite{Akaike:1980gh}, using likelihood
function and \aic estimates as the source of objectivity of Bayesian
modeling leads to the \bic procedure. However, those with a subjective
Bayesian perspective are not likely to be convinced by the desire of
objectivity itself. In addition, as noted by \textcite{Kass:1995vb}, the
Bayesian justification of \aic is only valid when the precision of the prior
distribution is comparable to that of the likelihood. This assumption is
hardly ever true in reality.

In both cases, we found that though the motivations of these Akaike's work are
likely purely for the defense of \aic rather than a general assessment of the
connections between \aic and Bayesian modeling, it is still interested to note
that the two frameworks are closely related. In practice different model
selection methods should be considered based on the purposes (inference of the
past or prediction of the future, or otherwise). There is no reason to argue
that one is absolutely superior to another.

\subsection{Corrected \protect\aic}
\label{sub:Corrected aic}

We briefly review one of the most widely used extension and modification of
\aic method, the corrected \aic. This and other similar methods try to improve
the model selection procedure in the situations that
\textcite{Akaike:1973uc}'s assumptions are not justified. Therefore, they
improve the robustness of the \aic procedure.

The corrected \aic, usually denoted by \aicc, mainly deals with the situation
that the sample size is not sufficient large in the sense that the dimension
of the parameter vector to be estimated is close to the sample size.
\textcite{Hurvich:1989ev} developed the \aicc for regression and time series
modeling. The \aicc is defined as,
\begin{equation}
  \text{\aicc} = -2\ell_n(\hbth;\bfx) + \frac{2nk}{n-k-1}
\end{equation}
or equivalently,
\begin{equation}
  \text{\aicc} = \text{\aic} + \frac{2k(k+1)}{n-k-1},
\end{equation}
where $n$ is the sample size, other notations are as defined before.

It should be noted that \aicc is just one way to improve \aic for small
samples. In particular, it is derived in the case of a model with linear
structure and normal errors \parencite{Hurvich:1989ev, Burnham:2002wc}. Under
other models, other form of improved \aic can be derived. In general,
\textcite{Hurvich:1989ev}'s form seems to be a good choice and adopted
extensively in the literature even in nonlinear cases
\parencite[e.g.,][]{Turkheimer:2003iy}.

However, to illustrate the limitation of the \aicc method, in
\textcite{Zhou:2011uo} which we examined the model selection for compartment
models, the use of \aicc was shown inefficient for high dimension, high noise
and limited size of data. The use of \aic based methods for such models was
first introduced by \textcite{Hawkins:1986ha}. However, their work, and some
recent use of \aic or \aicc mainly focus on low noise data. If the model has a
nonlinear structure and high level noise, it can hardly be approximated by
normal distribution. In such situations, as shown in \textcite{Zhou:2011uo}
for \pet data, \aicc does not perform well. It is possible to derive specific
\aic based information criterion for specific models, like
\textcite{Hurvich:1989ev} did for theirs. Nonetheless this will involve much
more analytical work which may or may not results in improved results.
Instead, in the our work, using Bayesian model selection for \pet data are
approached. The results are encouraging while the implementations still has
limitations.

\section{Bayesian model comparison}
\label{sec:Bayesian model comparison}

The Bayes' theorem, in its simplest form is stated as below,
\begin{equation}
  p(H|D) = \frac{p(D|H)p(H)}{p(D)} \label{eq:bayes}
\end{equation}
where $D$ is the data and $H$ is a hypothesis. Like many other probability
theories, technically the Bayes' theorem merely provides a method of
accounting for the uncertainty. However, what is far more important is the
associated philosophical interpretation -- the subjective perspective. Though
early development of Bayesian statistics showed interests of using likelihood
functions as objective input to the modeling processes
\parencite{Jeffreys:1961ua, Jeffreys:1946jf}, it is nowadays widely accepted
that a subjective view is more appropriated
\parencite[see][chap.~1]{Bernardo:1994vd}. Bayesian statistics offers a
rationalist theory of \emph{personal} beliefs and decisions in the contexts of
uncertainty. So it is natural to question the relevance of one's personal
beliefs to another. However, limited by the scope, these more philosophical
problems will not be elaborated further in this thesis. In the remaining of
this thesis We will focus on the technical aspects of Bayesian statistics, in
particular the model comparison and selection problem, and their
implementations.

A throughout treatment of Bayesian modeling from a decision theory perspective
can be found in \textcite{Robert:2007tc}. Formal mathematical representations
can also be found in \textcite[][sec.~5.1 and sec.~6.1]{Bernardo:1994vd},
where the notion of rational decisions in the context of uncertainty was made
precise in the form of axioms in \textcite[][chap.~2]{Bernardo:1994vd}. It is
assumed that a rational decision cannot be considered separately from rational
beliefs. And rational beliefs shall be built upon available information (the
data in the statistical term) and any personal preference input (the prior
information in Bayesian literatures).

\subsection{Bayesian model choice problem}
\label{sub:Bayesian model choice problem}

Consider a (possibly infinite) countable set of models $\calM = \{\calM_i,
i\in I\}$. Our aim is to choose the ``best'' model from this set. There will
usually be actions taken after the model is chosen, for example inference of
the past or prediction of the future. It is the consequences of these actions
of interest instead of the chosen model itself. Therefore, from a
decision-theoretic perspective, the ``best'' model should maximize the utility
for some quality of the interest. However, in practice it is common to ignore
the actions following the model selection and the sole interest is the true
model, say $\calM_t$. In this case, it is natural to define a zero-one utility
function, say $u(\calM_i,\calM_t)$, such that $u(\calM_i,\calM_t) = 1$ for
$\calM_t = \calM_i$ and $0$ otherwise.

It is easy to see that the model $\calM_i$ that maximizes the expected utility
given data $D$ is the model with the highest posterior probability
$\pi(\calM_i|D)$, see \textcite[][chap.~6]{Bernardo:1994vd}. This model is
thus our ``best'' model in this setting. It should be noted this argument is
only valid if the true model $\calM_t \in \calM$. Otherwise, the utility is
always zero for all models. In what follows, we presume that our aim is to
find the model with the highest posterior probability.

Since the set $\calM$ is countable, it is natural to define a discrete random
variable by an injective mapping $\kappa:\calM\to\bbN$, such that
$\kappa(\calM_i) = k_i$, $k_i \in \bbN$. We will refer to a model by $\calM_i$
or $\kappa = k_i$ in the remaining of this thesis whichever is more convenient
in context. The random variable $\kappa$ is often called the model indicator.

For parametric models, assuming that conditional on $\kappa = k$, the
parameter vector $\bth_k$ has dimension $n_k$ and parameter space
$\bTh_k\subset\bbR^{n_k}$. In addition, prior distributions for both $\bth_k$
conditional on $\kappa = k$ and the model indicator $\kappa$ are specified,
say $\pi(\bth_k|\kappa =k)$ and $p(\kappa = k)$, respectively. Therefore,
given the data $D$, the joint density and the posterior density are,
respectively,
\begin{align}
  \eta(D,\bth_k,\kappa=k)
  &= f(D|\bth_k,\kappa=k)\pi(\bth_k|\kappa=k)\pi(\kappa=k)
  \label{eq:full joint}\\
  \pi(\bth_k,\kappa=k|D)
  &= \pi(\bth_k|D,\kappa=k)\pi(\kappa=k|D)
  \label{eq:full posterior}
\end{align}
where $f$ is the likelihood function. Two approaches are commonly used in
Bayesian model selection problems. One is to compare the posterior probability
$\pi(\kappa=k_i|D)$ pairwise for models $\kappa = k_i$. This approach leads to
the Bayes factor, reviewed in the next section. The other approach is to
inference the distribution $\pi(\kappa=k_i|D)$ directly. This approach to
Bayesian model selection can be seen as a specific application of variable
dimension models, which is briefly reviewed in section~\ref{sub:Variable
  dimension models}.

\subsection{Bayes factor}
\label{sub:Bayes factor}

When the model set $\calM$ is finite, we can find the model with the highest
posterior probability by compare models pairwise. Rewrite the posterior
density~\eqref{eq:full posterior} as,
\begin{equation}
  \pi(\bth_k,\kappa=k|D) = \frac{\eta(D,\bth_k,\kappa=k)}{p(D)}.
\end{equation}
Substitute equation~\eqref{eq:full joint}, and integrate out the parameter
vector $\bth_k$ over its parameter space, we have the following form of the
posterior model probability $\pi(\kappa=k|D)$,
\begin{equation}
  \pi(\kappa=k|D) = \frac{\pi(\kappa = k)}{p(D)}
  \int f(D|\bth_k,\kappa = k)\pi(\bth_k|\kappa = k)\intd\bth_k.
\end{equation}
Therefore to compare the posterior probabilities of two models, say $\kappa =
k_1$ and $k_2$, one only need to compute their ratio,
\begin{align}
  \frac{\pi(\kappa = k_1|D)}{\pi(\kappa = k_2|D)} &= \frac
  {\int f(D|\bth_{k_1},\kappa=k_1)\pi(\bth_{k_1}|\kappa=k_1)\intd\bth_k}
  {\int f(D|\bth_{k_2},\kappa=k_2)\pi(\bth_{k_2}|\kappa=k_2)\intd\bth_k}
  \frac{\pi(\kappa=k_1)}{\pi(\kappa=k_2)} \notag\\
  & = B_{12} \frac{\pi(\kappa=k_1)}{\pi(\kappa=k_2)},
  \label{eq:posterior odd}
\end{align}
where
\begin{equation}
  B_{12} = \frac
  {\int f(D|\bth_{k_1},\kappa=k_1)\pi(\bth_{k_1}|\kappa=k_1)\intd\bth_k}
  {\int f(D|\bth_{k_2},\kappa=k_2)\pi(\bth_{k_2}|\kappa=k_2)\intd\bth_k}
  \label{eq:bayes factor}
\end{equation}
is called the Bayes factor. Equation~\eqref{eq:posterior odd} states how the
prior odds ratio is transformed into the posterior odds ratio by the Bayes
factor \parencite{Kass:1995vb}. The Bayes factor is the principle tool for
Bayesian model comparison and model selection. As it is made clear in the
above equation, to compute the Bayes factor, all that needs to be done is the
computation of the following quality for each value of $\kappa$,
\begin{equation}
  p(D|\kappa = k) = \int f(D|\bth_k,\kappa=k)\pi(\bth_k|\kappa=k)\intd\bth_k,
  \label{eq:marginal likelihood}
\end{equation}
which is called the \emph{marginal likelihood} of data $D$ under model $\kappa
= k$.  It is obvious that $B_{ij} = B_{ik} B_{kj}$, and thus the Bayes factor
approach is equivalent to choose the model with the highest marginal
likelihood, as long as the model set $\calM$ is finite.

It should be noted that the prior distribution $\pi(\bth_k|\kappa=k)$ is
chosen by the statistician in the modeling process. The choice of the prior
distribution is one of the most critical and criticized part of Bayesian
modeling. In principle, the prior distribution shall represent the prior
beliefs. That is, it represents how much is already known about the data
generating mechanism and what is believed about it before the observation of
the data, no more or no less. It shall not only describe all knowledge already
known, but also more importantly preserve all the ignorance. If it contains
more information than what is actually known, the inference will be biased. In
other words, one can always construct a prior distribution such that inference
will be biased towards one's preference, which is not necessarily rational. It
is often too difficult to elicit a precise distribution from prior
information.  Therefore it is necessary to make at least partially arbitrary
choice of the prior distribution
\parencites{Robert:2007tc}[][chap.~3]{Kass:1995vb}.

Often the priors need to be considered carefully on a per problem basis.
Nonetheless there are a few classes of prior distributions frequently used in
the Bayesian modeling. In the particular problem of Bayesian model comparison
and selection, it shall be noted that the Bayes factor is more sensitive to
the choice of prior than point or interval estimations \parencite{Kass:1993vy,
  Kass:1995vb}. One of the more important class of prior distribution is the
conjugate prior. See \textcite[][chap.~5]{Bernardo:1994vd} for its
justification.  In practice the conjugate prior is often used because of its
mathematical properties. However it is often argued that this is an objective
approach as the subjective input is reduced to the choice of parameters. In
addition, when there is no prior information available, the only justification
for conjugate priors is analytical, their closed form expression. There is no
way to justify the choice of prior distributions on a subjective basis in such
situations. In such settings, there are also efforts to make the prior
``non-informative''.  Among them the most important two are Jeffreys' priors
\parencite{Jeffreys:1946jf} and the reference prior
\parencite{Bernardo:1979uq}. See \textcite{Kass:1996jj} for more detailed
analysis of the Jeffreys' priors and other non-informative approach to
Bayesian analysis and \textcite{Berger:1989vj, Berger:1992kf, Berger:1992wo}
for reference priors.

It is also important to evaluate the Bayes factor over a range of possible
priors to assess the sensitivity issues. This is often computational expensive
since many high dimensional integrations are required. When there is enough
information to construct parametric priors, it is possible to alter the values
of parameters and recompute the Bayes factor
\parencite[e.g.,][]{McCulloch:1991hj}. In general situations, a less
computational expensive method is to use the Laplace approximation to compare
the Bayes factor using different priors \parencite[e.g.,][]{Kass:1992tz}. It
is also proposed in the literature to use the maximum of Bayes factor (and
thus the maximal evidence against a model) to evaluate the sensitivity problem
\parencite[e.g.,][]{Berger:1987iq}.

We will not concern much of the choice of priors in the remaining of this
thesis. It should however be noted that in practice, prior distributions
should always be chosen carefully and the sensitivity should be evaluated. In
what follows, we assume a suitable prior is already chosen such that the Bayes
factor is finite and nonzero, and we concerns the computation of the Bayes
factor. Broadly the computation can be divided into two classes: approximation
based and Monte Carlo methods based. We review one of the most important
information criteria, the Bayesian information criterion (\bic) in the next
subsection. We will review the Monte Carlo methods in chapter~\note{Monte
  Carlo methods}.

\subsubsection{Bayesian information criterion}
\label{ssub:Bayesian information criterion}

The Bayesian information criterion (\bic) was developed as an approximation to
the Bayes factor \parencite{Schwarz:1978uv}. The \bic is defined as,
\begin{equation}
  \text{\bic} = -2\log f(D|\hbth) + k\log(n).
\end{equation}
where $f(D|\hbth)$ is the likelihood function evaluated at the \mle ($\hbth$)
and $k$ is the number of parameters to be estimated. The \bic strategy choose
the model with the smallest \bic value. To justify its use, we first outline
its derivation, which will provide us some insights of the quality of the
approximation. We start with equation~\eqref{eq:marginal likelihood} while
dropping the model index $k$ and $\kappa=k$, since this computations is for
each model individually. Let $h(\bth) = - \log f(D|\bth)\pi(\bth)$ and let
$\tbth$ denote the value of $\bth$ that minimizes $h(\bth)$. Then,
\begin{equation}
  p(D) = \int f(D|\bth)\pi(\bth)\intd\bth = \int\exp\{-h(\bth)\}\intd\bth
\end{equation}
Applying Taylor expansion to $h(\bth)$ around $\tbth$,
\begin{align}
  h(\bth)
  &= h(\tbth) + (\bth - \tbth)^T h'
  + \frac{1}{2}(\bth - \tbth)^T h''(\tbth) (\bth - \tbth)
  + o(\lVert \bth - \tbth \rVert)^2 \notag\\
  &\approx h(\tbth) + \frac{1}{2}(\bth - \tbth)^T H (\bth - \tbth)
\end{align}
where $H = h''(\tbth)$, the Hessian matrix of $h$ evaluated at $\tbth$ and
$h'$ is the gradient of $h$. Since for $n\to\infty$, the posterior
distribution is concentrated around $\tbth$, and thus only those values close
to $\tbth$ contribute significantly to the integration,
\begin{equation}
  p(D)\approx\exp\{-h(\tbth)\}
  \int\exp\{-\frac{1}{2}(\bth - \tbth)^TH(\bth - \tbth)\}.
\end{equation}
Note the integrand is the kernel of of a multivariate normal distribution with
mean $\tbth$ and covariance matrix $H^{-1}$, which is assumed to be positive
definite and exist. It follows,
\begin{equation}
  p(D) \approx \exp\{-h(\tbth)\} (2\pi)^{k/2} \lvert H \rvert^{-1/2},
\end{equation}
where $k$ is the number of parameters, or the dimension of $\bth$. Thus,
\begin{equation}
  -2\log p(D) \approx
  -2\log f(D|\tbth) - 2\log\pi(\tbth) - k\log(2\pi) + \log\lvert H \rvert.
  \label{eq:bic approx}
\end{equation}
With \iid samples, for $n\to\infty$, $H/n \approx i$, where $i$ is the Fisher
information matrix. It follows that $\lvert H \rvert \approx n^k \lvert i
\rvert$ for large $n$. Omitting the $O(1)$ terms,
\begin{equation}
  -2\log p(D) \approx -2\log f(D|\tbth) + k\log(n).
\end{equation}
With $n\to\infty$, $\tbth\approx\hat{\bth}$, where $\hat{\bth}$ is the \mle,
replace $\tbth$ with $\hat{\bth}$, the right hand side of the above equation
becomes the formula for what is commonly known as \bic. The \bic method choose
the model with smallest \bic.

The \bic method is often compared and contrasted with the \aic method, which
we discussed in section~\ref{sec:Akaike's information criterion}. Despite the
similarity in formulas, \aic and \bic have very different assumptions. In
addition to the large sample assumption, which is difficult to assess in
practice, \bic also assumes ``good behavior'' of the likelihood function such
that the use of \mle in place of $\tbth$ is justified. Also, unlike \aic,
whose approximation is often at worst $O(1/n)$, the \bic requires that the
$O(1)$ term in equation~\eqref{eq:bic approx} is negligible compared to other
terms. These assumptions restricted the use of \bic in many situations. See
\textcite{Gelfand:1994ux} for an example where the $O(1)$ approximation
failed. See \textcite{Berger:2001uy} for examples that the irregularity of
likelihood function causes the \bic method failed. Moreover, in non-\iid
situations, just like \aic, the definition of the parameter dimension $k$ is
ambiguous \parencite{Spiegelhalter:1998uc, Kass:1995vb}. See also
\textcite[][chap.~5 and chap.~6]{Burnham:2002wc} for more comparison and
contrast of \aic and \bic approaches to model selection. There are other
criticism of the \bic strategy. For example
\textcite[][chap.~7]{Robert:2007tc} argued that the \bic strategy eliminated
the subjective input into the Bayes modeling since the value of \bic does not
depend on the prior distribution. However this is equally argued as an
advantage of this strategy in the case that priors, to which Bayes factors can
be very sensitive, are hard to specify.

\subsection{Variable dimension models}
\label{sub:Variable dimension models}

Recall the end of section~\ref{sub:Bayesian model choice problem}, instead of
using the marginal likelihood for Bayesian model selection, the posterior
density $\pi(\bth_k,\kappa=k|D)$ can also be directly used for model
inference. Since the posterior model probability $p^*(\kappa=k|D)$ is simply
the marginal density of $\mu$. Therefore if we have knowledges of $\mu$,
(usually from simulations, see section~\note{Reversible jump \mcmc}), it is
straightforward to make model inference. Formally, let $\calK$ denote the
possible values of $\kappa$ (this is equivalent to the notion of model set
$\calM$). Then the goal is to make inference about the parameter vector
$(\kappa, \bth_{\kappa})$ conditional on the data $D$, whose parameter space
is,
\begin{equation}
  \bTh = \bigcup_{k\in\calK}\{k\}\times\bTh_k,
  \label{eq:variable dimension parameter space}
\end{equation}
where $\bTh_k$ is the model parameter space of $\bth_k$.

Despite its straightforward interpretation of the inference objective, this
representation has some theoretical difficulties. Limited by the scope of this
thesis, we will not discuss the details such as the measure-theoretic
difficulty in the notions of prior density for a direct sum space.
Nonetheless, it shall be noted that in the context of Bayesian model
selection, the model parameter spaces $\bTh_k$ are independent of each other.
And thus a prior density $\pi(\bth_k|\kappa =k)$ need to be specified for each
model. However, it is often preferred to use some parameters common to all
models and thus reduce the modeling and computation difficulties. As it will
be discussed in section~\note{Reversible jump \mcmc}, the reversible jump
\mcmc techniques rely on such assumptions. The computation of the variable
dimension models are reviewed in section~\note{Reversible jump \mcmc}.

\ifx\inthesis\undefined
\printbibliography
\end{document}\else\relax\fi
