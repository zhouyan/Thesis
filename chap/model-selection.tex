\chapter{Model selection}
\label{cha:Model selection}

Model selection is a problem found throughout statistics and related
disciplines. A number of approaches of model selection has been developed
through the history of statistics. Each has its own strength and limitations.
There is no universally best model selection strategy. It is beyond the scope
of this thesis to give a comprehensive and systematic review of this topic. We
will review some of the more widely used methods. We are mostly interested in
the methods that are generic in the sense that their usefulness are not
limited to a particular class of models.

Section~\ref{sec:Information theoretic approach} reviews a few information
theoretic approaches. The most important one of them is the Akaike's
information criteria (\aic; \cite{Akaike:1973uc,Akaike:1974ih}). A few other
closely related methods are also reviewed in this section.
Section~\ref{sec:Minimum description length} review the minimum description
length (\mdl) method. And the last, Section~\ref{sec:Bayesian model
  comparison} reviews the Bayesian approach to model comparison.

\section{Information theoretic approach}
\label{sec:Information theoretic approach}

Information theory is a discipline that covers a wide range of theories and
methods that are fundamental to many of the sciences (see e.g.,
\cite{Cover:1991vx} for an overview). Most relevant here the \kld
\cite{Kullback:1951va}, which measures the discrepancy between two models.
Many model selection methods are based on estimators of this measure of
discrepancy.

\subsection{Kullback-Liebler divergence}
\label{sub:Kullback-Liebler divergence}

Assume that the distribution of data is continuous and has a density function
$g$. Let $f(x) = f(x|\theta)$ be the density function of some continuous
parametric distribution, where $\theta$ is the parameter vector. Then the \kld
between $f$ and $g$ is defined by,
\begin{equation}
  D_{\kl}(g,f) = \int g(x)\log\Round[bigg]{\frac{g(x)}{f(x|\theta)}}\intd x
  \label{eq:kl}
\end{equation}
In \cite{Kullback:1951va}, it was originally developed from information
theory, as it relates the ``information'' lost when $f$ is used to approximate
$g$. The \kld is always nonnegative and equals to zero if and only if $g(x) =
f(x)$ everywhere \cite[][sec.~6.8]{Burnham:2002wc}. The concept can be
generalized to discrete distributions and more general settings
\cite[][sec.~2.1.3]{Burnham:2002wc}. For the purpose of simplicity, in the
remaining of this section, we will assume that the distributions under
discussion are continuous.

The criterion of model selection under this theme is thus finding the model
that minimize the \kld. In reality, there are often a set of candidate models.
Each model is defined by a parametric distribution. Therefore the model
section process can be viewed as two steps. First for each model, a parameter
value is found such that the \kld is minimized within this class. Second, the
one with the smallest \kld among all models is selected.

It is clear that the calculation of $D_{\kl}(g,f)$ relies on the knowledge of
both $g$, which is unknown, and $f$ as well as the specific parameter
$\theta$. Rewrite equation~\eqref{eq:kl},
\begin{align}
  D_{\kl}(g,f)
  &= \int g(x)\log g(x) \intd x - \int g(x) \log f(x|\theta) \intd x \notag\\
  &= \Exp_g[\log g(X)] - \Exp_g[\log f(X|\theta)],
\end{align}
where the first term is a constant. Therefore, minimizing $D_{\kl}(g,f)$ is
equivalent to minimizing $-\Exp_g[\log f(X|\theta)]$. The later expectation is
also called the \emph{relative} \kld. Let $\tilde\theta$ denote the value of
parameter that minimizes the relative \kld and $\hat\theta(\data)$ denote an
estimator of it, where $\data$ is the data generated from $g$. We have the
minimum and estimated \kld,
\begin{align}
  \tilde{D}_{\kl}(g,f) &= \text{Constant} - \Exp_g[\log f(X|\tilde\theta)], \\
  \hat{D}_{\kl}(g,f) &= \text{Constant} - \Exp_g[\log f(X|\hat\theta(\data))].
\end{align}
Since $\hat\theta(\data)\ne\tilde\theta$ for (almost) all data $\data$, we
have $\hat{D}_{\kl}(g,f) > \tilde{D}_{\kl}(g,f)$. A more reliable criteria is
the expected value of $\hat{D}_{\kl}(g,f)$ or,
\begin{equation}
  \bar{D}_{\kl} = \Exp_{\hat\theta}\Exp_g[\log f(X|\hat\theta(\data))]
  \label{eq:expected kl}
\end{equation}
where the outer expectation is with respect to $g$ and integrate out the
estimated parameter $\hat\theta(\data)$. Note that we cannot compute this term
analytically since it depends on the true model $g$, which is assumed to be
unknown. The model selection criteria discussed below rely on the
approximations of this quantity. Consequently, these methods attempt to select
the model that on average minimizes, over a set of models, the expected \kld.

\subsection{Akaike's information criteria}
\label{sub:Akaike's information criteria}

The \aic strategy is based on an observation of the relationship between the
maximum likelihood estimator (\mle) and the \kld. Let $\data =
(y_1,\dots,y_n)$ denotes \iid samples generated from $g$. Then by the Strong
Law of Large Numbers (\slln),
\begin{equation}
  \frac{1}{n}\ell_n(\theta) \xrightarrow{\text{a.s.}} \Exp_g[\log f(Y|\theta)]
  \label{eq:ml convergence}
\end{equation}
where $\ell_n(\theta) = \sum_{i=1}^n \log f(y_i|\theta)$ is the log-likelihood
function. This suggests the use of the \mle, say $\hat\theta$, which maximizes
$\ell_n(\theta)$ as an estimator of $\tilde\theta$, which minimizes
$D_{\kl}(g,f)$. The expected \kld $\bar{D}_{\kl}(g,f)$ can be approximated by
empirical average $\ell_n(\hat\theta)$. However, as shown in
\cite{Akaike:1973uc}, this approximation is systematically biased upward (also
see \cite[][sec.~2.3]{Claeskens:2008tq} for some remarks on the bias). It can
be shown that the bias is approximately $k/n$ where $k$ is the length of the
parameter vector $\theta$. This lead to the adjusted estimator of the relative
\kld,
\begin{equation}
  -\frac{1}{n}\ell_n(\hat\theta) + \frac{k}{n}.
\end{equation}
In \cite{Akaike:1973uc}, it is rescaled to,
\begin{equation}
  \aic = -2\ell_n(\hat\theta) + 2k
  \label{eq:aic}
\end{equation}
and the \aic strategy selects the model with the smallest value of \aic.

A more rigorous derivation of the formula~\eqref{eq:aic} can be found in
\cite[][sec.~2.3]{Claeskens:2008tq} and \cite[][sec.~6.2]{Burnham:2002wc}.
Here, we are more interested in when this approximation is good enough for the
purpose of model selection. Some remarks below are given without proof. For
technical details, see the two references of the derivation of \aic.

First, the derivation of the bias term is based on a first order Taylor
expansion of $\Exp_g[\ell_n(\hat\theta)/n - \bar{D}_{\kl}(g,f)]$. The
assumption about the parametric model $g$ is quite minimal. The accuracy is of
order $o(n)$. Given more informations about the structure of the models, more
accurate estimator can be derived by using a second order expansion. See
Section~\ref{sub:A second order aic}

Second, more importantly, \aic assumes that the candidate models are close
enough to the true model. When there is significant misspecification of the
models, the results from using the \aic method can be misleading. Estimate of
the \kld that are more model robust can be derived. See
Section~\ref{sub:Takeuchi's information criterion}.

Third, though earlier we assumed \iid sample, which leads to the
convergence~\ref{eq:ml convergence} as a motivation of using \mle for the
estimation of relative \kld. This is not necessary for the derivation of \aic.
The \aic model selection method has also been successfully used for dependent
data. For example, \cite{Lee:2001tm} shows that \aic is efficient for
selecting the order of an autoregressive process when compared to other some
methods. However, \aic does assume that the model distribution
is well behaved in the sense that the estimator used to evaluate the criterion
is indeed close to the minimizer of \kld.

The last but not least, \aic has the tendency of overfitting in the sense that
when there is a best model with the minimum expected \kld, \aic is likely to
choose the model with more parameters. Intuitively, the log-likelihood
function increases linearly as the sample size increases while the penalize
term $2K$ is not affected. Therefore, the \aic strategy is going to select
more complex models when more data becomes available. This is formally shown
in \cite{Sin:1996vs}. When there are more than one model that minimizes the
\kld to the true data density, \aic does not necessarily choose the simplest
model.

\subsection{A second order \protect\aic}
\label{sub:A second order aic}

As shown in \cite{Sugiura:1978be}, the first order approximation can perform
poorly when the data size is small when compared to the number of parameters
to be estimated. A second order variant is derived in the same paper and
further studied by \cite{Hurvich:1989ev}, which led to a criterion that is
called \aicc, the corrected \aic,
\begin{equation}
  \aicc = -2\ell_n(\hat\theta) + \frac{2nk}{n-k-1}.
\end{equation}
It is clearly that the additional bias correction is negligible if $n$ is
large when compared to $k$, as $\lim_{n\to\infty}2nk/(n-k-1) = 2k$, which is
exactly the penalty term in the original \aic formula. A rule of thumb, found
in various source, is that \aicc shall be used in place of \aic when
$n/k\le40$, see e.g., \cite[][sec.~2.4]{Burnham:2002wc}.

It should be noted that \aicc is just one way to improve \aic for small sample
size. In particular, it is derived in the case of a model with linear
structure and normal errors (see \cite{Hurvich:1989ev} and
\cite[][sec.~6.4.1]{Burnham:2002wc} for derivations of \aicc). Under other
models, other form of improved \aic can be derived. However, this form has
also been adopted successfully in literature even in nonlinear non-Gaussian
cases, for example see \cite{Turkheimer:2003iy} for its applications to \pet
compartmental models.

It shall be noted that, both \aic and \aicc assumes the use of \mle for the
computation of the criteria. However, in many nonlinear situations, the
estimator is obtained through optimizations. For example, nonlinear least
squares (\nls) estimator and other optimization procedures are widely used in
the estimation of \pet compartmental models. Model selection criteria are
computed with these estimators. These estimators are commonly used because of
their easy of computation and other properties of the resulting parameter
estimates. However, the model selection results obtained this way may not be
satisfactory. For example, in \cite{Zhou2013} the model selection for \pet
compartmental models using \aic and other methods are studies.
Table~\ref{tab:pet aicc} shows the frequencies of model selected by \aicc for
2,000 data sets simulated from a three compartments model.

\input{tab/pet-aic.tex}

It can be seen that, for data sets with small noises, the \aicc method is able
to select the two compartments model with a very high frequency. Though this
is the true model that generated the data, but it is very close as the third
compartment is difficult to identify (see discussion in \cite{Zhou2013} and
reference therein). However, when the noise level increases, the method is
unable to identify the second compartment.

It is possible to derive more accurate second or even higher order
approximations to the expected \kld for some models. However, this may not be
possible for some realistic applications. For example, the \pet compartmental
models does not have an explicit form of likelihood function, which may create
significant technical difficulty if we want to refine the \aic approximation
of expected \kld. More importantly, such refinement of \aic relies on the
assumption about the explicit form of the true model $g$ or one that closely
approximate it. When the form of the model is drastically different from the
one used to derive criteria such as \aicc, then poor results of model
selection is likely to be obtained as we seen in the \pet compartmental model.

\subsection{Takeuchi's information criterion}
\label{sub:Takeuchi's information criterion}

As stated earlier, \aic (and some of its refinements such as \aicc) depends on
the assumption that the candidate models are close to the true data generating
density. However this might not be the case in reality. In
\cite{Takeuchi:1976vx} a more general derivation from \kld to \aic was
developed. An intermediate result indicated a selection criterion useful when
there is considerable model misspecification, formulated as \tic,
\begin{equation}
  \tic = -2\ell_n(\theta) + 2\tr(J(\theta)K(\theta)^{-1})
  \label{eq:tic}
\end{equation}
where $J(\theta)$ is the expectation of the Hessian matrix and $K(\theta)$ is
the variance matrix of the score vector, respectively,
\begin{equation}
  J(\theta) = -\Exp_g\Square[bigg]{
    \frac{\partial^2\log f(X|\theta)}{\partial\theta\partial\theta^T}
  }\quad\text{and}\quad
  K(\theta) = \var_f\Square[bigg]{
    \frac{\partial\log f(X|\theta)}{\partial\theta}
  }.
\end{equation}
provided that all the differentiations and integrations exist. Ideally \tic
shall be evaluated at the minimizer of \kld $\tilde\theta$. In reality, the
\mle is used to evaluate the likelihood function and various estimator of the
bias correction term $\tr(J(\tilde\theta)K(\tilde\theta)^{-1})$ has been
developed.

Note that, since $\tilde\theta$ minimize the relative \kld, the expectation of
the score vector is a zero vector under suitable continuity conditions.
Further, if $g(x) = f(x|\tilde\theta)$ everywhere, then it is obvious that the
above two matrices are equal and it becomes the Fisher information matrix,
provided the ability to exchange the order of integrations and
differentiations and other regularity conditions. In this case,
$\tr(J(\theta)K(\theta)^{-1}) = k$, and \tic leads to the \aic formula. It
becomes clearer now that \aic is an approximation of \tic in the situation
where the candidate models are close to the true data generating mechanism.
The \tic method does not have such assumptions and can perform considerably
better than \aic in the situation of model misspecification.

If the \mle is well behaved (see, for example, \cite{Lehmann:1983vx}), then we
can substitute \mle $\hat\theta$ into equation~\ref{eq:tic} and use empirical
averages as estimates of $J(\hat\theta)K(\hat\theta)$. The explicit form of
the bias correction term can also be derived for some models. For example, see
\cite[][sec.~6.6]{Burnham:2002wc}. This allows more accurate evaluations of
\tic.

Unlike the \aic and related methods, the \tic method relies heavily on the
assumption of large sample size. It is difficult to derive small sample
correction for the \tic approximation of expected \kld. See also the
discussion in \cite[][sec.~6.7.8]{Burnham:2002wc}

\subsection{Cross-validation}
\label{sub:Cross-validation}

Cross-validation has a long history in applied and theoretical statistics. It
has been formalized in \cite{Stone:1974vx} (also see the introduction in this
paper for an overview of earlier development on the method) and
\cite{Geisser:1975vx}. The idea is to split the data into two parts. One part
of the data is used for model fitting and the resulting estimates of
parameters are used to predict the other part of the data. By comparing the
predictions based on part of the data and the observed other part of the data,
the usefulness of the model is determined.

Formally, following \cite{Geisser:1975vx}, let $\data = (y_1,\dots,y_n)$ be
the data set and $\data^t\subset\data$ be a non-empty proper subset. The
subsample $\data^t$ is called the \emph{training set} and $\data^v =
\data\backslash\data^t$ is called the \emph{validation set}. For each model
$g(\cdot|\theta)$, a loss function is defined, say $\gamma(\cdot|\theta)$. The
choice of function $\gamma$ is formally arbitrary. However, it is taken as a
measure of fitness of the model. One commonly used is the log density
function, $\gamma(x|\theta) = -\log f(x|\theta)$ (see \cite{Stone:1977vx}).
The risk estimator of $\Exp_g[\gamma(X|\hat\theta(\data))]$, where
$\hat\theta(\data)$ is the estimate obtained with all data, is obtained
through averaging over the left-out data,
\begin{equation}
  \hat{R}_f^v(\data,\data^t) =
  \frac{1}{n_v}\sum_{y\in\data^v}\gamma(y|\hat\theta(y^t))
\end{equation}
where $n_v$ is the size of the validation set $\data^v$ and $\hat\theta(y^t)$
is the parameter estimate obtained with only the training set $\data^t$.
Further, let $\data_1^t,\dots,\data_m^t$ be a sequence of non-empty proper
subset of $\data$. The cross-validation estimator of the risk is defined as,
\begin{equation}
  \hat{R}_f^{\cv}(\data,\{\data_i^t\}_{i=1}^m) =
  \frac{1}{m}\sum_{i=1}^m \hat{R}_f^v(\data;\data_i^t).
\end{equation}
The model selection proceeds by choose the model with the smallest value of
$\hat{R}_f^{\cv}(\data,\{\data_i^t\}_{i=1}^m)$.

An alternative, as seen in \cite{Yang:2007vx}, is called cross-validation
\emph{with vote}. A model with density $f_1$ is chosen over a model with
density $f_2$ if and only if $\hat{R}_{f_1}^v(\data,\data_i^t) <
\hat{R}_{f_2}^v(\data,\data_i^t)$ for a majority of the partitions of sample.

There different ways to split the sample. The most commonly used is perhaps
the \emph{leave-one-out} procedure \cite{Stone:1974vx,Geisser:1975vx}. In this
case, training sets $\data_i^t = \data\backslash\{y_i\}$ for $i = 1,\dots,n$
are used. A more general scheme is that $k$ observations are left out for each
training set and all possible combinations are considered \cite{Shao:1993vx}.
It is clear that $k=1$ yields the leave-one-out procedure and for large sample
size, a modest $k$ can lead to higher computational cost as the number of
possible partitions is the binomial coefficient. Other procedures are also
possible. For more information we refer to, for example, \cite{Stone:1978vx}
and \cite{Hjorth:1994vx}. This is also called the $k$\emph{-fold} procedure.

There are also different choices of the loss function $\gamma$. The one
mentioned earlier, $\gamma(x|\theta) = -\log f(x|\theta)$, when combined with
the leave-one-out procedure, leads to the estimator,
\begin{equation}
  -\frac{1}{n}\sum_{i=1}^n\log f(y_i|\hat\theta(\data_i^t))
\end{equation}
where $\hat\theta(\data_i^t)$ is the estimator obtained using the subsample
$\data_i^t = \data\backslash\{y_i\}$. It was shown in \cite{Stone:1978vx} that
this is asymptotically equivalent to the \aic strategy for model selection.

An alternative for linear regression models was proposed in
\cite{Allen:1974vx}. The squared difference between the observation and the
predictor is used as the risk estimator. Given a model $y_i = \beta^T x_i +
\varepsilon_i$, and let $\hat{y}_i$ be the prediction of $y_i$ obtained with
the model fitted with all but the $i$\xth observations. Using the left-one-out
procedure, this leads to the \press statistic,
\begin{equation}
  \press = \sum_{i=1}^n(y_i - \hat{y}_i)^2
\end{equation}
The model with the smallest \press value is selected. This is one of the
widespread model selection methods used in regression models.

The performance of cross-validation for model selection depends on both the
risk estimator and partition of the sample. There is a large literature on
cross-validation for various model selection problems. For some models,
specific choice of the function $\gamma$ were proposed. For example, the
\press statistic shown earlier and its more robust variant such as replacing
the squared error by the absolute error \cite[][sec.~2.9]{Claeskens:2008tq}.
Also as argued in the same book, the use of $\gamma(x|\theta) = -\log
f(x|\theta)$ is a sensible choice for many applications, as the resulting
cross-validation estimator can be interpreted as an estimator of the expected
relative \kld.

The partition of the sample can influence the performance more significantly.
And the procedure that minimizes the bias and variance of risk estimator is
not necessary the same as the one that produce best model selection results.
For example, for regression models with random covariates,
\cite{Breiman:1992vx} gave examples that the best risk estimator is obtained
with the left-one-out procedure while a 10-fold cross-validation can produce
more accurate model selection results. More generally, the performance
depend on the asymptotic behavior of $n_t/n$ where $n_t$ is the size of the
training set. For instance, \cite{Shao:1997vx} showed that for linear model
selection, cross-validation is more efficient when $n_t$ is asymptotically
equal to $n$ when use a $k$-fold procedure.

\section{Minimum description length}
\label{sec:Minimum description length}

\section{Bayesian model comparison}
\label{sec:Bayesian model comparison}

Bayes' theorem, in its simplest form is stated as below,
\begin{equation}
  p(H|\data) = \frac{p(\data|H)p(H)}{p(\data)} \label{eq:bayes}
\end{equation}
where $\data$ is the data and $H$ is a hypothesis. Like many other probability
theories, technically Bayes' theorem merely provides a method of accounting
for the uncertainty. There are different interpretations, rooted in the views
of probabilities. One is the \emph{frequentist} (or \emph{statistical}) view
among other so called \emph{objective} views. Another is \emph{subjective}
view. The frequentist view an \emph{event} as a class of \emph{individual
  events} which are not only equally probable and \emph{stochastically
  independent}. With a subjective view, the only thing that matters is
uncertainty and how to make informed and rational discussions under
uncertainty. Some, mostly from an objective perspective, has argued that
Bayesian modeling merely provides a convenient formulation in some situations.
From a subjective perspective, Bayesian statistics provides a rational way to
update one's \emph{personal belief} and to make rational decisions. There has
been debate over views of probability as long as statistics has been a
discipline. These topics, though will not be focused on in this thesis, are
important in both theory and practice. For example, it can affect the choice
of priors. More discussions on these topics can be found in
\cite[][chap.~1]{Bernardo:1994vd}.

A throughout treatment of Bayesian modeling from a decision theory perspective
can be found in \cite{Robert:2007tc}. Formal mathematical representations can
also be found in \cite[][sec.~5.1 and sec.~6.1]{Bernardo:1994vd}. Notion of
rational decisions in the context of uncertainty was also made precise in the
form of axioms in \cite{DeFinetti:1974tg,DeFinetti:1975ua}. It is assumed that
a rational decision cannot be considered separately from rational beliefs. And
rational beliefs shall be built upon available information (the data in the
statistical term) and any personal preference input (the prior information in
Bayesian literatures).

In the remaining of this section, we first introduce the formalization of the
model choice problem within the Bayesian framework. In particular how it leads
to the important Bayes factor, Section~\ref{sub:Bayes factor}. It is followed
by the discussion on the calculation of this quantity. In Section~\ref{sub:The
  choice of priors} we discuss the construction of prior and its particular
relevance to model comparison.

\subsection{The model choice problem}
\label{sub:The model choice problem}

Consider a (possibly infinite) countable set of parametric models, denoted by
$\calM = \{\calM_k\}_{k\in\calK}$. Under each model, the data $\data =
(y_1,\dots,y_n)$ is generated according to a likelihood function
$p(\data|\theta_k,\calM_k)$ where $\theta_k$ is the parameter vector in the
space $\Theta_k\subset\Real^{d_k}$, and $f$ is the density function under the
model. Within the Bayesian framework, a prior distribution is chosen for these
parameters conditional on the model, say $\pi(\theta_k|\calM_k)$. And each
model itself has a prior distribution $\pi(\calM_k)$. Therefore according to
Bayes' theorem, the posterior distribution of the parameters and the model,
conditional upon the data is given by the following density, defined on the
space $\bigcup_{k\in\calK}\{\calM_k\}\times\Theta_k$,
\begin{equation}
  \pi(\theta_k,\calM_k|\data) =
  \frac{p(\data|\theta_k,\calM_k)\pi(\theta_k|\calM_k)\pi(\calM_k)}{p(\data)}
  \label{eq:full posterior}
\end{equation}
where
\begin{align}
  p(\data) &= \sum_{k\in\calK}p(\data|\calM_k)\pi(\calM_k) \\
  p(\data|\calM_k) &=
  \int p(\data|\theta_k,\calM_k)\pi(\theta_k|\calM_k)\intd\theta_k
  \label{eq:marginal likelihood}
\end{align}
This is sometimes also termed the \emph{full posterior}. The within model
posterior distribution of the parameter is,
\begin{equation}
  \pi(\theta_k|\data,\calM_k) =
  \frac{p(\data|\theta_k,\calM_k)\pi(\theta_k|\calM_k)}{p(\data|\calM_k)}
  \label{eq:within posterior}
\end{equation}
The term $p(\data|\calM_k)$ is called the \emph{marginal likelihood} or the
\emph{evidence} of the model. Note that the marginal likelihood is also the
normalizing constant of the posterior $p(\theta_k|\data,\calM_k)$.

From equation~\eqref{eq:full posterior}, it is clear that the posterior model
probability $\pi(\calM_k|\data)$ is a marginal of the full posterior,
\begin{equation}
  \pi(\calM_k|\data) =
  \frac{
    \pi(\calM_k)
    \int p(\data|\theta_k,\calM_k)\pi(\theta_k|\calM_k)\intd\theta_k
  }{
    \sum_{l\in\calK}\pi(\calM_l)
    \int p(\data|\theta_l,\calM_l)\pi(\theta_l|\calM_l)\intd\theta_l
  }.
  \label{eq:post model prob}
\end{equation}
The Bayesian model choice problem mostly centers around the inference of this
posterior model probability. Many methods for computing this probability is
reviewed in chapter~\ref{cha:Monte Carlo Methods}. In the remaining of this
section, we assume that the calculation of required quantities will be
available.

Our aim is to choose the ``best'' model from the set $\calM$. There will
usually be actions taken after the chosen model, for example inference of the
past or prediction of the future. It is the consequences of these actions of
interest instead of the chosen model itself. Therefore, from a
decision-theoretic perspective, the ``best'' model should maximize the utility
for some quality of interest. However, in practice it is common to ignore the
actions following the model selection and the sole interest is the true model,
say $\calM_t$. This is because that the Bayesian framework is often used to
simultaneously provide parameter estimation, model selection, model averaging
and other inferences. In this case, it is natural to define a zero-one utility
function, say $u(\calM_k,\calM_t)$,
\begin{equation}
  u(\calM_k,\calM_t) =
  \begin{cases}
    0, &\text{if } \calM_k = \calM_t,\\
    1  &\text{otherwise.}
  \end{cases}
\end{equation}
It is easy to see that the model $\calM_k$ that maximizes the expected utility
given data $\data$ is the model with the highest posterior probability
$\pi(\calM_k|\data)$, see \cite[][chap.~6]{Bernardo:1994vd}. Also see
\cite[][sec.~7.2.1]{Robert:2007tc} for an in depth discussion of the
difficulties of the Bayesian formulation in the model choice problem and the
reason of why such a maximum posterior probability approach is adapted.

It should be noted the use of the zero-one utility is only valid if the true
model $\calM_t \in \calM$. Otherwise, the utility is always zero for all
models. In what follows, we presume that our aim is to find the model with the
highest posterior probability.

Bayes model selection can be attractive for a few reasons. First, it provides
a natural probabilistic interpretation of the results. It is very easy to
account model uncertainty within this framework. The posterior model
probabilities can be used as weights to construct weighted estimator. This
leads to the Bayesian model averaging. See, for example
\cite{Raftery:1997vx,Clyde:1999vx,Draper:1995vx}, for more discussions and
examples.

Second, it is also consistent in the sense that if there is indeed a true
model, given enough data, it is guaranteed to be selected. Later we will see
some results for the \pet compartmental model showing that Bayesian model
selection indeed provides better results compared to methods such as \aic.

Third, and perhaps a more important factor, the Bayesian framework can be
applied to a wider range of applications. There are very minimal assumptions
about the models under consideration. Model selection methods reviewed earlier
often require the well behavior of estimator, or sufficient large sample size
among other things. In contrast, within Bayesian framework, the regularity of
the likelihood function is not an issue as long as the integrations in
equation~\ref{eq:post model prob} are finite. In addition, though large sample
size can be beneficial in the sense that it can reduce the uncertainty of the
model selection results, it is not necessary. The uncertainty of model
selection is well accounted within the Bayesian framework and improvements can
be obtained through model averaging as mentioned earlier. These advantage
allows Bayesian model selection to be successfully applied for a wide range of
applications.

\subsection{Bayes factor}
\label{sub:Bayes factor}

When the model set $\calM$ is finite, we can find the model with the highest
posterior probability by compare models pairwise. To compare the posterior
probabilities of two models, say $\calM_{k_1}$ and $\calM_{k_2}$, one only
need to compute their ratio. Recall the expression~\eqref{eq:post model prob},
the ratio can be written as,
\begin{equation}
  \frac{\pi(\calM_{k_1}|\data)}{\pi(\calM_{k_2}|\data)}
  = \frac{\pi(\calM_{k_1})}{\pi(\calM_{k_2})} \frac
  {\int p(\data|\theta_{k_1},\calM_{k_1})\pi(\theta_{k_1}|\calM_{k_1})
      \intd\theta_k}
  {\int p(\data|\theta_{k_2},\calM_{k_2})\pi(\theta_{k_2}|\calM_{k_2})
      \intd\theta_k}
  = \frac{\pi(\calM_{k_1})}{\pi(\calM_{k_2})}B_{12},
  \label{eq:posterior odd}
\end{equation}
where
\begin{equation}
  B_{12} = \frac
  {\int p(\data|\theta_{k_1},\calM_{k_1})\pi(\theta_{k_1}|\calM_{k_1})
      \intd\theta_{k_1}}
  {\int p(\data|\theta_{k_2},\calM_{k_2})\pi(\theta_{k_2}|\calM_{k_2})
      \intd\theta_{k_2}}
    = \frac{p(\data|\calM_{k_1})}{p(\data|\calM_{k_2})}
  \label{eq:bayes factor}
\end{equation}
is called the Bayes factor. Equation~\eqref{eq:posterior odd} states how the
prior odds ratio is transformed into the posterior odds ratio by the Bayes
factor \cite{Kass:1995vb}. The Bayes factor is the principle tool for
Bayesian model comparison and model selection. As it is made clear in the
above equation, to compute the Bayes factor, all that needs to be done is the
computation of the marginal likelihood for each model $\calM_k\in\calM$,
\begin{equation}
  p(\data|\calM_k) =
  \int p(\data|\theta_k,\calM_k)\pi(\theta_k|k)\intd\theta_k.
\end{equation}
It is obvious that $B_{ij} = B_{ik} B_{kj}$, and thus the Bayes
factor approach is equivalent to choosing the model with the highest marginal
likelihood, as long as the model set $\calM$ is finite.

The Bayes factor, $B_{1,2}$ can be interpreted as the evidence provided by the
data in favor model $\calM_{k_1}$ against model $\calM_{k_2}$. The marginal
likelihood $p(\data|\calM_k)$ is also called the evidence supporting model
$\calM_k$. In \cite{Jeffreys:1961ua}, it is suggested that the Bayes factor
can be interpreted in a $\log_{10}$ scale. His interpretations are reproduced
in Table~\ref{tab:bayes factor log10 scale}. It shall be noted that the
interpretation of the Bayes factor can be application dependent. The Jeffreys'
interpretation, and a similar scale based on the $2\log B_{12}$, which is on
the same scale as the likelihood ratio test, shown in \cite{Kass:1995vb} are
only general guidelines. Other interpretations can be more suitable for
specific applications. For example, \cite{Kass:1995vb} mentioned that for
forensic evidence to be conclusive in a criminal trial, the posterior odds of
guilt against innocence need to be at least $1000$.

A final remark of the Bayes factor is that, though obviously it can only be
used when the set of candidate models is finite, it is not necessarily
an issue for many interesting cases. The relative ease of computation,
compared to the evaluation of the full posterior probabilities offset the
limitation.

\input{tab/bayes-factor-scale}

The calculation of the Bayes factor can be made exact using analytical results
only occasionally. In most applications of interest, approximations has to be
used. Two approaches are widely used. On is using Monte Carlo approximations.
Another is based on the asymptotic behavior of the Bayes factor. A few of the
later is reviewed in the next to sections.

\subsubsection{Bayesian information criteria}
\label{ssub:Bayesian information criteria}

The Bayesian information criterion (\bic) was developed as a large sample
approximation to the marginal likelihood $p(\data|\theta_k,\calM_k)$
\cite{Schwarz:1978uv}. The \bic is defined as,
\begin{equation}
  \text{\bic} = -2\ell_n(\hat\theta) + k\log(n).
  \label{eq:bic}
\end{equation}
where $\ell_n(\hat\theta)$ is the log-likelihood function evaluated at the
\mle; $k$ is the number of parameters to be estimated and $n$ is the number of
observations. The \bic strategy chooses the model with the smallest \bic
value.

It is clear from the formulation of \bic that the influence of the prior
distribution $\pi(\theta|M)$ is eliminated in this strategy. This also leads
to some argue that \bic is not a full fledged Bayesian approach. A derivation
of \bic can be found in \cite[][sec.~3.2]{Claeskens:2008tq}.

Similar to \aic, the \bic assumes that the sample size is large enough so that
it can approximate the marginal likelihood properly. In addition \bic also
assumes ``good behavior'' in the sense that the \mle is in the high posterior
probability region. As seen in its formulation, \bic eliminates the prior for
large data set. These assumptions restricted the use of \bic in many
situations. For example, see \cite{Berger:2001uy} for examples that the
irregularity of likelihood function causes the \bic method failed. Moreover,
in non-\iid situations, the definition of the parameter dimension $k$ is
ambiguous \cite{Spiegelhalter:1998uc, Kass:1995vb}. There are other criticism
of the \bic strategy. For example \cite[][sec.~7.2.3]{Robert:2007tc} argued
that the \bic strategy eliminated the subjective input into the Bayes modeling
since the value of \bic does not depend on the prior distribution. However
this is equally argued as an advantage of this strategy in the case that
priors, to which Bayes factors can be very sensitive, are hard to specify.

Though \bic is not an estimator of \kld, in \cite{Sin:1996vs} it was shown
that asymptotically \bic is able to choose the simplest model that minimizing
the expected \kld between the true model and the candidate model. In contrast
to \aic (see discussions in Section~\ref{sub:Akaike's information criteria}),
\bic is less subject to overfitting. On the other hand, it may not be as
efficient as \aic for some applications. For example, in \cite{Lee:2001tm} it
was shown that for autoregressive process and some other time series
applications, \bic is not efficient in the sense that \bic may not choose the
model that minimize the prediction error. In
\cite[][sec.~4.7]{Claeskens:2008tq}, it was also shown that \bic is not
efficient for regression variable selection.

In Table~\ref{tab:pet bic} the results of using \bic for model the \pet
compartmental models is shown. Compared to the use of \aicc
(Table~\ref{tab:pet aicc}), the results are quite similar though \bic does
tend to select lower order models slightly more frequently. Intuitively, \bic
penalize complex models more than \aic for $n\ge\EE^2\approx7.4$ (and thus the
penalize term $k\log(n) > 2k$).

\input{tab/pet-bic}

\subsubsection{Laplace approximation}
\label{ssub:Laplace approximation}

An alternative large sample approximation of the marginal likelihood is given
by \cite{Tierney:1986vx},
\begin{equation}
  (2\pi)^{d_k/2}\sqrt{\Abs{-H(\tilde\theta_k)}}
  p(\data|\tilde\theta_k,\calM_k)\pi(\tilde\theta_k|\calM_k)
\end{equation}
where $\tilde\theta_k$ is the maximizer of the posterior density
$\pi(\theta_k|\data,\calM_k)$ and $H(\theta)$ is the Hessian matrix. The
accuracy of the Laplace approximation was examined in \cite{Kass:1992tz} and
other sources. In general, it is an adequate approximation if the likelihood
function is close to Normal \cite{Kass:1995vb}.

Often the maximizer of the posterior density is not easily obtained. A
variant of the Laplace approximation is to use the \mle instead. For
sufficient large sample size, the posterior is likely to peak at the same
region as the likelihood function.

Though less popular than the \bic approximation, the Laplace approximation
does not eliminate the effects of the priors. For some applications, it
provide a low cost (compared to simulation techniques) alternative to evaluate
the Bayes factor over a range of priors. As we will see soon, the choice of
priors may affect the Bayesian model comparison results considerably. See also
the next section on the issue of calibrating priors.

\subsection{The choice of priors}
\label{sub:The choice of priors}

The prior distribution $\pi(\theta_k|\calM_k)$ is chosen by statisticians in
the modeling process. The choice of the prior distribution is one of the most
critical and criticized part of Bayesian modeling. In principle, the prior
distribution shall represent the prior beliefs. That is, it represents how
much is already known about the data generating mechanism and what is believed
about it before the observation of the data, no more or no less. It shall not
only describe all knowledge already known, but also more importantly preserve
all the ignorance. If it contains more information than what is actually
known, the inference can be biased. In other words, one can always construct a
prior distribution such that inference will be biased towards one's
preference, which is not necessarily rational. The priors need to be
considered carefully on a per problem basis. It is often too difficult to
elicit a precise distribution from prior information. Therefore it is
necessary to make at least partially arbitrary choice of the prior
distribution \cite[][chap.~3]{Robert:2007tc}\cite{Kass:1995vb}. Nonetheless
there are a few classes of prior distributions frequently used in the Bayesian
modeling.

\subsubsection{Conjugate priors}
\label{ssub:Conjugate priors}

A conjugate prior, say $\pi(\theta_k|\calM_k)$, for a parametric model with
likelihood function $p(\data|\theta_k,\calM_k)$, is one such that the
posterior $\pi(\theta_k|\data,\calM_K)$ belongs to the same family of
distributions as the prior. In \cite[][sec.~5.2]{Bernardo:1994vd} it was
argued that the conjugate prior reduced the input of prior information to only
the choice of parameter values and thus cannot be fully justified from a
subjective perspective. Though their mathematical simplicity make them
attractive in practice, for many applications of interest it is nearly
impossible to find such priors.

\subsubsection{Non-informative priors}
\label{ssub:Non-informative priors}

In situations where no or little prior information are available, the
so-called ``non-informative'' priors are often used. Many of them are derived
from the data or the likelihood function of the models.

\paragraph{Flat tails}

The simplest form is a uniform distribution or some distribution with flat
tails such as a Cauchy distribution. This choice can provide a robust prior in
the sense that outliers and misspecification of priors will not affect the
results significantly. For example, see \cite{OHagan:1990vx} and
\cite{Fan:1992vx} for analysis of the use Student $t$ distribution as the
prior of location parameters such as the mean of a Normal distribution.

\paragraph{Jeffreys priors}

Jeffreys priors \cite{Jeffreys:1946jf} has the form,
\begin{equation}
  \pi(\theta_k|\calM_k)\propto\sqrt{\Abs{I(\theta_k)}}
\end{equation}
where
\begin{align}
  I(\theta_k)
  &= -\Exp\Square[bigg]{\frac{\partial^2\log p(\data|\theta_k,\calM_k)}
    {\partial\theta_k\partial\theta_k^T}} \notag\\
  &= -\int p(\data|\theta_k,\calM_k)
  \frac{\partial^2\log p (\data|\theta_k,\calM_k)}
  {\partial\theta_k\partial\theta_k^T}\intd\data,
\end{align}
is the \emph{Fisher's information}. The basic idea behind Jeffreys priors is
that the prior shall contain no more information than the observed data do.
The Fisher information is widely accepted as an indicator of the amount of
information brought by the model about the parameter $\theta_k$ given the data
\cite{Fisher:1956vx}. Therefore, intuitively values of $\theta_k$ for which
$I(\theta_k)$ is large is more likely the those $I(\theta_k)$ is small.

However, Jeffreys priors derived for many models may be problematic. It is
often the case that the derived Jeffreys prior can be improper. For example,
the Jeffreys prior for the mean parameter of Normal likelihood is uniform on
the real line. This can create technical difficulties if the improper priors
also lead to improper posteriors since the marginal likelihood, as the
normalizing constant of the posterior cannot be computed. This makes the
Bayesian model choice problem difficult to formalize. See also the discussion
in \cite{Kass:1995vb}. However, improper priors can also lead proper
posterior, for example see \cite[][sec.~2.9]{Gelman:2003vx},
\cite[][sec.~1.5]{Robert:2007tc}, \cite{Kass:1995vb} and references therein
for successful applications using improper priors. In general, for more
complex models using a proper priors is recommended since it might be easier
to verify that the posterior is also proper. In the situation where the
derived Jeffreys prior is improper, it is possible to use some proper
distribution to approximate it. For example, a Gamma distribution can be used
to arbitrarily closely approximate $\pi(x)\propto1/x$, which is often seen as
the Jeffreys prior for scale parameters, such as as the precision parameter
of the Normal distribution.

\paragraph{Reference priors}

Another class of non-informative priors is called \emph{reference priors}
introduced in \cite{Bernardo:1979uq}. Reference priors aims to derive priors
such that the distance between the posterior and prior is maximized, usually
measured in terms of \kld \cite{Kullback:1951va}. In some sense, a reference
prior is the least informative prior. See \cite{Berger:1989vj, Berger:1992kf,
  Berger:1992wo} and \cite[][sec.~5.4]{Bernardo:1994vd} for more information
on this class of priors.

Another way of deriving the reference prior is to partition the parameter
vector $\theta_k = (\theta_k^{(1)},\theta_k^{(2)})$ where $\theta_k^{(1)}$ is
the parameter of interest and $\theta_k^{(2)}$ is the nuisance parameter.
First $\pi(\theta_k^{(2)}|\theta_k^{(1)},\calM_k)$ is defined as the Jeffreys
prior associated with $p(\data|\theta_k,\calM_k)$ when $\theta_k^{(1)}$ is
fixed. Then define the marginal
\begin{equation}
  \tilde{p}(\data|\theta_k^{(1)},\calM_k) =
  \int p(\data|\theta_k,\calM_k)\pi(\theta_k^{(2)}|\theta_k^{(1)},\calM_k)
  \intd \theta_k^{(2)},
\end{equation}
and compute the Jeffreys prior $\pi(\theta_k^{(1)}|\calM_k)$ associated with
$\tilde{p}(\data|\theta_k^{(1)},\calM_k)$. By using the Jeffreys prior, given
fixed parameter of interest, the effects of the nuisance parameter is
eliminated. Note that the above integration may not be defined.

\paragraph{Using non-informative priors for \pet compartmental models}

In \cite{Zhou2013} the results of using non-informative priors for the
Bayesian analysis of the \pet compartmental models were obtained. An
approximation to the Jeffreys prior was used for the precision parameter of
the Normal distributed error. And uniform distributions are used for the rate
constants. The interval of the uniform distributions are the same as
considered feasible in the optimization procedures such as the \nls estimator.
The results of model selection is shown in Table~\ref{tab:pet vague}. Compared
to the results of using \aicc and \bic (Tables~\ref{tab:pet aicc}
and~\ref{tab:pet bic}), it is a vast improvement. For data with low level of
noise there is a high frequency of selecting the true model. For more noisy
data, it is expected the second and third compartments are difficult to
identify. However, the results are much more satisfactory than those of \aicc
and \bic.

\input{tab/pet-vague}

\subsubsection{Informative priors}
\label{ssub:Informative priors}

When the parameters bear real world meaning, it may be possible to construct
informative priors. For some applications, the approach to calibrate an
informative prior requires the substantial expertise and use some model
specific informations. There are also some general methods.

When the parameter space is finite, it might be possible to obtain subjective
evaluation of the probabilities of the different values of the parameter. When
the space is uncountable, the problem is obviously more complicated. One
simple approach is to to partition the parameter space and determine the
probabilities of the parameter falling into each of the partition
\cite[][sec.~3.2.2]{Robert:2007tc}.

A more systematic approach is the \emph{maximum entropy priors}
\cite{Jaynes:1989vx}. Assume that some characteristics of the parameter
$\theta_k$ is known in the form of prior expectations,
\begin{equation}
  \Exp_{\pi}[h_i(\theta_k)|\calM_k] = \gamma_i, \qquad\text{for } i =
  1,\dots,K
  \label{eq:exp constraints}
\end{equation}
where the expectation is with respect to the prior distribution
$\pi(\theta_k|\calM_k)$. In finite setting, define
\begin{equation}
  \calE(\pi) = -\sum_{\theta_k\in\Theta_k}
  \pi(\theta_k|\calM_k)\log \pi (\theta_k|\calM_k)
\end{equation}
where $\Theta_k$ is the parameter space. And the maximum entropy prior is the
$\pi$ that maximizes $\calE$ under the constraints of equations~\ref{eq:exp
  constraints}.

This can be extended to continuous case bey introducing a reference
distribution, say $\pi_0(\theta_k)$. And define the entropy as the negative
\kld between $\pi_0(\theta_k)$ and $\pi(\theta_k|\calM_k)$,
\begin{equation}
  \calE(\pi) = -\int \pi_0(\theta_k)\log\Round[bigg]{
    \frac{\pi_0(\theta_k)}{\pi(\theta_k|\calM_k)}}\intd\theta_k
\end{equation}
The reference distribution $\pi_0$ can be chosen as one of the non-informative
prior discussed earlier. Such choice is justified in
\cite[][chap.~9]{Robert:2007tc}.

The maximum entropy prior in essence select a prior that preserve the prior
information about the expectations of $\{h_i(\theta_k)\}_{i=1}^K$ while make
it as close to the non-informative prior as possible. Therefore prior
knowledges are presented in the prior and ignorance are also preserved.

\paragraph{Using informative priors for \pet compartmental models}

Here we give an example of constructing a biologically informed prior for the
\pet compartmental models. It is based on both prior knowledges of the
underlying biochemical process and mathematical properties of the models.

In \cite{Anderson:1983wk} some useful results about compartmental models in
general was provided. Recall the parameterizations in~\ref{sec:Positron
  emission tomography compartmental model}. Let $\gamma_{0j}$ denote the rate
constant of the outflow from the $j$\xth compartment into the environment.
Without loss of generality, assume that the parameters $\theta_i$ are ordered,
$\theta_1 \le \dots \le \theta_m$. Then,
\begin{enumerate}
  \item $0 \le \theta_i \le 2\max_j|A_{jj}|$ for all $i$.
  \item $\min_j\gamma_{0j} \le \theta_1 \le \max_j\gamma_{0j}$.
  \item when there is only one outflow into the environment, say the rate
    constant of this outflow is $k_2$, as in the plasma input model, then $0
    \le \theta_1 \le k_2$.
\end{enumerate}
In addition, $\sum_{i=1}^m \phi_i = K_1$, where $K_1$ is the rate constant of
input from the plasma into the tissues \cite{Gunn:2001cx}. Therefore $\phi_i <
K_1$ for $i = 1, \dots, m$. Given this information, more informative prior
distributions can be constructed. For simplicity, we restrict discussion to
imposing upper and lower bounds on the possible values of the parameters. As
we subsequently find that inference is not overly sensitive to the prior
specification we do not pursue more complicated approaches.

To demonstrate the idea, an informative prior distributions for a three
tissue compartments model is constructed. First note that the transition
matrix $A$ is,
\begin{equation}
  A = \begin{bmatrix}
    - k_2 - k_3 - k_5 & k_4  & k_6 \\
    k_3               & -k_4 & 0   \\
    k_5               & 0    & -k_6
  \end{bmatrix}.
\end{equation}
It is believed that all the rate constants take values in the range $[5
\times 10^{-4}, 10^{-2}]$. Without loss of generality, we impose the
identifiability constraint $\theta_1 \leq \theta_2 \leq \theta_3$, then,
\begin{gather}
  0 < \theta_1 \le k_2 \le 10^{-2} \\
  \theta_1 \leq \theta_2 \leq \theta_3 \leq \max\{2(k_2 + k_3 + k_5), 2k_4,
  2k_6\} \le 6 \times 10^{-2}
\end{gather}

Under the imposed ordering, as $\theta_1$ is the smallest exponent, the term
$\phi_1e^{-\theta_1 t}$ decays more slowly than any other term in the
expansion. Consequently, $\phi_1/\theta_1$ is likely to make a relatively
large contribution to $V_D = \sum_{i=1}^m \phi_i/\theta_i$. In fact, as $A$
has only negative real eigenvalues, $\theta_1$ is the spectral radius of $A$.
It is not well known how large the ratio $(\phi_1/\theta_1)/V_D$ will be.
However, it is easy to conduct a numerical study here, given the small number
of parameters. It is found that among all possible combination of $k$s,
$\phi_1/ \theta_1 \ge 0.5 V_D$. If the combinations of $k$'s are restricted to
those without excessively large differences between them, i.e., cases in
which, say, $k_5 \gg k_6$ are not considered, then $\phi_1/\theta_1 \ge 0.7
V_D$. The reason for not considering these cases is that such irreversible
(trapped) models yield infinite $V_D$ estimates and it is generally known in
advance that the tracer employed will exhibit reversible dynamics.

With these informations we can then construct informative priors for the
parameters of interest. More technical details can be found in
\cite{Zhou2013}. In particular, truncated Normal distributions are used as the
priors for the parameters given all the upper and lower bounds.
Table~\ref{tab:pet informative} shows the model selection results when using
these informative priors. It can be seen that, especially for data with higher
noise level, the results are considerably improved compared to using vague
priors (Table~\ref{tab:pet vague}).

\input{tab/pet-informative}

\subsubsection{Sensitivity analysis}
\label{ssub:Sensitivity analysis}

Though intuitively the influence of priors can be eliminated given enough
data. In the particular problem of Bayesian model comparison and selection, it
shall be noted that Bayesian model comparison is more sensitive to the choice
of prior than point or interval estimations in the sense that more data are
need to eliminate the influence of priors when it comes to the value of Bayes
factor than a posterior interval or point estimation
\cite{Kass:1993vy,Kass:1995vb}.

It is also important to evaluate the Bayes factor over a range of possible
priors to assess the sensitivity issues. This is often computational expensive
since many high dimensional integrations are required. When there is enough
information to construct parametric priors, it is possible to alter the values
of parameters and recompute the Bayes factor \cite{McCulloch:1991hj}. In
general situations, a less computational expensive method is to use the
Laplace approximation to compare the Bayes factor using different priors
\cite{Kass:1992tz}. It is also proposed in the literature to use the maximum
of Bayes factor (and thus the maximal evidence against a model) to evaluate
the sensitivity problem \cite{Berger:1987iq}.

\section{Discussion}
\label{sec:Model Selection Discussion}
