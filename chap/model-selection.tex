\chapter{Model selection}
\label{cha:Model selection}

Model selection is a problem found throughout statistics and related
disciplines. A number of approaches has been developed through the history of
statistics. We will review some of the more widely used methods. We are
mostly interested in methods that are generic in the sense that their
usefulness is not limited to any particular class of models.

Section~\ref{sec:Information theoretic approach} reviews a few
information-theoretic approaches. The most important one of them is perhaps
the Akaike's information criteria (\aic; \cite{Akaike:1973uc,Akaike:1974ih}).
A few other closely related methods are also reviewed in this section.
Section~\ref{sec:Bayesian model comparison} reviews the Bayesian approach to
model comparison and selection. This chapter is concluded by a discussion of
the methods reviewed.

\section{Information-theoretic approach}
\label{sec:Information theoretic approach}

Information theory is a discipline that covers a wide range of theories and
methods that are fundamental to many scientific disciplines (see e.g.,
\cite{Cover:1991vx} for an overview). The most relevant one here is the
\kldfull (\kld) \cite{Kullback:1951va}, which measures the discrepancy
between two models. Many model selection methods are based on estimators of
this measure of discrepancy.

\subsection{Kullback-Liebler divergence}
\label{sub:Kullback-Liebler divergence}

Assume that the distribution of data is continuous and has a density function
$g$. Let $f(x) = f(x|\theta)$ be the density function of some continuous
parametric distribution, where $\theta$ is the parameter vector. The \kld
between $f$ and $g$ is defined by,
\begin{equation}
  D_{\kl}(g,f) = \int g(x)\log\Round[bigg]{\frac{g(x)}{f(x|\theta)}}\intd x
  \label{eq:kl}
\end{equation}
In \cite{Kullback:1951va} it was originally developed from information
theory, as it relates the ``information'' lost when $f$ is used to
approximate $g$. The \kld is always nonnegative and equals to zero if and
only if $g(x) = f(x)$ everywhere \cite[][sec.~6.8]{Burnham:2002wc}. The
concept can be generalized to discrete distributions and more general
settings \cite[][sec.~2.1.3]{Burnham:2002wc}. For the purpose of simplicity,
in the remainder of this section, we will assume that the distributions under
discussion are continuous.

The criterion of model selection under this theme is thus finding the model
that has the minimum \kld between the true data generating distribuiton and
the model distribution. There are often a set of candidate models. Each model
is defined by a parametric distribution. Therefore model selection can be
viewed a two-step process. First, for each model a value of the parameter
vector is found such that the \kld is minimized within this model across the
parameter space. Second, the model with the smallest \kld among all models is
selected.

It is clear that the calculation of $D_{\kl}(g,f)$ relies on the knowledge of
both $g$, which is unknown, and $f$ as well as the value of the parameter
vector $\theta$. Rewrite Equation~\eqref{eq:kl} as the following,
\begin{align}
  D_{\kl}(g,f)
  &= \int g(x)\log g(x) \intd x - \int g(x) \log f(x|\theta) \intd x \notag\\
  &= \Exp_g[\log g(X)] - \Exp_g[\log f(X|\theta)].
\end{align}
The first term is a constant. Therefore, minimizing $D_{\kl}(g,f)$ is
equivalent to minimizing $(-\Exp_g[\log f(X|\theta)])$. The later is also
called the \emph{relative} \kldfull. Let $\tilde\theta$ denote the value of
the parameter vector that minimizes the relative \kld and $\hat\theta(\data)$
denote an estimator of it, where $\data$ is the data generated from $g$. We
have the minimum and estimated \kld,
\begin{align}
  \tilde{D}_{\kl}(g,f) &= \text{Constant} - \Exp_g[\log f(X|\tilde\theta)], \\
  \hat{D}_{\kl}(g,f) &= \text{Constant} - \Exp_g[\log f(X|\hat\theta(\data))].
\end{align}
Since $\hat\theta(\data)\ne\tilde\theta$ for (almost) all data $\data$, we
have $\hat{D}_{\kl}(g,f) > \tilde{D}_{\kl}(g,f)$. An alternative criterion is
the expected value of $\hat{D}_{\kl}(g,f)$,
\begin{equation}
  \bar{D}_{\kl} = \text{Constant} -
  \Exp_{\hat\theta}\Exp_g[\log f(X|\hat\theta(\data))]
  \label{eq:expected kl}
\end{equation}
where the outer expectation is with respect to $g$ and integrates out the
estimated parameter $\hat\theta(\data)$. It is again almost always larger than
the minimum relative \kld. However, using the expected \kld as a model
selection criterion allow us to select models that \emph{on average} minimize
the estimated \kld. Note that we cannot compute this term analytically since
it depends on the true model $g$, which is assumed to be unknown. The model
selection criteria discussed below rely on approximations of this quantity.
Consequently, these methods attempt to select the model that on average
minimizes, over a set of models, the expected \kld.

\subsection{Akaike's information criteria}
\label{sub:Akaike's information criteria}

The \aic strategy is based on an observation of the relationship between the
maximum likelihood estimator (\mle) and the \kld. Let $\data =
(y_1,\dots,y_n)$ denotes \iid samples generated from $g$. Then by the Strong
Law of Large Numbers (\slln),
\begin{equation}
  \frac{1}{n}\ell_n(\theta) \xrightarrow{\text{a.s.}} \Exp_g[\log f(Y|\theta)]
  \label{eq:ml convergence}
\end{equation}
where $\ell_n(\theta) = \sum_{i=1}^n \log f(y_i|\theta)$ is the
log-likelihood function. This suggests the use of the \mle, denoted by
$\hat\theta$, which maximizes $\ell_n(\theta)$ as an estimator of
$\tilde\theta$, which minimizes $D_{\kl}(g,f)$. The expected \kld
$\bar{D}_{\kl}(g,f)$ can be approximated by empirical average
$\ell_n(\hat\theta)$, upto an additive constant that is the same for all
models. However, as shown in \cite{Akaike:1973uc}, this approximation is
systematically biased upward (also see \cite[][sec.~2.3]{Claeskens:2008tq}
for some remarks on this bias). It can be shown that the bias is
approximately $k/n$ where $k$ is the length of the parameter vector $\theta$.
This leads to the adjusted estimator of the expected relative \kld,
\begin{equation}
  -\frac{1}{n}\ell_n(\hat\theta) + \frac{k}{n}.
\end{equation}
In \cite{Akaike:1973uc} it is rescaled to,
\begin{equation}
  \aic = -2\ell_n(\hat\theta) + 2k
  \label{eq:aic}
\end{equation}
and the \aic strategy selects the model with the smallest value of \aic.

A more rigorous derivation of Equation~\eqref{eq:aic} can be found in
\cite[][sec.~2.3]{Claeskens:2008tq} and \cite[][sec.~6.2]{Burnham:2002wc}.
Here, we are more interested in the conditions under which this approximation
is good enough for the purpose of model selection. Some remarks below are
given without proof. For technical details, see the two references of the
derivation of \aic.

First, the derivation of the bias term is based on a first order Taylor
expansion of $\Exp_g[\ell_n(\hat\theta)/n - \bar{D}_{\kl}(g,f)]$. The
accuracy is of order $o(n)$. The assumption about the parametric model $f$ is
quite minimal. Given more informations about the structure of the models,
more accurate estimator can be derived by using a second order expansion
(discussed later in Section~\ref{sub:A second order aic}).

Second, more importantly, \aic assumes that the candidate models are close
enough to the true model. When there is significant misspecification of the
models, the results from using the \aic method can be misleading. Estimator
of the \kld that are more model robust can be derived. Later, in
Section~\ref{sub:Takeuchi's information criterion} a more general estimator
of the expected relative \kld is discussed, of which \aic is a special case.

Third, though earlier we assumed \iid samples, which leads to the
convergence~\eqref{eq:ml convergence} as a motivation of using the \mle for
the estimation of expected relative \kld. This is not necessary for the
derivation of \aic. The \aic model selection method has also been
successfully used for dependent data. For example, \cite{Lee:2001tm} shows
that \aic is efficient for selecting the order of an autoregressive process.
However, \aic does assume that the model distribution is well behaved in the
sense that the estimator used to evaluate the criterion is indeed close to
the minimizer of the \kld.

The last but not least, \aic has the tendency of selecting more complex
models in the sense that when there are multiple models with the minimum
expected \kld, \aic is likely to choose the model with more parameters.
Intuitively, the log-likelihood function increases linearly as the sample
size grows, while the penalty term $2k$ is not affected. Therefore, the \aic
strategy is likely going to select more complex models when more data becomes
available. This is formally shown in \cite{Sin:1996vs}: When there are more
than one model that minimizes the \kld, \aic does not necessarily choose the
simplest model.

\subsection{A second order \protect\aic}
\label{sub:A second order aic}

As shown in \cite{Sugiura:1978be}, the first order approximation can perform
poorly when the data size is small (compared to the number of parameters to
be estimated). A second order variant is derived in the same paper and
further studied by \cite{Hurvich:1989ev}, which led to a criterion that is
called \aicc, the \emph{corrected} \aic,
\begin{equation}
  \aicc = -2\ell_n(\hat\theta) + \frac{2nk}{n-k-1}.
\end{equation}
It is clear that the additional bias correction is negligible if $n$ is large
when compared to $k$, as $\lim_{n\to\infty}2nk/(n-k-1) = 2k$, which is
exactly the penalty term in the original \aic formula. A rule of thumb, found
in various source, is that \aicc should be used in place of \aic when
$n/k\le40$; see e.g., \cite[][sec.~2.4]{Burnham:2002wc}.

\aicc is just one way to improve \aic for small sample size. In particular,
it is derived in the case of a model with linear structure and Gaussian
errors (see \cite{Hurvich:1989ev} and \cite[][sec.~6.4.1]{Burnham:2002wc} for
derivations of \aicc). With other models, other forms of improved \aic can be
derived. However, this form has also been used successfully in literature
even in nonlinear non-Gaussian cases. For example see
\cite{Turkheimer:2003iy} for its applications to \pet compartmental models.

Both \aic and \aicc assumes the use of the \mle for the computation of the
criteria. However, in many nonlinear applications, the estimator is obtained
through optimization of criteria other than the likelihood function. For
example, nonlinear least squares (\nls) estimator and other optimization
procedures are widely used in the estimation of the \pet compartmental
models. Model selection criteria are computed with these estimators. These
estimators are commonly used because of their ease of computation and other
properties. However, the model selection results obtained this way may not be
satisfactory.

For example, in \cite{Zhou2013} the model selection for \pet compartmental
models using the \aic and other methods were studies. Table~\ref{tab:pet
aicc} shows the frequencies of models selected by the \aicc for the 2,000
data sets simulated from a three-compartments model (see
Section~\ref{sec:Simulated and real pet data}) while using the \nls
estimator. It can be seen that, for data sets with small noise levels (the
highest variance of the Normally distributed error added to the simulated
time series, with others scaled in proportion), the \aicc method is able to
select the two-compartments model with a very high frequency. Though this is
not the true model that generated the data, it is very close as the third
compartment is difficult to identify (see discussion in \cite{Zhou2013} and
references therein). However, when the noise level increases, the method is
unable to identify the second compartment.

\input{tab/pet-aic.tex}

It is possible to derive more accurate second or even higher order
approximations to the expected relative \kld for some models. However, this
may not be feasible for some realistic applications. For example, the \pet
compartmental model does not have an explicit form of the likelihood
function, which may create significant technical difficulty if we want to
refine the \aic approximation. More importantly, such refinement of \aic
relies on assumptions about the explicit form of the true model $g$ or one
that closely imitates it. When the form of the model is drastically different
from the one used to derive criteria such as \aicc, poor results of model
selection is likely to be obtained as we have seen here.

\subsection{Takeuchi's information criterion}
\label{sub:Takeuchi's information criterion}

As stated earlier, \aic (and some of its refinements such as \aicc) depends
on the assumption that the candidate models are close to the one that
generated the data. However, this might not be the case in reality. In
\cite{Takeuchi:1976vx}, a general derivation from \kld to \aic was developed.
An intermediate result indicated a selection criterion useful when there is
considerable model misspecification, formulated as \tic,
\begin{equation}
  \tic = -2\ell_n(\theta) + 2\tr(H(\theta)K(\theta)^{-1})
  \label{eq:tic}
\end{equation}
where $-H(\theta)$ is the expectation of the Hessian matrix and $K(\theta)$
is the variance matrix of the score vector, respectively, that is,
\begin{equation}
  H(\theta) = -\Exp_g\Square[bigg]{
    \frac{\partial^2\log f(X|\theta)}{\partial\theta\partial\theta^T}
  }\quad\text{and}\quad
  K(\theta) = \var_g\Square[bigg]{
    \frac{\partial\log f(X|\theta)}{\partial\theta}
  },
\end{equation}
provided that all differentiations and integrations exist. The expectations
are taken with respect to the true data generating distribution $g$. Ideally
\tic should be evaluated at the minimizer of the \kld, $\tilde\theta$. In
reality, the \mle is often used to evaluate the likelihood function and
various estimator of the bias correction term,
$\tr(H(\tilde\theta)K(\tilde\theta)^{-1})$, has been developed (see e.g.,
\cite{Claeskens:2008tq}). If the \mle is well behaved \cite{Lehmann:1983vx},
then we can substitute the \mle into Equation~\eqref{eq:tic} and use
empirical averages as estimates of $H(\hat\theta)K(\hat\theta)^{-1}$. The
explicit form of the bias correction term can also be derived for some
models. For example, see \cite[][sec.~6.6]{Burnham:2002wc}. This allows more
accurate evaluations of \tic.

Note that, since $\tilde\theta$ minimizes the \kld, the expectation of the
score vector is a zero vector when evaluated at $\tilde\theta$, under
suitable continuity conditions. Further, if $g(x) = f(x|\tilde\theta)$
everywhere, then it is obvious that the above two matrices are equal and
become the \emph{Fisher information matrix}, provided the ability to exchange
the order of integrations and differentiations and other regularity
conditions. In this case, $\tr(H(\theta)K(\theta)^{-1}) = k$, and the \tic
leads to the \aic formula. It becomes clear now that \aic is an approximation
of \tic in the situation where the candidate models are close to the true
data generating mechanism. The \tic method does not have such assumptions and
may perform considerably better than \aic in the situation of model
misspecification.

Unlike the refinements of \aic such as \aicc, the \tic method relies heavily
on the assumption of large sample size in order to obtain accurate estimation
of the bias term. It is difficult to derive small sample correction for the
\tic approximation. See also the discussions in
\cite[][sec.~6.7.8]{Burnham:2002wc}

\subsection{Cross-validation}
\label{sub:Cross-validation}

Cross-validation has a long history in applied and theoretical statistics. It
has been formalized in \cite{Geisser:1975vx} and \cite{Stone:1974vx} (also
see the introduction in this paper for an overview of earlier development on
this method). The basic idea is to split the data into two parts. One part of
the data is used for model fitting and the resulting estimates of parameters
are used to predict the other part of the data. By comparing the predictions
based on part of the data and the observed other part, the usefulness of the
model is determined.

Formally, following \cite{Geisser:1975vx}, let $\data = (y_1,\dots,y_n)$ be
the data set and $\data^t\subset\data$ be a non-empty proper subset. The
subsample $\data^t$ is called the \emph{training set} and its complement
$\data^v = \data\backslash\data^t$ is called the \emph{validation set}. For
each model, defined by a parametric distribution with density
$f(\cdot|\theta)$, a loss function is defined, say $\gamma(\cdot|\theta)$.
The choice of the loss function $\gamma$ is formally arbitrary. It is taken
as a measurement of the fitness of the model. A commonly used one is the log
density function, $\gamma(x|\theta) = -\log f(x|\theta)$ \cite{Stone:1977vx}.
The risk estimator of $\Exp_g[\gamma(X|\hat\theta(\data))]$, where
$\hat\theta(\data)$ is the estimate obtained with all data and the
expectation is taken with respect to the unknown distribution $g$ that
generates the data, is obtained through averaging over the left-out data,
\begin{equation}
  \hat{R}_f^v(\data,\data^t) = \frac{1}{\Abs{\data\backslash\data^t}}
  \sum_{y\in\data\backslash\data^t}\gamma(y|\hat\theta(\data^t))
\end{equation}
where $\hat\theta(\data^t)$ is the parameter estimate obtained with only the
training set $\data^t$. Further, let $\data_1^t,\dots,\data_m^t$ be a
sequence of non-empty proper subsets of $\data$. The cross-validation
estimator of the risk is defined as,
\begin{equation}
  \hat{R}_f^{\cv}(\data,\{\data_i^t\}_{i=1}^m) =
  \frac{1}{m}\sum_{i=1}^m \hat{R}_f^v(\data,\data_i^t).
\end{equation}
The model selection proceeds to choose the model with the smallest value of the estimated risk $\hat{R}_f^{\cv}(\data,\{\data_i^t\}_{i=1}^m)$.

An alternative, as seen in \cite{Yang:2007vx}, is called cross-validation
\emph{with voting}. A model with density $f_1$ is chosen over a model with
density $f_2$ if and only if $\hat{R}_{f_1}^v(\data,\data_i^t) <
\hat{R}_{f_2}^v(\data,\data_i^t)$ for a majority of the partitions of the
data $\data$. When there are multiple candidate models, the same paper
proposed the following procedure: For each partition of the data $\data$, and
the corresponding training set $\data_i^t$ and validation set $\data_i^v =
\data\backslash\data_i^t$, a model with the smallest value of
$\hat{R}_f^v(\data,\data_i^t)$ is selected. Then the model selected most
frequently (the most voted) among all partitions is chosen as the best model.

There are different ways to split the sample. The most commonly used is
perhaps the \emph{leave-one-out} procedure
\cite{Stone:1974vx,Geisser:1975vx}. In this case, training sets $\data_i^t =
\data\backslash\{y_i\}$ for $i = 1,\dots,n$ are used. A more general scheme
is that $k$ observations are left out for each training set and all possible
combinations are considered \cite{Shao:1993vx}. It is clear that $k=1$ yields
the leave-one-out procedure and for large sample size, a modest $k$ can lead
to higher computational cost as the number of possible partitions is the
binomial coefficient. This is also called the $k$\emph{-fold} procedure.
Other procedures are also possible. For more informations we refer to
\cite{Stone:1978vx} and \cite{Hjorth:1994vx}.

There are also different choices of the loss function $\gamma$. The one
mentioned earlier, $\gamma(x|\theta) = -\log f(x|\theta)$, when combined with
the leave-one-out procedure, leads to the estimator,
\begin{equation}
  -\frac{1}{n}\sum_{i=1}^n\log f(y_i|\hat\theta(\data_i^t))
\end{equation}
where $\hat\theta(\data_i^t)$ is the estimate obtained using the subsample
$\data_i^t = \data\backslash\{y_i\}$. It was shown in \cite{Stone:1978vx} that
this is asymptotically equivalent to the \aic strategy for model selection.

An alternative loss function for linear regression models was proposed in
\cite{Allen:1974vx}. The squared difference between the observation and the
predictor is used as the risk estimator. Given a model $y_i = \beta^T x_i +
\varepsilon_i$, and let $\hat{y}_i$ be the predictor of $y_i$ obtained with
the model fitted with all but the $i$\xth observation, i.e., using the
leave-one-out procedure, this leads to the \press statistic,
\begin{equation}
  \press = \sum_{i=1}^n(y_i - \hat{y}_i)^2
\end{equation}
The model with the smallest \press value is selected. This is one of the
commonly used model selection methods used in regression models.

The performance of cross-validation for model selection depends on both the
choice of the loss function and the partition of the sample. There is a large
amount of literature on cross-validation for various model selection
problems. For some models, specific choice of the function $\gamma$ were
proposed. For example, the \press statistic shown earlier and its more robust
variant such as replacing the squared error by the absolute error
\cite[][sec.~2.9]{Claeskens:2008tq}. Also as argued in the same book, the use
of $\gamma(x|\theta) = -\log f(x|\theta)$ is a sensible choice for many
applications, as the resulting cross-validation estimator can be interpreted
as an estimator of the expected relative \kld.

The partition of the sample can influence the performance more significantly.
And the procedure that minimizes the bias and variance of the risk estimator
is not necessary the same as the one that produces the best model selection
results. For example, for regression models with random covariates,
\cite{Breiman:1992vx} gave examples that the best risk estimator is obtained
with the leave-one-out procedure while a 10-fold cross-validation can produce
more accurate model selection results. More generally, the performance
depends on the asymptotic behavior of $n_t/n$ where $n_t$ is the size of the
training set. For instance, \cite{Shao:1997vx} showed that for linear model
selection, cross-validation is more efficient when $n_t$ is asymptotically
equal to $n$ while using a $k$-fold procedure.

\section{Bayesian model comparison}
\label{sec:Bayesian model comparison}

Bayes' theorem, in its simplest form is stated as below,
\begin{equation}
  p(H|\data) = \frac{\Pr(\data|H)\Pr(H)}{\Pr(\data)} \label{eq:bayes}
\end{equation}
where $\data$ is the data and $H$ is a hypothesis. Like many other
probability theories, technically Bayes' theorem merely provides a method of
accounting for the uncertainty. There are different interpretations, rooted
in the views of probabilities. See \cite[][chap.~1]{Bernardo:1994vd} and
references therein for discussions on this topic. In this thesis, we are more
concerned with the practical applications of the Bayesian model comparison
technique, its computational difficulties, and its implementation for
realistic models. More philosophical issues will not be elaborated in this
thesis.

A throughout treatment of Bayesian modeling from a decision-theoretic
perspective can be found in \cite{Robert:2007tc}. Formal mathematical
representations can also be found in \cite[][sec.~5.1 and
sec.~6.1]{Bernardo:1994vd}. Notions of rational decisions in the context of
uncertainty was also made precise in the form of axioms in
\cite{DeFinetti:1974tg,DeFinetti:1975ua}. It is assumed that a rational
decision cannot be considered separately from rational beliefs. And rational
beliefs shall be built upon available information (the data) and any personal
preference input (the prior information).

In the remainder of this section, we first introduce the formalization of the
model choice problem within the Bayesian framework. It leads to the important
Bayes factor, discussed in Section~\ref{sub:Bayes factor}. In
Section~\ref{sub:Choice of priors} we discuss the construction of prior and
its particular relevance to the Bayesian model comparison problem.

\subsection{Model choice problems}
\label{sub:Model choice problems}

Consider a (possibly infinite) countable set of parametric models, denoted by
$\calM = \{\calM_k\}_{k\in\calK}$. Under each model, the data $\data =
(y_1,\dots,y_n)$ is generated according to a likelihood function
$p(\data|\theta_k,\calM_k)$ where $\theta_k$ is the parameter vector in the
space $\Theta_k\subset\Real^{d_k}$. Within the Bayesian framework, a prior
distribution is chosen for the parameters conditional upon the model, say
$\pi(\theta_k|\calM_k)$. And each model itself has a prior distribution
$\pi(\calM_k)$. For the purpose of simplicity, all distributions are assumed
to be continuous except $\pi(\calM_k)$, which is assumed to be discrete.
According to Bayes' theorem, the posterior distribution of the parameters and
the model, conditional upon the data, is given by the following density,
defined on the space $\bigcup_{k\in\calK}\{\calM_k\}\times\Theta_k$,
\begin{equation}
  \pi(\theta_k,\calM_k|\data) =
  \frac{p(\data|\theta_k,\calM_k)\pi(\theta_k|\calM_k)\pi(\calM_k)}{p(\data)},
  \label{eq:full posterior}
\end{equation}
where
\begin{align}
  p(\data) &= \sum_{k\in\calK}p(\data|\calM_k)\pi(\calM_k), \\
  p(\data|\calM_k) &=
  \int p(\data|\theta_k,\calM_k)\pi(\theta_k|\calM_k)\intd\theta_k.
  \label{eq:marginal likelihood}
\end{align}
The distribution $\pi(\theta_k,\calM_k|\data)$ is termed the \emph{full
  posterior}. The within model posterior distribution of the parameters is
given by,
\begin{equation}
  \pi(\theta_k|\data,\calM_k) =
  \frac{p(\data|\theta_k,\calM_k)\pi(\theta_k|\calM_k)}{p(\data|\calM_k)}.
  \label{eq:within posterior}
\end{equation}
The term $p(\data|\calM_k)$ is called the \emph{marginal likelihood} or the
\emph{evidence} of the model. Note that the marginal likelihood is also the
normalizing constant of the posterior $p(\theta_k|\data,\calM_k)$.

From Equation~\eqref{eq:full posterior}, it is clear that the posterior model
probability $\pi(\calM_k|\data)$ is a marginal of the full posterior, and can
be calcualted given the prior $\pi(\calM_k)$,
\begin{equation}
  \pi(\calM_k|\data) = \frac{
    \pi(\calM_k)
    \int p(\data|\theta_k,\calM_k)\pi(\theta_k|\calM_k)\intd\theta_k
  }{
    \sum_{l\in\calK}\pi(\calM_l)
    \int p(\data|\theta_l,\calM_l)\pi(\theta_l|\calM_l)\intd\theta_l
  }.
  \label{eq:post model prob}
\end{equation}
The Bayesian model choice problem mostly centers around the inference of this
posterior model probability. Many methods for computing this probability is
reviewed in Chapter~\ref{cha:Monte Carlo Methods}. In the remainder of this
section, we assume that the calculation of required quantities is possible and
accurate.

Our aim is to choose the ``best'' model from the set $\calM$. There will
usually be actions taken after the model selection, for example, parameter
estimation, or prediction of future events, etc. It is the consequences of
these actions of interest instead of the chosen model itself. Therefore, from
a decision-theoretic perspective, the ``best'' model should maximize the
utility for some quality of interest. However, in practice it is common to
ignore the actions following the model selection and the sole interest is the
true model, say $\calM_t$. This is because that the Bayesian framework is
often used to simultaneously provide parameter estimation, model selection,
model averaging and other inferences. It is difficult to define a criterion
that chooses models best for all these purposes. In the simplified setting,
where only the true model is of interest, it is natural to define a zero-one
utility function, say $u(\calM_k,\calM_t)$,
\begin{equation}
  u(\calM_k,\calM_t) =
  \begin{cases}
    0, &\text{if } \calM_k = \calM_t,\\
    1  &\text{otherwise.}
  \end{cases}
\end{equation}
It is easy to see that the model $\calM_k$ that maximizes the expected
utility given data $\data$ is the model with the highest posterior
probability $\pi(\calM_k|\data)$ \cite[][chap.~6]{Bernardo:1994vd}. Also see
\cite[][sec.~7.2.1]{Robert:2007tc} for an in-depth discussion of the
difficulties of the Bayesian formulation in the model choice problem and the
reason of why such a simplified maximum posterior probability approach is
used.

It should be noted that the use of the zero-one utility is only valid if the
true model $\calM_t$ belongs to $\calM$. Otherwise, the utility is always zero
for all models. In what follows, we presume that our aim is to find the model
with the highest posterior probability.

Bayesian model selection can be attractive for a few reasons. First, it
provides a natural probabilistic interpretation of the results. It is very
easy to account model uncertainty within this framework. When there are more
than one models well supported by the data and it is uncertain which one shall
be chosen as the best model, the posterior model probabilities can be used as
weights to construct weighted estimator. This leads to Bayesian model
averaging. See, e.g., \cite{Raftery:1997vx,Clyde:1999vx,Draper:1995vx}, for
more discussions and examples.

Second, it is also consistent in the sense that if there is indeed a true
model, given enough data, it is guaranteed to be selected. Later we will see
some results for the \pet compartmental model showing that Bayesian model
selection indeed provides better results compared to methods such as \aic.

Third, and perhaps a more important factor, the Bayesian framework can be
applied to a wider range of applications compared to methods based on
asymptotic behaviors of the data. There are very minimal assumptions about
the models under consideration. Model selection methods reviewed earlier
often require the well behavior of estimator, or sufficient large sample size
among other things. In contrast, within Bayesian framework, the regularity of
the likelihood function is not an issue as long as the integrations in
Equation~\eqref{eq:post model prob} are finite. In addition, though large
sample size can be beneficial in the sense that it can reduce the uncertainty
of the model selection results, it is not necessary. The uncertainty of model
selection is well accounted within the Bayesian framework and improvements
can be obtained through model averaging as mentioned earlier. These
advantages allow Bayesian model selection to be successfully applied for a
wide range of applications.

\subsection{Bayes factor}
\label{sub:Bayes factor}

When the model set $\calM$ is finite, we can find the model with the highest
posterior probability by compare models pairwise. To compare the posterior
probabilities of two models, say $\calM_{k_1}$ and $\calM_{k_2}$, one only
needs to compute their ratio. Recall Equation~\eqref{eq:post model prob}, the
ratio can be written as,
\begin{equation}
  \frac{\pi(\calM_{k_1}|\data)}{\pi(\calM_{k_2}|\data)}
  = \frac{\pi(\calM_{k_1})}{\pi(\calM_{k_2})} \frac
  {\int p(\data|\theta_{k_1},\calM_{k_1})\pi(\theta_{k_1}|\calM_{k_1})
      \intd\theta_k}
  {\int p(\data|\theta_{k_2},\calM_{k_2})\pi(\theta_{k_2}|\calM_{k_2})
      \intd\theta_k}
  = \frac{\pi(\calM_{k_1})}{\pi(\calM_{k_2})}B_{k_1k_2},
  \label{eq:posterior odd}
\end{equation}
where
\begin{equation}
  B_{k_1k_2} = \frac
  {\int p(\data|\theta_{k_1},\calM_{k_1})\pi(\theta_{k_1}|\calM_{k_1})
      \intd\theta_{k_1}}
  {\int p(\data|\theta_{k_2},\calM_{k_2})\pi(\theta_{k_2}|\calM_{k_2})
      \intd\theta_{k_2}}
    = \frac{p(\data|\calM_{k_1})}{p(\data|\calM_{k_2})}
  \label{eq:bayes factor}
\end{equation}
is called the \emph{Bayes factor}. Equation~\eqref{eq:posterior odd} states
how the prior odds ratio is transformed into the posterior odds ratio by the
Bayes factor \cite{Kass:1995vb}. The Bayes factor is the principle tool for
Bayesian model comparison and model selection. As it is made clear in the
above equation, to compute the Bayes factor, all that needs to be done is the
computation of the marginal likelihood for each model $\calM_k\in\calM$,
\begin{equation*}
  p(\data|\calM_k) =
  \int p(\data|\theta_k,\calM_k)\pi(\theta_k|k)\intd\theta_k.
\end{equation*}
It is obvious that $B_{ij} = B_{ik}B_{kj}$, and thus the Bayes factor
approach is equivalent to choosing the model with the highest marginal
likelihood provided that the model prior distribution $\pi(\calM_k)$ is
uniform, as long as the model set $\calM$ is finite.

The Bayes factor, $B_{k_1k_2}$, can be interpreted as the evidence provided
by the data in favor of model $\calM_{k_1}$ against model $\calM_{k_2}$. As
noted earlier, the marginal likelihood $p(\data|\calM_k)$ is also called the
evidence supporting model $\calM_k$. In \cite{Jeffreys:1961ua} it is
suggested that the Bayes factor can be interpreted on a $\log_{10}$ scale.
His interpretations are reproduced in Table~\ref{tab:bayes factor log10
scale}. The interpretation of the Bayes factor can be application dependent.
The Jeffreys' interpretation, and a similar scale based on $2\log
B_{k_1k_2}$, which is on the same scale as the likelihood ratio test
\cite{Kass:1995vb}, are only general guidelines. Other interpretations can be
more suitable for specific applications. For example, \cite{Kass:1995vb}
mentioned that for forensic evidence to be conclusive in a criminal trial,
the posterior odds of guilt against innocence need to be at least 1,000.

\input{tab/bayes-factor-scale}

A final remark about the Bayes factor is that, though obviously it can only
be used when the set of candidate models is finite, it is not necessarily an
issue for many interesting cases. As we will see later in the next chapter,
evaluating the marginal likelihood by simulating samples from the model
posterior can be relatively easy compared to to evaluating the full posterior
probabilities. The ease of computation might offset the limitation that only
finite sets of models can be dealt with.

The calculation of the Bayes factor can be made exact using analytical results
only occasionally. In most applications of interest, approximations has to be
used. Two approaches are widely used. One is to use Monte Carlo
approximations. Another is based on the asymptotic behavior of the Bayes
factor. Two of the later are reviewed here.

\subsubsection{Bayesian information criteria}
\label{ssub:Bayesian information criteria}

The Bayesian information criterion (\bic) was developed as a large sample
approximation to the marginal likelihood $p(\data|\theta_k,\calM_k)$
\cite{Schwarz:1978uv}. The \bic is defined as,
\begin{equation}
  \text{\bic} = -2\ell_n(\hat\theta_k) + k\log(n),
  \label{eq:bic}
\end{equation}
where $\ell_n(\hat\theta_k)$ is the log-likelihood function evaluated at the
\mle, $k$ is the number of parameters to be estimated and $n$ is the number
of observations. The \bic strategy chooses the model with the smallest value
of \bic.

It is clear from the formulation of \bic that the influence of the prior
distribution $\pi(\theta_k|\calM_k)$ is eliminated in this strategy. This
also leads to some argue that the \bic stragey is not a full fledged Bayesian
approach. A derivation of \bic can be found in
\cite[][sec.~3.2]{Claeskens:2008tq}.

Similar to \aic, \bic assumes that the sample size is large enough in order
to approximate the marginal likelihood properly. In addition, \bic also
assumes ``good behavior'' of the likelihood function in the sense that the
\mle is in the high posterior probability region. These assumptions
restricted the use of \bic in some situations. See \cite{Berger:2001uy}
for examples where the irregularity of the likelihood function causes the
\bic method failed. There are other criticism of the \bic strategy. For
example \cite[][sec.~7.2.3]{Robert:2007tc} argued that the \bic strategy
eliminated the subjective input into the Bayes modeling since the value of
\bic does not depend on the prior distribution. However this is equally
argued as an advantage of this strategy in the case that priors, to which the
Bayes factors can be very sensitive, are hard to specify.

Though \bic is not an estimator of the \kldfull, in \cite{Sin:1996vs} it was
shown that asymptotically \bic is able to choose the simplest model that
minimizes the expected \kld between the true model and the candidate model.
In contrast to \aic (see discussions in Section~\ref{sub:Akaike's information
criteria}), \bic is less subject to overfitting. On the other hand, it may
not be as efficient as \aic for some applications. For example, in
\cite{Lee:2001tm} it was shown that for autoregressive process and some other
time series applications, \bic is not efficient in the sense that \bic may
not choose the model that minimizes the prediction error. In
\cite[][sec.~4.7]{Claeskens:2008tq}, it was also shown that \bic is not
efficient for regression variable selection.

Table~\ref{tab:pet bic} shows the frequencies of models selected by the \bic
for the 2,000 \pet data sets simulated from the three-compartments model (see
Section~\ref{sec:Simulated and real pet data}) while using the \nls
estimator. Compared to the use of \aicc (Table~\ref{tab:pet aicc}), the
results are quite similar though \bic does tend to select lower order models
more frequently. Intuitively, \bic penalizes model complexity more than \aic
for $n\ge\EE^2\approx7.4$ (and thus the penalty term $k\log(n) > 2k$).

\input{tab/pet-bic}

\subsubsection{Laplace approximation}
\label{ssub:Laplace approximation}

An alternative large sample approximation of the marginal likelihood is given
by \cite{Tierney:1986vx},
\begin{equation}
  (2\pi)^{d_k/2}\sqrt{\Abs{-H(\tilde\theta)^{-1})}}
  p(\data|\tilde\theta_k,\calM_k)\pi(\tilde\theta_k|\calM_k)
\end{equation}
where $\tilde\theta_k$ is the maximizer of the posterior density
$\pi(\theta_k|\data,\calM_k)$ and $H(\tilde\theta)$ denotes the Hessian
matrix evaluated at $\tilde\theta$. The accuracy of the Laplace approximation
was examined in \cite{Kass:1992tz} and other sources. In general, it is an
adequate approximation if the likelihood function is close to Normal
\cite{Kass:1995vb}.

Often the maximizer of the posterior density is not easily obtained. A
variant of the Laplace approximation is to use the \mle instead. For a
sufficient large sample size, the posterior is likely to peak at the same
region as the likelihood function.

Though less commonly used than the \bic approximation, the Laplace
approximation does not eliminate the effects of the priors. For some
applications, it provides a low cost (compared to simulation techniques)
alternative for evaluating the Bayes factor over a range of priors.

\subsection{Choice of priors}
\label{sub:Choice of priors}

The prior distribution $\pi(\theta_k|\calM_k)$ is chosen by statisticians in
the modeling process. The choice of the prior distribution is one of the most
critical and criticized part of Bayesian modeling. In principle, the prior
distribution shall represent the prior beliefs. That is, it represents how
much is already known about the data generating mechanism and what is
believed about it before the observation of the data, no more or no less. It
shall not only describe all knowledge already known, but also more
importantly preserve all ignorance. If it contains more informations than
what is actually known, the inference can be biased. In other words, one can
always construct a prior distribution such that inference will be biased
towards one's preference, which is not necessarily rational. Ideally, the
priors need to be considered carefully on a per problem basis. It is often
too difficult to elicit a precise distribution from prior information.
Therefore it is necessary to make at least partially arbitrary choice of the
prior distribution \cite[][chap.~3]{Robert:2007tc}\cite{Kass:1995vb}.
Nonetheless there are a few classes of prior distributions frequently used in
practice.

\subsubsection{Conjugate priors}
\label{ssub:Conjugate priors}

A conjugate prior, say $\pi(\theta_k|\calM_k)$, for a parametric model with a
likelihood function $p(\data|\theta_k,\calM_k)$, is one such that the
posterior $\pi(\theta_k|\data,\calM_K)$ belongs to the same family of
distributions as the prior. In \cite[][sec.~5.2]{Bernardo:1994vd} it was
argued that a conjugate prior reduces the input of prior information to only
the choice of parameter values and thus cannot be fully justified from a
subjective perspective. Though their mathematical simplicity makes them
attractive, for many applications of interest it is difficult to find such
priors.

\subsubsection{Non-informative priors}
\label{ssub:Non-informative priors}

In situations where no or little prior information are available, so-called
``non-informative'' priors are often used. Many of them are derived from the
data or the likelihood function of the models.

\paragraph{Flat tails}

The simplest form is a uniform distribution or some distribution with flat
tails such as a Cauchy distribution. This choice can provide a robust prior
in the sense that outliers and misspecification of priors will not affect the
results significantly. For example, see \cite{OHagan:1990vx} and
\cite{Fan:1992vx} for analysis of the use of the Student~$t$ distribution as
the prior of location parameters such as the mean of a Normal distribution.

\paragraph{Jeffreys priors}

Jeffreys priors \cite{Jeffreys:1946jf} have the form,
\begin{equation}
  \pi(\theta_k|\calM_k)\propto\sqrt{\Abs{I(\theta_k)}}
\end{equation}
where
\begin{equation}
  I(\theta_k)
  = -\Exp_{\data}\Square[bigg]{\frac{\partial^2\log p(\data|\theta_k,\calM_k)}
    {\partial\theta_k\partial\theta_k^T}}
\end{equation}
where the expectation is taken with repsect to the likelihood function
$p(\data|\theta_k,\calM_k)$, is the \emph{Fisher information}. The defining
property of Jeffreys priors is its invariance in the sense that, for a
one-to-one transformation $\phi_k = h(\theta_k)$, we have the Jacobian
transformation,
\begin{equation*}
  I(\phi_k) = (J(h^{-1}(\phi_k)))^T I(h^{-1}(\phi_k)) 
  (J(h^{-1}(\phi_k)))
\end{equation*}
whre $J(h^{-1}(\phi_k))$ is the Jacobian matrix. For $\theta_k =
h^{-1}(\phi_k)$, it follows,
\begin{equation*}
  \pi(\phi_k)
  \propto \sqrt{\Abs{I(\theta_k)}\Abs{J(h^{-1}(\phi_k))}}
  \propto \pi(\theta_k)
  \Abs[bigg]{\det\Round[bigg]{\frac{\diff\theta_k}{\diff\phi_k}}}
\end{equation*}
The above expression states that the Jeffreys prior of the transformed
parameter $\phi_k$ is the same as the one obtained by changing variable of
the Jeffreys prior of $\theta_k$. In other words, a change of variable does
not change the prior under the Jeffreys rule. Another informal interpretation
is that, the prior shall contain no more information than the observed data
do. The Fisher information is widely accepted as an indicator of the amount
of information brought by the model about the parameter $\theta_k$ given the
data \cite{Fisher:1956vx}. Therefore, intuitively values of $\theta_k$ for
which $I(\theta_k)$ is large is more likely than those for which
$I(\theta_k)$ is small.

However, Jeffreys priors derived for many models may be problematic. It is
often the case that the derived Jeffreys priors can be improper. For example,
the Jeffreys prior for the mean parameter of a Normal likelihood is uniform on
the real line. This can create technical difficulties if the improper priors
also lead to improper posteriors since the marginal likelihood, as the
normalizing constant of the posteriors, cannot be computed. This makes the
Bayesian model choice problem difficult to formalize. See also the discussion
in \cite{Kass:1995vb}. However, improper priors can also lead proper
posteriors, for example see \cite[][sec.~2.9]{Gelman:2003vx},
\cite[][sec.~1.5]{Robert:2007tc}, \cite{Kass:1995vb} and references therein
for successful applications using improper priors. In general, for more
complex models using proper priors is recommended since it might be easier to
verify that the posteriors are also proper. In the situation where the derived
Jeffreys priors are improper, it is possible to use some proper distributions
to approximate them. For example, an inverse Gamma distribution can be used to
arbitrarily closely approximate $\pi(x)\propto1/x$, which is often seen as the
Jeffreys prior for scale parameters, such as the precision parameter of a
Normal distribution.

\paragraph{Reference priors}

Another class of non-informative priors is called \emph{reference priors},
introduced in \cite{Bernardo:1979uq}. Reference priors aim to derive priors
such that the distance between the posterior and prior is maximized, usually
measured in terms of the \kldfull \cite{Kullback:1951va}. In some sense, a
reference prior is the least informative prior. See \cite{Berger:1989vj,
Berger:1992kf, Berger:1992wo} and \cite[][sec.~5.4]{Bernardo:1994vd} for more
information on this class of priors.

Another form of the reference priors is to partition the parameter vector
$\theta_k = (\theta_k^{(1)},\theta_k^{(2)})$ where $\theta_k^{(1)}$ is the
parameter of interest and $\theta_k^{(2)}$ is the nuisance parameter. First
$\pi(\theta_k^{(2)}|\theta_k^{(1)},\calM_k)$ is defined as the Jeffreys prior
associated with $p(\data|\theta_k,\calM_k)$ when $\theta_k^{(1)}$ is fixed.
Then define the marginal,
\begin{equation}
  \tilde{p}(\data|\theta_k^{(1)},\calM_k) =
  \int p(\data|\theta_k,\calM_k)\pi(\theta_k^{(2)}|\theta_k^{(1)},\calM_k)
  \intd \theta_k^{(2)},
\end{equation}
and compute the Jeffreys prior $\pi(\theta_k^{(1)}|\calM_k)$ associated with
$\tilde{p}(\data|\theta_k^{(1)},\calM_k)$. By using the Jeffreys prior, given
fixed parameter of interest, the effects of the nuisance parameter is
eliminated.

\paragraph{Using non-informative priors for \pet compartmental models}

In \cite{Zhou2013} the results of using non-informative priors for the
Bayesian analysis of the \pet compartmental models were obtained. Here we
consider the simulated data (see Section~\ref{sec:Simulated and real pet
data}) and using Normally distributed errors (see Section~\ref{sec:Error
models}). An inver Gamma disribution approximation to the Jeffreys prior was
used for the precision parameter of the Normal distribution. Uniform
distributions are used for the $\theta_{1:r}$ and $\phi_{1:r}$ parameters
(see Section~\ref{sec:Application to positron emission tomography} for the
parameterization). The interval of the uniform distributions are the same as
considered feasible in the optimization procedures such as the \nls
estimator. The results of model selection is shown in Table~\ref{tab:pet
vague}. Compared to the results of using \aicc and \bic (Tables~\ref{tab:pet
aicc} and~\ref{tab:pet bic}), it is a significant improvement. For data with
low level of noise there is a high frequency of selecting the true model. For
more noisy data, it is expected the second and third compartments are
difficult to identify. Overall, the results are much more satisfactory than
those of \aicc and \bic.

\input{tab/pet-vague}

\subsubsection{Informative priors}
\label{ssub:Informative priors}

When the parameters bear real world meaning, it may be possible to construct
informative priors. For some applications, calibrating an informative prior
requires substantial expertise and some model specific informations.
Nonetheless, there are also some general methods.

When the parameter space is finite, it might be possible to obtain subjective
evaluation of the probabilities of the different values of the parameter.
When the space is uncountable, the problem is obviously more complicated. One
simple approach is to partition the parameter space and determine the
probabilities of the parameter falling into each of the partition
\cite[][sec.~3.2.2]{Robert:2007tc}.

A more systematic approach is the \emph{maximum entropy priors}
\cite{Jaynes:1989vx}. Assume that some characteristics of the parameter vector
$\theta_k$ in the model $\calM_k$ is known in the form of prior expectations,
\begin{equation}
  \Exp_{\pi}[h_i(\theta_k)|\calM_k] = \gamma_i, \qquad\text{for } i =
  1,\dots,K
  \label{eq:exp constraints}
\end{equation}
where the expectation is taken with respect to the prior distribution
$\pi(\theta_k|\calM_k)$ and $h_i$ is some function. For example, if the mean
and variance of the parameter is known, then we might have $h_1(x) = x$ and
$h_2(x) = x^2$ in the univariate case. In finite setting, define
\begin{equation}
  \calE(\pi) = -\sum_{\theta_k\in\Theta_k}
  \pi(\theta_k|\calM_k)\log \pi (\theta_k|\calM_k)
\end{equation}
where $\Theta_k$ is the parameter space. And the maximum entropy prior is the
$\pi$ that maximizes $\calE$ under the constraints of Equations~\eqref{eq:exp
  constraints}.

This can be extended to continuous case by introducing a reference
distribution, say $\pi_0(\theta_k)$. And define the entropy as the negative
\kld between $\pi_0(\theta_k)$ and $\pi(\theta_k|\calM_k)$,
\begin{equation}
  \calE(\pi) = -\int \pi_0(\theta_k)\log\Round[bigg]{
    \frac{\pi_0(\theta_k)}{\pi(\theta_k|\calM_k)}}\intd\theta_k
\end{equation}
The reference distribution $\pi_0$ can be chosen as one of the non-informative
priors discussed earlier. Such choice is justified in
\cite[][chap.~9]{Robert:2007tc}.

The maximum entropy procedure in essence selects a prior that preserves the
prior information -- the expectations of $\{h_i(\theta_k)\}_{i=1}^K$ while
making it as close to the non-informative prior as possible. Therefore, the
prior knowledge is presented in while the ignorance is also preserved.

\paragraph{Using informative priors for \pet compartmental models}

Here we give an example of constructing a biologically informed prior for the
\pet compartmental models. It is based on both the prior knowledge of the
underlying biochemical process and mathematical properties of the models.

In \cite{Anderson:1983wk} some useful results about compartmental models in
general were provided. Recall the parameterizations in
Section~\ref{sec:Application to positron emission tomography}. Let
$\gamma_{0j}$ denote the rate constant of the outflow from the $j$\xth
compartment into the environment. Without loss of generality, assume that the
parameters $\theta_{1:r}$ are ordered, $\theta_1 \le \dots \le \theta_r$.
Then,
\begin{enumerate}
  \item $0 \le \theta_i \le 2\max_j|A_{jj}|$ for all $i$.
  \item $\min_j\gamma_{0j} \le \theta_1 \le \max_j\gamma_{0j}$.
  \item when there is only one outflow into the environment, say the rate
    constant of this outflow is $k_2$, as in the plasma input model, then $0
    \le \theta_1 \le k_2$.
\end{enumerate}
In addition, $\sum_{i=1}^r \phi_i = K_1$, where $K_1$ is the rate constant of
input from the plasma into the tissues \cite{Gunn:2001cx}. Therefore $\phi_i
< K_1$ for $i = 1, \dots, r$. Given this information, more informative prior
distributions can be constructed. For simplicity, we restrict discussion to
imposing upper and lower bounds on the possible values of the parameters. As
we subsequently find that inference is not overly sensitive to the prior
specification we do not pursue more complicated approaches.

To demonstrate the idea, an informative prior distribution for parameters
$\theta_{1:3}$ and $\phi_{1:3}$ in the three-compartments model is
constructed. First note that the transition matrix $A$ is,
\begin{equation*}
  A = \begin{bmatrix}
    - k_2 - k_3 - k_5 & k_4  & k_6 \\
    k_3               & -k_4 & 0   \\
    k_5               & 0    & -k_6
  \end{bmatrix}.
\end{equation*}
It is believed that all the rate constants take values in the range $[5
\times 10^{-4}, 10^{-2}]$ (for example see \cite{Zhou2013,Peng:2008fx}).
Without loss of generality, we impose the identifiability constraint
$\theta_1 \leq \theta_2 \leq \theta_3$, then,
\begin{gather}
  0 < \theta_1 \le k_2 \le 10^{-2} \\
  \theta_1 \leq \theta_2 \leq \theta_3 \leq \max\{2(k_2 + k_3 + k_5), 2k_4,
  2k_6\} \le 6 \times 10^{-2}
\end{gather}

Under the imposed ordering, as $\theta_1$ is the smallest exponent, the term
$\phi_1e^{-\theta_1 t}$ decays more slowly than any other term in the
expansion. Consequently, $\phi_1/\theta_1$ is likely to make a relatively
large contribution to $V_D = \sum_{i=1}^r \phi_i/\theta_i$. It is not well
known how large the ratio $(\phi_1/\theta_1)/V_D$ will be. However, it is
easy to conduct a numerical study here, given the small number of parameters.
It is found that among all possible combination of rate constants, $\phi_1/
\theta_1 \ge 0.5 V_D$. If the combinations of the rate constants are
restricted to those without excessively large differences among them, i.e.,
cases in which, say, $k_5 \gg k_6$ are not considered, then $\phi_1/\theta_1
\ge 0.7 V_D$. The reason for not considering these cases is that such
irreversible (trapped) models yield infinite $V_D$ estimates and it is
generally known in advance that the tracer employed will exhibit reversible
dynamics.

With these informations we can then construct informative priors for the
parameters of interest. More technical details can be found in
\cite{Zhou2013}. In particular, truncated Normal distributions are used as
the priors for the parameters given all the upper and lower bounds.
Table~\ref{tab:pet informative} shows the model selection results when using
these informative priors. It can be seen that, especially for data with
higher noise level, the results are considerably improved compared to using
vague priors (Table~\ref{tab:pet vague}).

\input{tab/pet-informative}

\subsubsection{Sensitivity analysis}
\label{ssub:Sensitivity analysis}

Though intuitively the influence of priors can be eliminated given enough
data. In the particular problem of Bayesian model comparison, it should be
noted that the Bayes factor can be more sensitive to the choice of prior than
the posterior means of parameters, in the sense that more data are needed to
eliminate the influence of priors. \cite{Kass:1993vy,Kass:1995vb}.

It is therefore of interest to evaluate the Bayes factor over a range of
possible priors to assess the sensitivity issues. This is often
computationally expensive since many high dimensional integrations are
required. When there is enough information to construct parametric priors, it
is possible to alter the values of parameters and recompute the Bayes factor
\cite{McCulloch:1991hj}. In general situations, a less computational
expensive method is to use the Laplace approximation to compare the Bayes
factor using different priors \cite{Kass:1992tz}. It is also proposed in the
literature to use the maximum of the Bayes factor (and thus the maximal
evidence against a model) to evaluate the sensitivity problem
\cite{Berger:1987iq}.

\section{Discussion}
\label{sec:Model Selection Discussion}

We have reviewed a few model selection methods in this chapter. The
information-theoretic approach is well understood and has been practiced for
a long history. The Bayesian approach has gain substantial interests in the
last few decades. The computation of the integrations required by Bayesian
model comparison will be reviewed in the next chapter.

The Bayesian approach can be appealing in at least two situations. First,
when there are substantial prior knowledge about the underlying data
generating mechanisms. The Bayesian framework provides a way to incorporate
these information to lead to rational decisions. Even when no or little prior
informations are available, Bayesian approach can be useful when the
underlying assumptions about the models or data that lead to the asymptotic
results regarding the various information criteria are not met, as Bayesian
model comparison imposes very few assumptions on the form of models. For
example, through the running example of this thesis, the \pet compartmental
model, we have found that Bayesian model selection even with vague priors can
provide significant improvement over methods such as \aicc or \bic which is
only an approximation to the Bayes factor. The results can be further
improved via using informative priors.

Despite all the advantages, using Bayesian model comparison also has
considerably higher computational cost since many high dimensional
integrations are required. As we will see later, many widely used techniques
may fail to evaluate these integrations accurately. The work of this thesis
aims to provide a new framework within which the computational difficulty is
further lowered than current practice.

This review is far from comprehensive. We have restricted ourselves to methods
that have minimal assumptions about the form of the models. For particular
models, many other methods have been developed. For example, in regression
analysis, the Mallow's $C_p$ is a popular model selection criterion. For more
information on information-theoretic approaches we refer to
\cite{Burnham:2002wc,Claeskens:2008tq}.

One generic approach to model selection that is not reviewed in this chapter
but worth mention is the minimum description length (\mdl) method. It is
somehow different from the methods reviewed so far. It selects the model with
the least complexity which is measured as the length of using a programming
language to describe the data given the model and the model itself. The
description length of data given the model can be measured with $-\log
p(\data|\calM_k)$ while the description length of the model can be more
problematic since there is no universally agreed good form of this length. We
refer to \cite{Grunwald:2005vx} for more information on this approach and its
modern refinements.

The review of Bayesian model comparison in this chapter gives context for
later chapters. In Chapter~\ref{cha:Monte Carlo Methods}, some algorithms for
evaluating the Bayes factor and posterior model probabilities are reviewed
and in Chapter~\ref{cha:Sequential Monte Carlo for Bayesian Computation},
novel algorithms are developed. Limited by the scope, there are many
important topics within the Bayesian framework were not discussed. We refer
to \cite{Bernardo:1994vd,Robert:2007tc} for more systematic treatments. In
particular, \cite{Bernardo:1994vd} also has comprehensive bibliographies on
many of the topics related to Bayesian statistics.
