\chapter{Monte Carlo Methods}
\label{cha:Monte Carlo Methods}

As shown in the last chapter, Bayesian model comparison usually involves
computations of some integrations with respect to complex posterior
distributions. Only in very special situations, these can be resolved
analytically. In most cases, these integrations are approximated using
simulation techniques. In this chapter, we review some of the widely used
Monte Carlo methods with an emphasis on their applications to Bayesian
computation.

In Section~\ref{sec:Classical Monte Carlo} we introduce the basic idea of
Monte Carlo integration. Section~\ref{sec:Importance sampling} discusses the
importance sampling technique. Section~\ref{sec:Markov chain Monte Carlo}
reviews a class of important Monte Carlo algorithms, Markov chain Monte Carlo.
This chapter is concluded by Section~\ref{sec:Monte Carlo Discussion}, a
discussion on the reviewed algorithms and some other development in this area
that is not reviewed in detail.

\section{Classical Monte Carlo}
\label{sec:Classical Monte Carlo}

Classical Monte Carlo integration approximates the expectation of a function
$\varphi$ with respect to a distribution $\pi$,
\begin{equation}
  \Exp_{\pi}[\varphi(X)] = \int\varphi(x)\pi(x)\intd x
\end{equation}
provided that the above expectation exists, by drawing \iid samples from
$\pi$, say $\{X^{(i)}\}_{i=1}^N$, and approximating the expectation by the
empirical average,
\begin{equation}
  \hat\varphi_{\mc}^N = \frac{1}{N}\sum_{i=1}^N\varphi(x^{(i)}).
  \label{eq:vanilla mc}
\end{equation}
This method is sometime also called \emph{vanilla} Monte Carlo or \emph{na\"\i
  ve} Monte Carlo. The estimator $\hat\varphi_{\mc}^N$ converges almost surely
to $\Exp_{\pi}[\varphi(X)]$ when $N\to\infty$ by the Strong Law of Large
Numbers (\slln).

Clearly this method can only be applied when drawing samples directly from the
target distribution $\pi$ is possible. There are a few ways to draw random
variates from a reasonably well behaved distribution. See
\cite[][chap.~2]{Robert:2004tn} on this topic. In many cases, simulation from
a distribution $\pi$ efficiently requires the evaluations of its density
function point-wise, or finding some easy to simulate distribution that
closely imitate $\pi$. In the context of Bayesian computation, the target
distributions are usually complex posterior only known up to some normalizing
constants. And thus point-wise evaluation is not possible. In addition, the
high-dimensional natural of many models makes it near impossible to find a
distribution closely imitate the target. In addition, even when it is
possible, the accuracy of the estimator $\hat\varphi_{\mc}^N$ depends heavily
on the function $\varphi$.

For example, consider the approximation of the marginal likelihood (see
Section~\ref{sub:Bayes factor}),
\begin{equation*}
  p(\data|\calM_k) =
  \int f(\data|\theta_k,\calM_k)\pi(\theta_k|\calM_k) \intd \theta_k,
\end{equation*}
or equivalently,
\begin{equation}
  p(\data|\calM_k) = \Exp_{\pi}[f(\data|\theta_k,\calM_k)],
  \label{eq:prior expectation}
\end{equation}
where the expectation is taken with respect to the prior distribution
$\pi(\theta_k|\calM_k)$. It is possible to use samples from
$\pi(\theta_k|\calM_k)$, the prior distribution, to approximate the
integration. With samples generated from $\pi(\theta_k|\calM_k)$, say
$\{\theta_k^{(i)}\}_{i=1}^N$, we can approximate $p(\data|\calM_k)$ by,
\begin{equation}
  \widehat{p(\data|\calM_k)}_{\mc}^N
  = \frac{1}{N}\sum_{i=1}^N f(\data|\theta_k^{(i)},\calM_k).
\end{equation}
This estimator was studied in \cite{McCulloch:1991vx} and also mentioned in
\cite{Kass:1995vb}. However, this approach often results in large variances.
The likelihood function (and thus the posterior distribution) is often much
more concentrated than the prior distribution. And there may be a large
proportions of samples with small likelihood values and a few with high
values. For instance, consider the one-compartment \pet model (see
Section~\ref{sec:Application to positron emission tomography}) and
informative priors (see Section~\ref{sub:Choice of priors}). Using 100,000
samples from the prior distribution, the empirical mean and standard deviation
of the estimates from 100 simulations is $-40.9$ and $2.1$, respectively.
However when the dimension of the model is increased by using two-compartments
model, the estimates have a empirical mean and variance $-39.6$ and $12.6$,
respectively. The variance is too large for practical use of evaluating the
Bayes factor. Similar problems were also shown in \cite{McCulloch:1991vx}.

\section{Importance sampling}
\label{sec:Importance sampling}

The \emph{importance sampling} method is based on the observation of the
following identity,
\begin{equation}
  \Exp_{\pi}[\varphi(X)]
  = \int\varphi(x)\pi(x)\intd x
  = \int\varphi(x)\frac{\pi(x)}{\eta(x)}\eta(x)\intd x
  = \Exp_{\eta}\Square[bigg]{\varphi(X)\frac{\pi(X)}{\eta(X)}},
  \label{eq:impotance fundamental}
\end{equation}
where $\eta$ is a distribution with respect to which $\pi$ is absolutely
continuous. The above equation is termed \emph{importance fundamental
  identity} in \cite{Robert:2004tn}. The distribution $\eta$ is often called
the \emph{proposal} or \emph{instrumental} distribution. Thus given \iid
samples $\{X^{(i)}\}_{i=1}^N$ from distribution $\eta$, the expectation
$\Exp_{\pi}[\varphi(X)]$ can be approximated by the following importance
sampling estimator,
\begin{equation}
  \hat\varphi_{\is}^N
  = \frac{1}{N}\sum_{i=1}^N\varphi(X^{(i)})\frac{\pi(X^{(i)})}{\eta(X^{(i)})}.
  \label{eq:is normalized}
\end{equation}
The above estimator converges almost surely to $\Exp_{\pi}[\varphi(X)]$ when
$N\to\infty$. However its variance is not necessarily finite. In general, the
variance is finite if and only if, \cite[][sec.~3.3.2]{Robert:2004tn},
\begin{equation}
  \int(\varphi(x))^2\frac{(\pi(x))^2}{\eta(x)} < \infty.
\end{equation}
To access the above inequality, evaluating a more complex integration than the
original problem is required. In \cite{Geweke:1989tm}, two types of sufficient
conditions were mentioned. One is that $\pi/\eta$ is upper bounded and
$\var_{\pi}[\varphi(X)]$ is finite. Another is that the support is compact,
$\pi$ is upper bounded and $\eta$ is lower bounded by $\varepsilon > 0$. The
later is rather restrictive.

Both $\pi$ and $\eta$ are often only known up to some normalizing constants,
which can be approximated with the same samples. This leads to the estimator,
\begin{equation}
  \hat\varphi_{\wis}^N
  = \frac{\sum_{i=1}^Nw^{(i)}\varphi(X^{(i)})}{\sum_{i=1}^Nw^{(i)}}
  \label{eq:is unnormalized}
\end{equation}
where $w^{(i)} \propto \pi(X^{(i)})/\eta(X^{(i)})$, and are termed the
\emph{weights}. This estimator also converges almost surely to
$\Exp_{\pi}[\varphi(X)]$ when $N\to\infty$. This estimator has a bias since it
is the ratio of two unbiased estimator. However, even when the normalizing
constants of $\pi$ and $\eta$ are both known, this estimator can be preferable
to $\hat\varphi_{\is}$ due to its possible smaller mean squared error. In
fact, \cite{Casella:1998tj} showed an example of using the Cauchy distribution
as the proposal distribution for the evaluation of expectations under the
Student-$t$ distribution, where for some functions, such as $\varphi(x) =
|x|$, $\hat\varphi_{\wis}$ can outperform $\hat\varphi_{\is}$ considerably.

In general, the performance of importance sampling depends not only on the
choice of the proposal distribution $\eta$, but also the function of interest
$\varphi$. As suggested in \cite[][sec.~3.3.2]{Robert:2004tn}, to minimize the
variance of the estimator, the distribution $\eta$ should be chosen such that
$|\varphi(x)|\pi(x)/\eta(x)$ is almost constant with a finite variance. That
is, it is preferable for the distribution $\eta$ to be proportional to
$|\varphi|\pi$ and to have heavier tails. Though heavier tails do not
necessarily lead to the sufficient conditions such as those mentioned in
\cite{Geweke:1989tm}, thinner tails are more likely to result in infinite
variances as extreme large weights are more likely to occur in this situation.
Often the same samples are used to evaluate the expectations of different
functions. And the proposal distribution is chosen such that $\eta$ is close
to $\pi$ with heavier tails.

\draftnote{Rewrite the paragraph on using importance sampling for the
  evaluation of marginal likelihood. An explicit estimator is given. And it is
  made clear that for such an estimator to have good performance, we need
  knowledge of the posterior distribution, with which we usually can just
  construct efficient \mcmc algorithms and obtain not only marginal
  likelihood, but also posterior means, etc.}

In the context of Bayesian model comparison, we are interested in the
evaluation of the marginal likelihood $p(\data|\calM_k)$. We may use a
proposal distribution, say $\eta$ and samples drawn from it to approximate the
expectation~\eqref{eq:prior expectation}. This leads to the estimator,
\begin{equation}
  \widehat{p(\data|\calM_k)}_{\is}^N =
  \frac{\sum_{i=1}^N w^{(i)} f(\data|\theta_k^{(i)},\calM_k)}
  {\sum_{i=1}^N w^{(i)}}
\end{equation}
where $w^{(i)}\propto\pi(\theta_k^{(i)}|\calM_k)/\eta(\theta_k^{(i)})$ and
$\{\theta_k^{(i)}\}_{i=1}^N$ are distributed with $\eta$. Good performance can
be obtained when $\eta$ is close to
$f(\data|\theta_k,\calM_k)\pi(\theta_k|\calM_k)$. In other words, we need some
knowledge of the posterior distribution $\pi(\theta_k|\data,\calM_k)$, which
is often not available for complex models. Even when some characteristics of
the posterior distribution are available, other techniques might be preferred.
For example, with such informations, algorithms reviewed in the next section
will allow us to simulate dependent samples with the posterior distribution as
the limiting distribution. They can not only provide estimators of the
marginal likelihood, but also other quantities of interest, such as the
posterior mean of parameters.

One possible solution to such problems is to use some form of adaptive
schemes. For example, \cite{ManSuk:1992vx} proposed to use a family of
parametric distributions as proposal distributions and the parameters are
iteratively tuned. In their paper, a multivariate $t$ distribution is used as
an example, which is flexible for some applications. However, for complex
target distribution, especially that are high dimensional and multimodal, it
is difficult to find an explicit parametric distribution that can sample those
local modes efficiently. We will see in the next chapter, sequential Monte
Carlo algorithms can iteratively construct efficient importance sampling
proposals and are particularly suited for a wide range of applications,
including Bayesian model comparison.

\section{Markov chain Monte Carlo}
\label{sec:Markov chain Monte Carlo}

Both the vanilla Monte Carlo and importance sampling methods require
simulations directly from a distribution, which is often not feasible in
realistic applications for reasons mentioned in the last two sections.
Estimation techniques based on dependent samples were developed. The most
important type, Markov chain Monte Carlo (\mcmc), uses dependent samples
generated by a Markov chain with the target $\pi$ as a limiting distribution
for the approximation of the desired integration. A limiting distribution,
informally is one such that if $X_n$, the state of the Markov chain at step
$n$ is distributed with $\pi$, then $X_{n+1}$, the next step is also
distributed with $\pi$.

The basic idea is that, given a Markov chain $(X^{(1)},\dots,X^{(i)},\dots)$,
with a limiting distribution $\pi$, then with suitable conditions (outlined
for each algorithm later), for a $\pi$-integrable function $\varphi$,
\begin{equation}
  \lim_{N\to\infty}\frac{1}{N}\sum_{i=1}^N\varphi(X^{(i)}) =
  \Exp_{\pi}[\varphi(X)].
  \label{eq:mcmc convergence}
\end{equation}
And therefore samples generated by this Markov chain can be used for
estimation of various quantities in a similar fashion as with vanilla Monte
Carlo or importance sampling. This leads to the estimator,
\begin{equation}
  \hat\varphi_{\mcmc}^N = \frac{1}{N}\sum_{i=1}^N\varphi(X^{(i)})
  \label{eq:mcmc est}
\end{equation}
where $\{X^{(i)}\}_{i=1}^N$ are $N$ samples from the Markov chain.

The construction of such a $\pi$-invariant Markov kernel leads to the
development of various widely used \mcmc algorithms. In this section, some of
the more important ones are reviewed.

\subsection{Discrete time Markov chain}
\label{sub:Discrete time Markov chain}

This section briefly reviews some notions of discrete time Markov chains.
Because of its utility natural, it will be compact and only reviews results
required by later sections.

A Markov chain can be defined in terms of \emph{transition kernels}. Consider
a measurable space, say $(E,\calE)$, a transition kernel $K$ is a function defined
on $E\times\calE$ such that $K(x,\cdot)$ is a probability measure for every
$x\in E$ and $K(\cdot,A)$ is measurable for every $A\in\calE$. In the
continuous case, we also call the corresponding conditional density
\emph{kernel}, denoted by $K(x,x')$, $x,x'\in E$. That is, $\Pr(X\in A|x) =
\int_A K(x,x')\intd x'$.

A (discrete time) Markov chain, denoted by $(X_n)$ is a sequence of random
variables $X_0,X_1,\dots,X_n,\dots$ such that conditional on
$x_{n-1},\dots,x_0$, $X_n$ has the same distribution as it has conditional on
$x_{n-1}$. Clearly a transition kernel $K(x_{n-1},\diff x_n)$ is such a
conditional distribution. In the context of \mcmc, we are mostly concerned
with \emph{time homogeneous} Markov chains. A Markov chain $(X_n)$ is said to
be time homogeneous if for every $t_0\le t_1\le\dots\le t_k$, the distribution
of $(X_{t_1},\dots,X_{t_k})$ conditional on $x_{t_0}$ is the same as
$(X_{t_1-t_0},\dots,X_{t_k-t_0})$ conditional on $x_0$. In other words, given
the initial state $x_0$ or its distribution, the Markov chain is determined
solely by its transition kernel.

Another way to define a time homogeneous Markov chain, or its defining
property is the (\emph{weak}) \emph{Markov property}: For every initial
distribution of $X_0$, $\mu$, and every $(n+1)$ sample $(X_0,\dots,X_n)$,
\begin{equation}
  \Exp_{\mu}[\varphi(X_{n+1},X_{n+2},\dots)|x_0,\dots,x_n] =
  \Exp_{x_n}[\varphi(X_1,X_2,\dots)],
\end{equation}
where $\Exp_{\mu}$ denotes the expectation with respect to $P_{\mu} = \mu
K^n$, $K^n = K\vysmwhtcircle K^{n-1}$ and $\varphi$ is a $P_{\mu}$-integrable
function.

We will be mostly concerned with the sensitivity of the Markov chain with
respect to the initial value $X_0$ or its distribution and the existence and
the speed at which the Markov chain converges to its limiting distribution. A
few properties of a given Markov chain are discussed below. In short,
\emph{irreducibility} states that all states communicate. The Markov chain can
reach any state $y\in E$ starting from any other state $x\in E$. A stronger
version says that the chain can travel any distance in one step. Another
property is \emph{aperiodic}, which says that for a chain leaving a group of
states it does not need to take $k$ or a multiple of $k$ steps to return to it
with $k>1$. Informally, a sufficient but not necessary condition for an
irreducible chain to be aperiodic is that the chain can stay in a neighborhood
of a state (or at the state in the discrete case) for an arbitrary number of
instances without being forced to leave it. Or it does not need to go through
a cycle to reach back into the neighborhood of the current state. These two
properties guarantee a Markov chain to explore a space freely. A third
property we will discuss is \emph{recurrence}, which states that the Markov
chain can visit any state for infinite times. In other words, the Markov chain
can explore a space throughout starting from almost anywhere. A stronger
version, \emph{Harris recurrence}, allows the chain to start from
\emph{everywhere}. A property fundamental to \mcmc algorithms is the existence
of \emph{invariant} distribution, which states that the Markov chain can be
stable under suitable conditions and converge to a desired distribution. A
sufficient condition for the existence of invariant distribution,
\emph{detailed balance}, is perhaps the most useful tool in practice to check
the validity of a given algorithm. Last, we will discuss the \emph{ergodicity}
of Markov chains, which measures the speed at which a Markov chain converges
to its invariant (and thus its limiting) distribution.

In what follows, we give a more formal yet compact treatment of the properties
discussed above. Most results are stated without proofs. One can see, for
example \cite[][chap.~6]{Robert:2004tn} for a treatment of the topic in more
detail in the context of \mcmc algorithms.

\subsubsection{Irreducibility}
\label{ssub:Irreducibility}

A Markov chain $(X_n)$ with transition kernel $K$ on $(E,\calE)$ is said to be
$\psi$-\emph{irreducible}, for a given measure $\psi$, if for every
$A\in\calE$ such that $\psi(A)>0$, there exists $n$ such that $K^n(x,A)>0$ for
all $x\in E$. This chain is said to be \emph{strongly}
$\psi$-\emph{irreducible} if $n=1$ for every $A\in\calE$ such that
$\psi(A)>0$.

An equivalent way of saying irreducibility is that for $x\in E$, $A\in\calE$,
$P_x(\tau_A\le\infty) > 0$, where $\tau_A = \inf\{n\ge1;X_n\in A\}$ is the
first value of $n$ that the chain enters the set $A$, namely the \emph{first
  hitting time} at $A$ and $P_x$ denote the probability conditional on the
initial state $x$. In other words, the probability of reach any the set $A$ in
finite many steps is positive.

Two related concepts, \emph{atom} and \emph{small set}, are useful for
formally defining aperiodicity and ergodicity later. A Markov chain $(X_n)$ is
said to have an \emph{atom} $\alpha\in\calE$ if there exists an associated
nonzero measure $\eta$ such that, $K(x,A) = \eta(A)$ for all $x\in\alpha$ and
$A\in\calE$. A set $C$ is said to be \emph{small} if there exists
$m\in\Natural$ and a nonzero measure $\eta_m$ such that
$K^m(x,A)\ge\eta_m(A)$, for all $x\in C$ and $A\in\calE$.

\subsubsection{Cycles and Aperiodicity}
\label{ssub:Cycles and Aperiodicity}

A $\psi$-irreducible chain $(X_n)$ has a \emph{cycle of length} $d$ if there
exists a small set $C$, an integer $M$, and a probability distribution
$\eta_M$ such that $d$ is the greatest common denominator of
\begin{equation*}
  \{m\ge1;\text{ There exists }\varepsilon_m > 0\text{ such that }C\text{ is
    small for }\eta_m\ge\varepsilon_m\eta_M\}.
\end{equation*}
A chain is \emph{aperiodic} if $d = 1$. If there exists a small set $A$ and an
associated measure $\eta_1$ such that $\eta_1(A) > 0$, that is it is possible
to go from $A$ to $A$ in a single step, the chain is said to be
\emph{strongly aperiodic}.

A sufficient condition for aperiodicity is that the kernel is positive in a
neighborhood of a state $x$. Then the chain can stay in this neighborhood for
an arbitrary time. If a chain is not aperiodic, then the return from one state
to its own neighborhood will requires a forced passage through another part of
the space, which is clearly a undesired property for an \mcmc algorithm. It
will be shown that for the algorithms discussed in this chapter, they are
aperiodic.

\subsubsection{Recurrence}
\label{ssub:Recurrence}

For a Markov chain $(X_n)$ on $(E,\calE)$, a set $A\in\calE$ is said to be
\emph{recurrent} if for all $x\in A$, $\Exp_x[N_A] = \infty$ where $N_A =
\sum_{n=1}^{\infty}\bbI_A(X_n)$ is the number of passages through $A$. The
Markov chain is said to be recurrent if there exists a measure $\psi$ such
that the chain is $\psi$-irreducible and for all $A\in\calE$ such that
$\psi(A)>0$, $A$ is recurrent.

A sufficient condition for a $\psi$-irreducible chain to be recurrent is that
there exists a small set $C$ with $\psi(C)>0$ such that $P_x(\tau_C<\infty) =
1$ for all $x\in C$ where $\tau_C$ is the first hitting time at $C$.

A stronger property is called the \emph{Harris recurrence}. A set $A\in\calE$
is said to be Harris recurrent if for all $x\in A$, $P_x(N_A = \infty) = 1$.
The $\psi$-irreducible Markov chain is said to be Harris recurrent if for all
$A\in\calE$ such that $\psi(A)>0$, $A$ is Harris recurrent.

Informally, Harris recurrence says that starting from \emph{everywhere} in the
space $E$, every part of the space will be visited for infinite instances with
probability one. This is important for \mcmc algorithms in the sense that
Harris recurrence guarantees unique limiting distribution (up to a
multiplicative factor).

\subsubsection{Invariant measure}
\label{ssub:Invariant measure}

A $\sigma$-finite measure $\pi$ is \emph{invariant} for a Markov chain with
transition kernel $K$ if,
\begin{equation}
  \pi(A) = \int K(x,A)\pi(\diff x)
\end{equation}
for all $A\in\calE$. When an invariant \emph{probability} measure exists for a
$\psi$-irreducibility chain, the chain is said to be \emph{positive}. A
positive chain is always recurrent. Also the invariant measure is unique for a
recurrent chain, up to a multiplicative factor. It is trivial to see that, for
a invariant probability measure $\pi$ of a Markov chain $(X_n)$, if
$X_0\sim\pi$, then $X_n\sim\pi$ for all $n\ge1$. Thus this distribution is
also often referred to as the \emph{stationary} measure.

A related concept is the \emph{reversibility}. A stationary Markov chain
$(X_n)$ is said to be \emph{reversible} if the distribution of $X_{n+1}$
conditional on $X_n = x$ is the same as the distribution of $X_n$ conditional
on $X_{n+1} = x$. Intuitively, this says that the direction of time is
irrelevant. The chain has the same stationary if it travels backward in time.
A sufficient condition for a Markov chain to have an invariant probability
distribution $\pi$ and be reversible is the existence of the \emph{detailed
  balance} condition,
\begin{equation}
  \pi(x)K(x,y) = \pi(y)K(y,x).
\end{equation}

\subsubsection{Ergodicity}
\label{ssub:Ergodicity}

As stated in the beginning of this section, \mcmc algorithms relies on the
limiting $\pi$ of a Markov chain $(X_n)$, which has the property that, if
$X_n\sim\pi$, then $X_{n+1}\sim\pi$. In other words, we are interested in the
convergence of the distribution $P_{\mu}^n = \mu K^n$ where $\mu$ is the
initial distribution of $X_0$. More importantly, we are interested in the
independence of initial condition $\mu$ of its limiting behavior when
$n\to\infty$. In the following we establish that the invariant distribution
$\pi$ is such a limiting distribution.

% TODO check the definition of K(\alpha,\alpha)
For a Harris recurrent and positive Markov chain $(X_n)$ with transition
kernel $K$ and invariant distribution $\pi$, an atom $\alpha$ is
\emph{ergodic} if
\begin{equation}
  \lim_{n\to\infty}\Abs{K^n(\alpha,\alpha) - \pi(\alpha)} = 0,
\end{equation}
where $K(\alpha,\alpha) = \int_{\alpha}K(x,\alpha)\pi(\diff\alpha)$ and $K^n =
K\vysmwhtcircle K^{n-1}$. For more general situations, the convergence is
established through the \emph{total variance norm}, defined for two measure
$\mu_1$ and $\mu_2$ on the space $(E,\calE)$,
\begin{equation}
  \Norm{\mu_1-\mu_2}_{TV} = \sup_{A\in\calE}\Abs{\mu_1(A) - \mu_2(A)}.
\end{equation}

Two important results are that, if a Markov chain $(X_n)$ is Harris recurrent,
positive and aperiodic, with transition kernel $K$ and invariant distribution
$\pi$, then
\begin{equation}
  \lim_{n\to\infty}\Norm[bigg]{
    \int K^n(x,\cdot)\mu(\diff x) - \pi}_{TV} = 0
\end{equation}
for every initial distribution $\mu$. And this total variance norm decreases
in $n$. From here, it becomes clearly why the recurrence and aperiodicity
discussed before are important for \mcmc algorithms. Note that the above
results also implies that, for bounded function $\varphi$,
\begin{equation}
  \lim_{n\to\infty}\Abs{\Exp_{\mu}[\varphi(X_n)] - \Exp_{\pi}[\varphi(X)]}
  = 0,
\end{equation}
where the first expectation is with respect to $P_{\mu}^n$, the initial
distribution of $X_0$ and the second is for a random variable distributed with
$\pi$. This result establishes the validity of using dependent samples from
\mcmc algorithms for the approximation of integration with respect to $\pi$.

However, the above results only states that the Markov chain will converge. It
does not imply how fast the chain converges to its limiting distribution. Two
stronger form of convergence is \emph{geometric} and \emph{uniform}
ergodicity.

A Markov chain $(X_n)$ with transition kernel $K$ on $(E,\calE)$ and invariant
distribution $\pi$, is said to be \emph{geometrically ergodic}, if there
exists $r > 1$ such that for all $x\in E$,
\begin{equation}
  \sum_{n=1}^{\infty} r^n\Norm{K^n(x,\cdot)-\pi}_{TV} <\infty.
\end{equation}
This implies that,
\begin{equation}
  \Norm{K^n(x,\cdot)-\pi}_{TV}\le r^{-n}M(x)
\end{equation}
where $M(x) = \sum_{n=1}^{\infty} r^n\Norm{K^n(x,\cdot)-\pi}_{TV}$. In other
words, the chain converges at least at a speed of a geometric sequence. We
emphasize that $M(x)$ is a function of the initial value $x$.

A stronger form of convergence, \emph{uniform} ergodicity requires that the
convergence speed is the same for all $x\in E$, or in other words $M(x)$ is
bounded. That is, the above convergence holds for a constant $M = \sup_x
M(x)$.

\subsection{Metropolis-Hastings algorithm}
\label{sub:Metropolis-Hastings algorithm}

The Metropolis-Hastings algorithm produces a Markov chain with limiting
distribution $\pi$ with a conditional density $q(\cdot|x)$ called the
\emph{proposal} or \emph{instrumental} distribution through the following
transition. At time $t$, given sample $X^t$, first $Y^t$ is drawn from
$q(y|x^t)$. Then, set
\begin{equation*}
  X^{t+1} =
  \begin{cases}
    Y^t, &\text{with probability } \alpha(x^t,y^t),\\
    X^t  &\text{with probability } 1 - \alpha(x^t,y^t).
  \end{cases}
\end{equation*}
where
\begin{equation}
  \alpha(x,y) =
  \min\Curly[bigg]{\frac{\pi(y)}{\pi(x)}\frac{q(x|y)}{q(y|x)},1}.
\end{equation}
The probability $\alpha(x,y)$ is called the \emph{Metropolis-Hastings
  acceptance probability}. This leads to Algorithm~\ref{alg:mh}.

\input{alg/mh}

The conditions under which the Markov chain produced by this algorithm has
$\pi$ as its limiting distribution are quite minimal
\cite[][sec.~7.3.2]{Robert:2004tn}. Intuitively, the generated Markov chain is
aperiodic since the algorithm allows events such as $\{X^{t+1} = X^t\}$. A
sufficient condition for irreducibility is that the conditional density
$q(\cdot|x)$ is positive. In other words, it allows that every set in $\calE$
with a positive Lebesgue measure can be reached in a single step. It can be
proved that with these two conditions, the convergence in
Equation~\eqref{eq:mcmc convergence} holds \cite[][Theorem~7.4 and
Corollary~7.5]{Robert:2004tn}.

The Metropolis-Hastings algorithm is important not only because it has found
many applications, but also because it is the foundation of many other
algorithms. For example the reversible jump \mcmc and the population \mcmc
algorithms, reviewed later in Section~\ref{sub:Reversible jump mcmc}
and~\ref{sub:Population mcmc}, respectively, can both be viewed as extensions
to this algorithm.

The design of the proposal distributions can greatly influence the performance
of the estimators. It has been a difficult problem and has attracted
substantial attention in the past. In the following, we discuss three commonly
used designs.

\subsubsection{Independent proposals}
\label{ssub:Independent proposals}

A proposal independent of the current state $X^t$ leads to the independent
Metropolis-Hastings algorithm. Let $\eta$ denote this proposal. The acceptance
probability becomes,
\begin{equation}
  \alpha(x,y) = \min\Curly[bigg]{\frac{\pi(y)\eta(x)}{\pi(x)\eta(y)},1}.
\end{equation}
The resulting Markov chain is uniformly ergodic if the target $\pi$ is bounded
by the proposal $\eta$ up to a multiplier. In other words, there exists a
constant $M$ such that $\pi(x)\le M\eta(x)$ for all $x$ in the support of
$\pi$.

Though uniform ergodicity is a much desired property for a given algorithm,
without proper optimizing, the performance of an independent proposal is often
far from ideal. The proposal $\eta$ should be chosen such that it maximizes the
\emph{average acceptance rate} $\bar\alpha = \Exp[\alpha(x,y)]$. Given a
stationary chain and thus the state $X$ is distributed with $\pi$, and a
proposal $Y$ which is distributed with $\eta$, it is defined as,
\begin{equation}
  \bar\alpha
  = \Exp\Square[bigg]{\min\Curly[bigg]{\frac{\pi(Y)\eta(X)}{\pi(X)\eta(Y)},1}}
  = 2\Pr\Round[bigg]{\frac{\pi(Y)}{\eta(Y)}\ge\frac{\pi(X)}{\eta(X)}},
\end{equation}
provided that $\pi/\eta$ is absolutely continuous and the expectation is taken
with respect to $f(x,y) = \pi(x)\eta(y)$. The quantity $\bar\alpha$ measures
how often a new proposal is accepted in the long run of the algorithm. This
optimization is generic in the sense that the function of interest $\varphi$
is not involved. In practice, $\eta$ should be chosen such that it is close to
$\pi$ as much as possible. The requirement for $\pi/\eta$ to be bounded also
suggest that $\eta$ at least should not have too thin tails compared to $\pi$.
Ideally it should have slightly heavier tails than $\pi$ but not much less
concentrated. In this aspect, the choice of $\eta$ is similar to the choice of
the proposal distribution for importance sampling. And hence it inherits the
same difficulties as outlined in Section~\ref{sec:Importance sampling}.

\subsubsection{Random walks}
\label{ssub:Random walks}

The random walk Metropolis-Hastings algorithm, originally introduced in
\cite{Metropolis:1953ex}, uses proposals that are symmetric, often in the form
$q(y|x) = q(\Abs{y - x})$. This leads to the acceptance probability,
\begin{equation}
  \alpha(x,y) = \min\Curly[bigg]{\frac{\pi(y)}{\pi(x)},1}.
\end{equation}
This algorithm does not satisfies conditions for the uniform ergodicity in
general. However it is geometrically ergodic under certain conditions. In
\cite{Mengersen:1996th}, a condition based on log-concavity of $\pi$ in the
tails was given. The Markov chain is geometrically ergodic if,
\begin{equation}
  \log\pi(x_1) - \log\pi(x_2) \ge \alpha\Abs{x_1 - x_2}
\end{equation}
for some $\alpha > 0$ and some $x_0$ such that $x_0 < x_1 < x_2$ or $x_2 < x_1
< -x_0$.

The random walk is one of the most widely used type of \mcmc algorithms. It
provides a generic working solution to many otherwise difficult problems.
However, without optimization, its performance is often far from satisfactory.
For example, multimodal distributions often have modes that are separated by
extremely small probability areas. These areas limit the move of the random
walk. If the chain proposes bigger steps, than it is possible that most
proposed values fall in small probability areas and the probability of jumping
from one mode to another is arbitrarily small. This often leads to extreme
small acceptance rates. On the other hand, if the chain proposes smaller
steps, it will take long time for the chain to explore the whole parameter
space. In either case, if the scaling (such as the variance of a Normal
distribution or some other measures of the dispersion of the proposal
distribution) of the random walk is chosen poorly, it can take arbitrarily
long time for the chain to move outside the neighborhood of one local mode of
the target distribution. In this situation, the sampler is said to be in a
\emph{trapping state}.

For instance, consider the one-compartment \pet model (see
Section~\ref{sec:Application to positron emision tomography}), a Normally
distributed error structure (see Section~\ref{sec:Error models}), and
non-informative priors (see Section~\ref{sub:Choice of priors}) for the
simulated data. We construct a random walk algorithm with three blocks. Recall
the parameterization in Section~\ref{sec: Applicaiton to positron emission
  tomography},
\begin{enumerate}
  \item Update $\phi_{1:r}$ with a multivariate Normal random walk proposal.
  \item Update $\theta_{1:r}$ with a multivariate Normal random walk proposal
  \item Update $\lambda$ with a Normal random walk proposal on the log scale,
    i.e., on $\log\lambda$.
\end{enumerate}
Both Figure~\ref{fig:pet mh tuned} and~\ref{fig:pet mh untuned} show the trace
of $(\phi_1, \theta_1)$ from three samplers, initialized with different
values. Each sampler is iterated for 10,000 times. In the former, the proposal
scales (the covariance matrices of the multivariate Normal distributions,
which are diagonal in this case) are well tuned and the later uses scales five
times of the former. In Figure~\ref{fig:pet mh tuned}, each sampler is able to
find the high probability region quickly and explore it efficiently. In
contrast, in Figure~\ref{fig:pet mh untuned}, none of the samplers is able to
find the high probability region and they are trapped around the initial
values.

\begin{figure}
  \includegraphics[width=\linewidth]{fig/PET_MH_Path.pdf}
  \caption[Trace of parameters in the random walk algorithm for the
  \protect\pet compartmental model (calibrated)]
  {Trace of $(\phi_1,\theta_1)$ from three random walk
    Metropolis-Hastings samplers for a \pet model with one components and
    non-informative prior, using well tuned proposal scales. First few values
    of each trace is not shown in the plots since they are far away from the
    high probability region with an order of magnitude difference in values.}
  \label{fig:pet mh tuned}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{fig/PET_MH_H_Path.pdf}
  \caption[Trace of parameters in the random walk algorithm for the
  \protect\pet compartmental model (uncalibrated)]
  {Trace of $(\phi_1,\theta_1)$ from three random walk
    Metropolis-Hastings samplers for a \pet model with one components and
    non-informative prior, using proposal scales five times of those tuned}
  \label{fig:pet mh untuned}
\end{figure}

\paragraph{Optimal proposal scales}

As seen in the above example, finding the optimal scales for a random walk
algorithm can be important for realistic applications. One way to measure the
optimality of the random walk is based on the asymptotic behavior of an
\emph{efficiency criterion} equal to the ratio of the variance of an estimator
based on an \iid sample and the variance of the estimator
$\hat\varphi_{\mcmc}^N$ in Equation~\eqref{eq:mcmc est}. In
\cite{Roberts:1997dg} it is recommended that the optimal proposal distribution
should produce chains with acceptance rate close to $0.5$ for models with
dimension $1$ or $2$ and $0.25$ for models with higher dimensions. One of the
more widely used type of proposals is the Normal distribution or its
multivariate variant. In \cite{Gelman:1995vx}, a form of optimal covariance is
given as $(2.38^2/d)\Sigma_{\pi}$, where $d$ is the dimension of the target
distribution $\pi$ and $\Sigma_{\pi}$ is the true covariance matrix of the
parameters under $\pi$. In \cite{Roberts:2001ta}, it was further established
that when the dimension goes to infinity, and the covariance matrix is assumed
to be diagonal, say $I_d\sigma_d^2$, the optimal scaling of $\sigma_d$ has a
corresponding acceptance rate $0.234$. This optimal rate has been commonly
used as a rule of thumb in practice.

\subsubsection{Adaptive proposal}
\label{ssub:Adaptive proposal}

It often requires substantial efforts to tune an algorithm's proposal scales
towards optimality. Alternatively, many adaptive strategies have been
developed. See \cite{Andrieu:2008kh} for a recent review. The basic theme is
that a family of proposal distributions, indexed by some parameter, say
$q(\cdot|x) = q(\cdot|x, \theta)$ with $\theta\in\Theta$, is considered. And
the value of the parameter is updated along with the state $X$. This leads to
Algorithm~\ref{alg:adaptive rw}.

\input{alg/adaptive_rw}

There are many methods for updating of the parameters. See
\cite{Andrieu:2008kh} for some common algorithms. One of the more widely used
is based on the Normal random walk or its multivariate variant. In
\cite{Haario:1999dh,Haario:2001gu}, it is proposed to use the past samples to
approximate the optimal covariance matrix $(2.38^2/d)\Sigma_{\pi}$
\cite{Gelman:1995vx}. The algorithm first initialize $\mu^0$, a $d$-vector and
$\Sigma^0$, a covariance matrix. At time $t$, they are updated with,
\begin{align}
  \mu^{t+1} &= \mu^t + \gamma^{t+1} (X^{t+1} - \mu^t) \\
  \Sigma^{t+1} &= \Sigma^t + \gamma^{t+1}((X^{t+1} - \mu^t)(X^{t+1} - \mu^t)^T
  - \Sigma^t)
\end{align}
where the $\{\gamma^t\}_{t>0}$ is a sequence of small numbers, which are
formally arbitrary but could influence the performance. As noted by
\cite{Andrieu:2008kh}, though it is possible to set the sequence to a constant
$\gamma$, it is more common to set it to a deterministic decreasing sequence
such that $\sum_{t\ge1}\gamma^t = \infty$ and
$\sum_{t\ge1}(\gamma^t)^2<\infty$ to allow the effect of adaptation becomes
smaller and smaller as the algorithm progresses. This algorithm has also been
studied by \cite{Andrieu:2006tw} and others.

One obvious problem with such adaptive scheme is that the resulting chains are
no longer Markovian. And the limiting distribution, if it exists, may no
longer be $\pi$ as shown by examples in \cite{Andrieu:2008kh}. It is a common
practice to adapt the algorithm up to some point $t$, and stop the adaptation
and use parameter $\theta^t$ for all iterations onwards, as seen in
Algorithm~\ref{alg:adaptive rw}. The first part of the generated chain is
called the \emph{burn-in period} and is usually discarded afterwards and
estimations are based on iterations after the burn-in period. Some rules
for when to stop adaptation was discussed in \cite{Andrieu:2008kh} and
references therein. There are more advanced techniques where the adaptive
scheme attains a vanishing adaptation property. Intuitively, after a long
enough period, the adaptive algorithm will only change the parameter $\theta$
slightly and eventually it becomes stable. These algorithms requires more
careful designs to assure convergence.

Adaptive scheme is often necessary for realistic applications. For example,
consider the \pet compartmental model. As seen earlier, the scaling of the
Normal random walk influences the performance greatly. However, in a single
\pet scan, there are a quarter of a million data sets, each results in a
different posterior surface. Manual tuning for each of them is a difficult
task. When the random walk algorithm is applied for the real data, we used the
adaptive Normal random walk for the parameters $(\phi_{1:r},\theta_{1:r})$
(see parameterization in Section~\ref{sec:Application to positron emision
  tomography}). Using 10,000 iterations as the burn-in period, we were able to
obtain satisfactory acceptance rates (in the range from $0.2$ to $0.4$) for
the majority of the vast amount of data sets.

\subsection{Gibbs sampling}
\label{sub:Gibbs sampling}

The Metropolis-Hastings algorithm is generic in the sense that it requires
minimal knowledge of the target distribution to construct a valid sampler
(though not necessarily an efficient one). There also exists a class of \mcmc
algorithms that are more model dependent and they can use the (conditional)
features of the target distribution to construct potentially more efficient
samplers. One more important of them is the \emph{Gibbs sampling}. As we will
see later that, it is a special case of the Metropolis-Hastings algorithm.

A Gibbs sampler, as first introduced by \cite{Geman:1993bp}, assumes that the
random variable $X$ can be written as $X = (X_1,\dots,X_p)$, where $X_i$'s are
either unidimensional or multidimensional. Moreover, suppose that we can
simulate from the corresponding conditional distributions $\pi_1,\dots,\pi_p$,
\begin{equation}
  X_i|x_1,\dots,x_{i-1},x_{i+1},\dots,x_p
  \sim \pi_i(x_i|x_1,\dots,x_{i-1},x_{i+1},\dots,x_p)
\end{equation}
for $i = 1,\dots,p$. The associated Gibbs sampler is given by the following
algorithm that transits $X^t$ to $X^{t+1}$ in $p$ steps. At each step $i$
(within one iteration), $X_i^{t+1}$ is generated from $\pi_i$,
\begin{equation}
  X_i^{t+1} \sim
  \pi_i(x_i|x_1^{t+1},\dots,x_{i-1}^{t+1},x_{i+1}^t,\dots,x_p^t).
\end{equation}
The distributions $\pi_1,\dots,\pi_p$ are called the \emph{full conditionals}.
This leads to Algorithm~\ref{alg:gibbs}. See \cite[][chap.~8
and~9]{Robert:2004tn} for a full theoretical treatment of the Gibbs sampling.

\input{alg/gibbs}

The Markov chain produced by a Gibbs sampler is irreducible if $\pi$ satisfies
the so-called \emph{positive condition}: All $\pi_i$ are positive implies that
$\pi$ is also positive \cite[][Theorem~10.8]{Robert:2004tn}. An easier to
verify condition for Harris recurrent is that the transition kernel associated
with Algorithm~\ref{alg:gibbs} is absolutely continuous with respect to $\pi$
\cite{Tierney:1994uk}. Some simpler conditions can be found in
\cite{Hobert:1997vx}. Stronger convergence results such as geometrically
ergodicity are more difficult to be established for the Gibbs sampling in
general.

Intuitively, the decomposition of the joint density gives a particular
coordinate system with each step only explore one of the coordinates. It may
take many cycles for the sampler to move around the surface of the joint
distribution. As shown in \cite[][note~9.7.1]{Robert:2004tn}, poor
parameterizations or decomposition can lead to slow convergence to the extend
of getting into a trapping state. This kind of situations most commonly occur
when two highly correlated parameters, say $X_{k_1}$ and $X_{k_2}$, are
updated in separate Gibbs moves. When $X_{k_1}$ is updated, because of its
dependence on $X_{k_2}$, its move is limited, and \emph{vice versa}.

For a particular parameterization of the target distribution, one can use
better decompositions to speed up convergence in a Gibbs sampler. There are
few general methodologies to solve this problem. The practical rule is to
create decompositions as independent as possible. For instance, in the special
case that $(X_1,\dots,X_p)$ are mutually independent, then a Gibbs sampler is
equivalent to sampling directly from the target distribution. Admittedly, such
decompositions, though exist, can hardly be implemented, otherwise one would
not need to consider an \mcmc algorithm in the first place.

Another approach is to reparameterize the target distribution with the same
principle of decomposition. For example, in
\cite[][sec.~10.4.1]{Robert:2004tn}, an example is given for a bivariate
normal distribution. To sample $(X_1,X_2)$ from $\calN_2(0, \Sigma)$, using a
Gibbs sampler operating on $(X_1 + X_2, X_1 - X_2)$ is much faster than that
on $(X_1,X_2)$. The reparameterization here is based on the eigen basis in
this example. Note that, this kind of techniques is not unique to the Gibbs
sampling. They can also be used for other \mcmc algorithms. See, e.g.,
\cite{Hills:1993vb,Gilks:1996vx} for more discussions on this topic.

\paragraph{Relation with the Metropolis-Hastings algorithm}

The Gibbs sampling can be viewed as a special case of the Metropolis-Hastings
algorithm. It is equivalent to a composition of Metropolis-Hastings samplers
in which at each step a single component is updated using its full conditional
as the proposal distribution. It is easy to verify that the acceptance
probability is uniformly equal to one \cite[][Theorem~10.13]{Robert:2004tn}.
Let $Y = (X_{1:i-1},Y_i,X_{i+1:p})$ where $Y_i$ is the value proposed with
$\pi_i$ at step $i$,
\begin{align*}
  \alpha(x,y) &= \frac{\pi(y)}{\pi(x)}
  \frac{\pi_i(x|x_{1:i-1},x_{i+1:p})}{\pi_i(y|x_{1:i-1},x_{i+1:p})} \\
  &= \frac{\pi_i(y_i|x_{1:i-1}, x_{i+1:p})\pi(x_{1:i-1},x_{i+1:p})}
  {\pi_i(x_i|x_{1:i-1}, x_{i+1:p})\pi(x_{1:i-1},x_{i+1:p})}
  \frac{\pi_i(x_i|x_{1:i-1}, x_{i+1:p})}{\pi_i(y_i|x_{1:i-1}, x_{i+1:p})} \\
  &= 1
\end{align*}

\paragraph{Random scan}

Algorithm~\ref{alg:gibbs} is also called the \emph{deterministic scan} Gibbs
sampling, in the sense that the components are updated in a deterministic
order $(X_1,\dots,X_p)$. Consequentially, this resulting chain is not
reversible. Another way of doing Gibbs sampling is to use a \emph{random scan}
\cite{Liu1995Gibbs}, where at each time $t$, a sequence of integers
$(k_1,\dots,k_p)$ is generated, usually uniformly across all permutations of
$(1,\dots,p)$, and the components are updated in the order of
$(X_{k_1},\dots,X_{k_p})$. This leads to Algorithm~\ref{alg:gibbs random}, the
random scan Gibbs sampling. The resulting Markov chain is reversible
\cite{Liu1995Gibbs}. This property can be useful when applying the Central
Limit Theorem \cite[][sec.~10.1.2]{Robert:2004tn}.

\input{alg/gibbs_random}

\paragraph{Completion}

A common difficulty of the Gibbs sampling is that some of the full
conditionals may not be easily sampled from. In some situations, the full
conditionals of the target are not explicit at all. For example, missing data
models are often in the form,
\begin{equation*}
  f(\data|\theta) = \int f(\data, \mathbfit{z}|\theta)\intd \mathbfit{z}.
\end{equation*}
where $\mathbfit{z}$ is the unobserved data. In these situations, it is
possible to use \emph{completion} to construct a Gibbs sampler. A
distribution, say $\eta$ with the following property is chosen,
\begin{equation}
  \int\eta(x,y)\intd y = \int\pi(x)\intd x.
\end{equation}
Then the Gibbs sampler is constructed with the full conditionals of $\eta$
instead of $\pi$. The subchain of the resulting Markov chain that
corresponding to the marginal $\pi$ is then $\pi$-invariant. When such a
technique is used, there are many possible choices of $\eta$ to complete
$\pi$. Some applications provide natural choices, such as the aforementioned
missing data models.

\subsection{Reversible jump \protect\mcmc}
\label{sub:Reversible jump mcmc}

The reversible jump \mcmc (\rjmcmc) algorithm, introduced by
\cite{Green:1995dg}, is a technique widely used for simulations where the
dimension of the parameter space is not fixed. In the context of Bayesian
model selection, it can used for inference of the full posterior
$\pi(\theta_k,\calM_k|\data)$, which is defined on the space $\Theta =
\bigcup_{k\in\calK}(\{\calM_k\}\times\Theta_k$). Situations where this can be
reduced to the estimation of the Bayes factor (Section~\ref{sub:Bayes
  factor}), techniques reviewed so far in this chapter can be used. However,
when $\calK$ is (infinite) countable, or for other reasons, direct inference
on the full posterior distribution is desired, \rjmcmc is the most widely used
technique.

The \rjmcmc algorithm adapts the Metropolis-Hastings algorithm to construct
such kernels. Instead of a single type of moves defined by a proposal density,
a countable set of moves are considered, say $m\in\calM$. Each type of moves
is capable of moving the current state of the Markov chain between, say
$\Theta_k$ and $\Theta_{k'}$ (where in the case of $k = k'$, the move is
similar to a those in a \mcmc algorithm on a fixed dimension space). At state
$\theta_k\in\Theta_k$, a move type $m$ together with a new state
$\theta_{k'}\in\Theta_{k'}$ are proposed according to the
$q_m(\theta_{k'}|\theta_k)r_m(\theta_k)$, where $r_m(\theta_k)$ is the
probability of choosing type $m$ move when at state $\theta_k$; and
$q_m(\theta_{k'}|\theta_k)$ is the proposal kernel for the new state when a
move of type $m$ is made. The move is accepted with probability,
\begin{equation}
  \alpha(\theta_k,\theta_{k'}) =
  \min\Curly[bigg]{1,
    \frac{\pi(M_{k'})\pi(\theta_{k'}|M_{k'})p(\data|\theta_{k'},M_{k'})}
    {\pi(\calM_k)\pi(\theta_k|\calM_k)p(\data|\theta_k,\calM_k)}
    \frac{q_m(\theta_k|\theta_{k'})r_m(\theta_{k'})}
    {q_m(\theta_{k'}|\theta_k)r_m(\theta_k)}
  }.
\end{equation}
In practice, the proposed new state $\theta_{k'}$ is often implemented by
drawing a vector of continuous random variables, say $u$, independent of
$\theta_k$ and a deterministic bijection of vector $(\theta_k,u)$ to
$\theta_{k'}$, say $\theta_{k'} = T(\theta_k,u)$. The inverse of the move from
$\theta_{k'}$ back to $\theta_k$ then uses the inverse of this transformation.
Through a simple change of variable, the conditional density
$q_m(\theta_{k'}|\theta_k)$ can be expressed in terms of the density of vector
$u$ and its density $q(u)$. The acceptance probability becomes
\begin{equation}
  \alpha(\theta_k,\theta_{k'}) =
  \min\Curly[bigg]{1,
    \frac{\pi(M_{k'})\pi(\theta_{k'}|M_{k'})p(\data|\theta_{k'},M_{k'})}
    {\pi(\calM_k)\pi(\theta_k|\calM_k)p(\data|\theta_k,\calM_k)}
    \frac{r_m(\theta_{k'})}{r_m(\theta_k)}
    \frac{1}{q(u)}\Abs[bigg]{\frac{\partial\theta_{k'}}{\partial(\theta_k,u)}}
  },
\end{equation}
where the last term is the determinant of the Jacobian of the transformation.
The design of efficient between-model moves is often difficult, and the mixing
of these moves largely determines the performance of the algorithm.

The main difficulties lie in the choice of cross-model proposals and the
bijection $T$. Though the mapping $T$ theoretically is quite flexible, its
creation and optimization can be quite difficult in practice. This is
specifically true when the parameter space is complicated. In some extreme
cases, creating a valid kernel is already difficult. For example, in
multimodal models, where \rjmcmc has gain substantial attention, information
available in posterior distributions of any given model does not characterize
the modes that exists only in models of higher dimension; and thus a
successful between model move between these dimensions becomes difficult
\cite{Jasra:2007id}. Inefficient proposals results in Markov chains that are
slow to explore the whole parameter space. However, the natural ideas of
neighborhood and others, which proved to be useful concepts in practice, for
within model simulations, may no longer be intuitive in the variable dimension
model settings. For instance, when a cross-model occurs, the previous state of
the parameters, which may be in a high probability region of the model in the
last iteration, when transformed might be in a low probability region of the
model of the current iteration. In addition, \rjmcmc does not characterize all
models well as some may be visited by the chain only rarely. This may not be a
problem when the model is indeed of low posterior probability and there are
little interest in such models. However, in some cases it will be difficult to
determine whether the low acceptance rates of between model moves results from
actual characteristics of the posterior or from a poorly adapted proposal
kernel.

Some discussion of the optimization of the cross-model moves can be found in
\cite{Green:2009tr}. Also the adaptive scheme for the Metropolis-Hastings
algorithms has been extended to for \rjmcmc, for example \cite{Hastie:2005vi}.
However little other work are known for the actual performance of this kind of
improvement to \rjmcmc. In \cite{Green:2001tk} a method called \emph{delayed
  rejection} was discussed. In this method, a rejection of a proposal does not
immediately lead to the acceptance of current state, instead a second proposal
is attempted. Their numerical results showed efficiency improvement but with
increased computation cost.

\subsection{Population \protect\mcmc}
\label{sub:Population mcmc}

Population-based methods have been considered in recent researches. An entire
family of such algorithms, sequential Monte Carlo, is considered in
Chapter~\ref{cha:Sequential Monte Carlo for Bayesian Computation}. Another
algorithm, the population \mcmc, which has seen applications in the area of
Bayesian model comparison, is reviewed in this section.

The population \mcmc operates by constructing a sequence of distributions
$\{\pi_t\}_{t=0}^T$ with at least one of them being the target distribution
$\pi$. Parallel \mcmc chains are simulated for each of these distributions. In
addition, the chains interact with each other with swapping or crossover
moves, which allows fast mixing chains to ``lend'' information to slow mixing
chains. The outputs are therefore samples that approximate the product
$\prod_{t=0}^T\pi_t$ with the target distribution being a marginal.

Different choices of the sequence of distributions is possible. One commonly
used in practice is called tempering. For a target $\pi$, a sequence
$\{\pi_t\}_{t=0}^T$ is constructed such that,
\begin{equation}
  \pi_t(x) \propto [\pi(x)]^{\alpha(t/T)}
\end{equation}
where the mapping $\alpha:[0,1]\to[0,1]$ is monotonically increasing with
$\alpha(1) = 1$. Other similar schemes can be constructed. For example, in the
context of Bayesian modeling where $\pi$ is the posterior distribution
$\pi(\theta_k|\data,\calM_k) \propto
\pi(\theta_k|\calM_k)f(\data|\theta_k,\calM_k)$, one can construct a sequence
\begin{equation}
  \pi_t(\theta_k) =
  \pi(\theta_k|\calM_k)[f(\data|\theta_k,\calM_k)]^{\alpha(t/T)}
  \label{eq:power post}
\end{equation}
where the monotonically increasing mapping $\alpha$ satisfies $\alpha(0) = 0$
and $\alpha(1) = 1$. Therefore the sequence of distributions moves smoothly
from the prior, which usually can be sampled from easily, into the posterior.

The algorithm targets the distribution $\prod_{t=0}^T\pi_t$. After
initializing $T$ Markov chains for each of the marginals $\{\pi_t\}_{t=0}^T$
with a common support $E$, at each iteration, two types of moves are
performed. One is \emph{local} moves, sometimes termed \emph{mutation}, that
advances each chain individually using an \mcmc algorithms such as the
Metropolis-Hastings algorithm or the Gibbs sampling. One can select one chain
at random in each iteration to mutate or advance all chains in parallel. The
other type is \emph{global} moves. The purpose is to allow fast mixing chains
to transfer information into slowly mixing chains. This leads to
Algorithm~\ref{alg:pmcmc}. There are several approaches of the global move
\cite{Jasra:2007in}. Two more widely used are the following.

\input{alg/pmcmc}

\paragraph{Exchange}

The exchange move selects two chains at random, say $t_1$ and $t_2$, and
propose to exchange the states between them. The proposed exchange is accepted
or rejected according to the Metropolis-Hastings acceptance probability. That
is, given $X_1$ and $X_2$ between the current states of the two selected
chains, the exchange is accepted with probability
\begin{equation}
  \alpha(x_1, x_2) =
  \min\Curly[bigg]{\frac{\pi_1(x_2)\pi_2(x_1)}{\pi_1(x_1)\pi_2(x_2)}, 1}.
  \label{eq:exchange accept}
\end{equation}
For this to work, usually the two chains are chosen such that they are
adjacent to each other in the sense that one is chosen randomly and the other
is selected to be the one most close to it. For example, in the tempering
scheme, usually a chain with index $t\in{0,1,\dots,T-1}$ is chosen randomly
and it is proposed to be exchanged with the chain with index $t+1$. The
delayed rejection approach in \cite{Green:2001tk} (see
Section~\ref{sub:Reversible jump mcmc}) can also be used in the exchange
moves. Thus two chains in some sense that are very different can also be
chosen. In either case, the key is that the chains chosen to be exchanged are
chosen uniformly over all chains.

\paragraph{Crossover}

Another type of global move, called crossover was mentioned in
\cite{Liang:2001dc}. Instead of proposing to exchange the whole states $X_1$
and $X_2$, after the two chains are chosen, only parts of the two states are
proposed to be exchanged. Suppose the state $X$ can be partitioned into $X =
(X_1,\dots,X_p)$ in the same way for each chain. Then a random position, say
$l$ is chosen and the position $l$ of $X_1$ is proposed to be exchanged with
its counter-part in $X_2$. The acceptance probability is the same as
Equation~\eqref{eq:exchange accept} with suitable notation changes. In
\cite{Jasra:2007in}, it was found that this can be more efficient than the
exchange move.

The population \mcmc algorithm can be more efficient than simulating from a
single chain. Consider the situation where the \mcmc algorithm targeting
distributions $\pi_1$ and $\pi_2$ might be trapped. If $\pi_1$ and $\pi_2$ are
similar in the sense of the shape of the locations of local modes. And each of
them are trapped within different modes. The global move, say the exchange
move, proposes to exchange the values from one high probability region with
those in another high probability region. It is more likely that such an
exchange is accepted than the \mcmc algorithm jumps into the other modes
itself. Those chains that mix fast can explore the parameter space more
efficiently than those mix slower and are more likely to visit all the high
probability regions. Through the global moves they propose values in other
high probability regions to slowly mixing chains to help them avoiding
trapping states.

\paragraph{Optimal placement of distributions}

As discussed earlier, the population \mcmc allows efficient simulation of
previously difficult problem, though at a cost of increasing computational
cost. However, the algorithm requires another layer of optimization in
additional to the mixing speed of each local move -- the placement of the
sequence of distributions $\{\pi_t\}_{t=0}^T$. If too many chains are present,
the information can take many global moves to transfer from fast mixing chains
to slowly mixing ones. If there are too few chains and they are placed far
apart of each other, the global moves are likely to have small acceptance
rates. In \cite{Atchade:2010ha}, based on the idea of maximizing the average
information exchanged at each iteration, it was recommended that an optimal
placement of the distributions should have an global acceptance rate around
$0.234$. The optimal placement of the distributions can be obtained
iteratively if $\{\pi_t\}_{t=0}^T$ belongs to a family of distributions, say
$\pi_{\alpha} = \pi(\cdot|\alpha)$, indexed by $\alpha$. The algorithm first
finds $\alpha_0$ and $\alpha_1$ such that the population \mcmc algorithm
operating on $\{\pi_{\alpha_t}\}_{t=0}^1$ has a global acceptance rate close
to $0.234$. For example, if a tempering scheme is used, one can set $\alpha_0
= 0$ and use a binary search algorithm to find $\alpha_1$ since the smaller
$\alpha_1$, the higher the acceptance rate. The algorithm proceeds in the same
way to find $\alpha_t$ for $t>1$.

\subsection{Convergence diagnostic}
\label{sub:Convergence diagnostic}

One important issue of \mcmc algorithms is its speed of convergence. It is
well understood yet sometime over looked in practice. In the previous sections
we demonstrated for many algorithms, under fairly general conditions, the
chains produced are ergodic, or even geometrically ergodic (random walk). In
some cases the chain can be uniformly ergodic (independent Metropolis-Hastings
algorithm). However, such development provides little insight on how many
iterations the algorithm should be run to produce accurate estimates.

Convergence of an \mcmc algorithm is assessed with monitoring certain
statistics of samples. This process is also called \emph{convergence
  diagnostic}. There are two types of convergence
\cite[][chap.~12]{Robert:2004tn} widely used in practice. As we will see
later, a convergence diagnostic can at best determine that a chain has not
converged yet. One cannot be certain that a chain does converge.

\subsubsection{Convergence to the stationary distribution}
\label{ssub:Convergence to the stationary distribution}

It might seem that a minimal requirement for samples from an \mcmc algorithm
to be used to approximate a target distribution $\pi$, is that the chain
converges to this stationary distribution. However, $\pi$ is only the limiting
distribution and the stationarity is at best achieved asymptotically.
Nonetheless, one possible assessment of such convergence is to obtain bounds
on the total variation norm,
\begin{equation*}
  \Norm{K^n(x,\cdot)-\pi}_{TV}
\end{equation*}
where $K^n(x,\cdot)$ is the distribution of samples at the $n$\xth iteration.
However, obtaining analytical bounds can be prohibitively difficult.

\paragraph{Graphical approach}

A natural empirical approach is to draw plots of simulated samples to detect
non-stationary behaviors. For instance, \cite{Gelfand:1990it} drew sequence of
the samples ${X^t}_{t\ge1}$ against the time $t$, which is a functionality now
commonly seen in softwares for implementing \mcmc algorithms.

It should be emphasized that, even when the plots appears to show stationary
behavior, it is still possible that the algorithm has not converged or
explored the support of the target distribution surface efficiently. For
example, consider the three-compartments \pet model and non-informative priors
without ordering (that is, parameters such as $(\phi_1,\theta_1)$ and
$(\phi_2,\theta_2)$ are exchangeable, see Section~\ref{sec:Application to
  positron emission tomography}). We use one of the real data set and
constructed a random walk algorithm (see Section~\ref{sub:Metropolis-Hastings
  llgorithm}). Figure~\ref{fig:pet diag} shows the trace and histogram plots
of parameter $\theta_1$. It appears that the \mcmc chain has converged well.
However, from the properties of the model, it is known that this parameter has
at least three local modes. In fact, this sampler has been trapped into one of
them. In Figure~\ref{fig:pet diag c} the trace and histogram plots of the same
sampler but with better calibrated proposal scales are shown.

\begin{figure}
  \includegraphics[width=\linewidth]{fig/PET_MH_Diag}
  \caption[Trace and histogram of parameters in the random walk algorithm for
  the \protect\pet compartmental model (calibrated)]
  {Trace and histogram plots of parameter $\theta_1$ from a \mcmc
    sampler for \pet model with three components and non-informative priors
    without ordering. The trace plot has 1,000 samples and the histogram plot
    has 10,000 samples. The sampler is not well calibrated.}
  \label{fig:pet diag}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{fig/PET_MH_Diag_C}
  \caption[Trace and histogram of parameters in the random walk algorithm for
  the \protect\pet compartmental model (uncalibrated)]
  {Trace and histogram plots of parameter $\theta_1$ from a \mcmc
    sampler for \pet model with three components and non-informative priors
    without ordering. The trace plot has 1,000 samples and the histogram plot
    has 10,000 samples. The sampler is well calibrated.}
  \label{fig:pet diag c}
\end{figure}

\paragraph{Non-parametric test}

Standard non-parametric tests can be applied in stationarity assessment. This
is based on the idea that, if the chain is stationary, than $X^{t_1}$ and
$X^{t_2}$ have the same distribution for any two arbitrary time points $t_1$
and $t_2$. Therefore standard tests can be used to compare the distribution of
samples $(X^t,\dots,X^{t+p-1})$ and $(X^{t+p},\dots,X^{t+2p})$. It should be
noted that the correlations between samples should be taken into
considerations. One solution is to use subsamples. A \emph{batch size} $G$ is
introduced. Quasi-independent samples
$(X^{t_1+G},X^{t_1+2G},\dots,X^{t_1+pG})$ and
$(X^{t_2+G},X^{t_2+2G},\dots,X^{t_2+pG})$ are used to conduct the tests. See
\cite[][sec.~12.2.2]{Robert:2004tn} for some examples of such tests.

A simpler statistic to use, as seen in \cite{Gelman:2011vx}, is the ratio of
the variance of last few samples to that of all samples. Formally, for some
function $\varphi$, define the following ratio,
\begin{equation}
  R_T^N = \frac{\var[\varphi(X^{N-T+1},\dots,X^{N}]}
  {\var[\varphi(X^{1},\dots,X^{N}]}
\end{equation}
A value of $R_T^N$ between $0.9$ and $1.1$ was recommended. For example,
Figure~\ref{fig:pet diag ratio} shows the ratios of the variance of $V_D$
estimated using the final 10,000 samples to that of all 100,000 post burn-in
(the iterations used to adapt the sampler to optimal acceptance rates)
samples, for real \pet scan data sets. For the majority of data sets, the
ratios fall in the desired interval.

\begin{figure}
  \includegraphics[width=\linewidth]{fig/PET_Converge}
  \caption[Convergence diagnostics for the random walk algorithm for the
  \protect\pet compartmental model using summary statistics]
  {Convergence diagnostics for the three-compartments \pet using ratio of
    variance of $V_D$ estimates using final 10,000 samples to that of all
    100,000 post burn-in samples.}
  \label{fig:pet diag ratio}
\end{figure}

\subsubsection{Convergence of averages}
\label{ssub:Convergence of averages}

In \cite{Yu:1998fn} it was proposed to use the cumulative sums and plot the
partial differences,
\begin{equation}
  D_N^t = \sum_{i=1}^t (\varphi(X^{(i)}) - S^N), \qquad t = 1,\dots,N,
\end{equation}
where
\begin{equation}
  S^N = \frac{1}{N}\sum_{i=1}^N \varphi(X^{(i)})
\end{equation}
is the final average. A simple variant of this method is to plot the average
of the first $t$ samples, $S^t$. The use of these quantities can be appealing
because they directly measure the stability of the estimator of interest. For
instance, Figure~\ref{fig:pet vd mean} shows the posterior mean estimate of
$V_D$ for a three-compartments \pet model. The posterior mean from five
samplers initialized with different values converge to the same value.

\begin{figure}
  \includegraphics[width=\linewidth]{fig/PET_VD}
  \caption[Convergence diagnostics for the random walk algorithm for the
  \protect\pet compartmental model using averages]
  {Estimates of $V_D$ when starting the \mcmc chain from different
    values for a typical data set of a \pet model with three component.}
  \label{fig:pet vd mean}
\end{figure}

A more robust approach was proposed in \cite{Robert:1995ge}. The idea is to
use several convergent estimators based on the same samples. The chain is
stopped until all estimators are close to each other in the sense that the
differences are smaller than a preset tolerance value. One obvious estimator
is the empirical average. Another one is the importance sampling estimator,
\begin{equation}
  \varphi_{\mcmc-\is}^N = \sum_{i=1}^N
  \varphi(Y^{(i)})\frac{\pi(Y^{(i)})}{\eta(Y^{(i)})}
\end{equation}
where $\{Y^{(i)}\}_{i=1}^N$ are samples from $\eta$, the proposal distribution.
The variant of this estimator can be used when the normalizing constants of
either $\pi$ or $\eta$ are unknown (see Section~\ref{sec:Importance
  sampling}). In the particular case of the Gibbs sampling, the distribution,
\begin{equation}
  \eta(x) \propto \prod_{i=1}^p \pi_i(x_i|x_{1:i-1},x_{i+1:p})
\end{equation}
is a natural choice to be used as the proposal distribution and samples from
Gibbs sampler can be used. For the generic Metropolis-Hastings algorithm, the
samples simulated from the proposal distributions (both those accepted and
rejected) can be recycled to calculate the importance sampling estimate. This
lead to,
\begin{equation}
  \varphi_{\mh-\is}^N = \sum_{i=1}^N
  \varphi(Y^{(i)})\frac{\pi(Y^{(i)})}{q(Y^{(i)}|X^{(i)})}
  \label{eq:sn is}
\end{equation}
or its variant when the normalizing constants of either $\pi$ or $q$ are
unknown, where $\{Y^{(i)}\}_{i=1}^N$ are the proposed values (both accepted
and rejected) and $\{X^{(i)}\}_{i=1}^N$ are the accepted values. This approach
is more robust to trapping state than the simple plots of averages.

For example, consider an independent Metropolis-Hastings algorithm with
proposal $\eta$ have only one high probability region $A$ while the target
$\pi$ has two high probability region $A$ and $B$ such that
$\pi(A)\approx\pi(B)$. The chain is likely to be trapped in $A$ and $S^N$
might appear to be stable. However, $\varphi_{\mh-\is}^N$ is much less stable
since those values occasionally proposed within $B$, though very likely to be
accepted, they also have extreme large value of the weight
$\pi(Y^{(i)})/\eta(Y^{(i)})$. This leads to large variance of the importance
sampling estimator. In this case, $\varphi_{\mh-\is}^N$ is stable when values
of high target density values are also proposed frequently.

\subsection{Application to Bayesian model comparison}
\label{sub:MCMC Application to Bayesian model comparison}

It is clear that \rjmcmc can be used directly for the purpose of Bayesian
model selection, as it generates samples from the full posterior with the
model posterior distribution $\pi(\calM_k|\data)$ as a marginal.

For within model simulations, that is the algorithms generate Markov chains
targeting the posterior distribution $\pi(\theta_k|\data,\calM_k) \propto
f(\data|\theta_k,\calM_k)\pi(\theta_k|\calM_k)$, the dependent samples can be
used for the purpose of Bayesian model comparison for finite a set of models
through approximating the marginal likelihood and thus the Bayes factor, which
is the ratio of the marginal likelihood of two models. A few methods are
discussed here.

\subsubsection{Generalized harmonic mean estimator}
\label{ssub:Generalized harmonic mean estimator}

Recall that, the marginal likelihood is written as,
\begin{equation*}
  p(\data|\calM_k) = \int
  f(\data|\theta_k,\calM_k)\pi(\theta_k|\calM_k)\intd\theta_k.
\end{equation*}
For the purpose of simplicity, in this section we drop the dependency on the
model $\calM_k$ and simply write $p(\data) = \int
f(\data|\theta)\pi(\theta)\intd\theta$. With samples generated by an \mcmc
algorithm targeting the posterior distribution $\pi(\theta|\data) \propto
f(\data|\theta) \pi(\theta)$ available, say $\{\theta^{(i)}\}_{i=1}^N$, an
estimator of $p(\data)$ can be obtained by the harmonic mean
\cite{Newton:1994wm},
\begin{equation}
  \widehat{p(\data)}_{\hm}^N =
  \Round[bigg]{\frac{1}{N}\sum_{i=1}^N\frac{1}{f(\data|\theta^{(i)})}}^{-1}
\end{equation}
Unfortunately this estimator can suffer instability problem when samples with
small likelihoods are generated. In fact this estimator does not always have a
finite variance and therefore in general does not satisfy a Central Limit
Theorem. An improvement seen in \cite{Kass:1995vb} is,
\begin{equation}
  \widehat{p(\data)}_{\ghm}^N = \Round[bigg]{
    \frac{1}{N}\sum_{i=1}^N
    \frac{\gamma(\theta^{(i)})}{f(\data|\theta^{(i)})\pi(\theta^{(i)})}}^{-1},
\end{equation}
where $\gamma$ is a proper density function. This is based on the identity,
\begin{equation}
  \frac{1}{p(\data)}
  = \int \frac{\gamma(\theta)}{p(\data,\theta)}
  \frac{p(\data,\theta)}{p(\data)} \intd \theta
  = \int \frac{\gamma(\theta)}{p(\data,\theta)} \pi(\theta|\data) \intd \theta
\end{equation}
It can be seen that the density $\gamma$ plays a role similar to that of the
proposal distribution for the importance sampling. For similar reasons, high
efficiency is most likely to be obtained when $\gamma$ is roughly proportional
to $f(\data|\theta)$ \cite{Kass:1995vb}. And the above equation suggests that
the estimator has a finite variance if the tails of $\gamma$ is thin enough
compared to the posterior distribution $\pi(\theta|\data)$. In
\cite{Gelfand:1994ux} the use of a multivariate Normal distribution with
moments approximated from the samples as a natural choice of $\gamma$ was
suggested. In addition, the variance of the estimator can also be estimated
for $1/\widehat{p(\data)}_{\ghm}^N$ from the posterior samples through,
\begin{equation}
  \widehat{\var}\Square[bigg]{\frac{1}{\widehat{p(\data)}_{\ghm}^N}} =
  \frac{1}{N^2}\sum_{i=1}^N \Round[bigg]{
    \frac{\gamma(\theta^{(i)})}{f(\data|\theta^{(i)})\pi(\theta^{(i)})}
    - \frac{1}{\widehat{p(\data)}_{\ghm}^N}}^2.
\end{equation}
The above estimator provides a way of monitoring the convergence of the
estimator. Though more stable than the harmonic mean estimator
$\widehat{p(\data)}_{\hm}^N$, this generalized estimator still requires
considerable care in the implementation, especially the choice of the density
$\gamma$, to ensure good performance and indeed a finite variance estimator.

The method described above can be used for most \mcmc algorithms, such as the
Metropolis-Hastings algorithm, the Gibbs sampling and the population \mcmc (by
only using the subchain that corresponding to the distribution of interest).

\paragraph{Results for \pet compartmental model}

We conclude the discussion on the generalized harmonic mean estimator with
results for the \pet compartmental model with real data. For a
$r$-compartments \pet model (see Section~\ref{sec:Applicaiton to positron
  emission tomography}), a Student $t$ distributed error structure (see
Section~\ref{sec:Error models}), and informative priors (see
Section~\ref{sub:Choice of priors}), we construct a random Metropolis-Hastings
algorithm with four blocks. Again, recall the parameterization in
Section~\ref{sec:Application to positron emission tomography},
\begin{enumerate}
  \item Update $\phi_{1:r}$ with a multivariate Normal random walk proposal.
  \item Update $\theta_{1:r}$ with a multivariate Normal random walk proposal.
  \item Update $\tau$ with a Normal random walk proposal on the logarithm
    scale, i.e., on $\log\tau$.
  \item Update $\nu$ with a Normal random walk proposal on the logarithm
    scale, i.e., on $\log\nu$.
\end{enumerate}
There is a 100,000 iterations burn-in period and 10,000 iterations are used
for estimation. The generalized harmonic mean estimator $\hat\varphi_{\ghm}^N$
is used to compute the Bayes factor. The model selection results, along with
that from \aic and \bic methods for the purpose of comparison, are shown in
Figure~\ref{fig:pet mo}. It can be seen that the Bayesian model selection
results shows more plausible structure than the information criteria.

\begin{figure}
  \includegraphics[width=\linewidth]{fig/PET_MO}
  \caption[Model selection results for the \protect\pet compartmental model]
  {Model selection results for \pet model using real data set. From
    top to bottom: Model order selected by \aicc (see Section~\ref{sub:A
      second order aic}); Model order selected by \bic (see
    Section~\ref{sub:Bayes factor}); Model order selected by using Bayesian
    model comparison with marginal likelihood approximated by generalized
    harmonic mean estimator; The posterior model probability
    $\pi(\calM_k|\data)$ (see Section~\ref{sub:Model choice problems}) with
    uniform prior model probability $\pi(\calM_k)$.}
  \label{fig:pet mo}
\end{figure}

The convergence results for this simulation was already shown in
Figure~\ref{fig:pet diag ratio}. It should be noted that, though as shown in
Figure~\ref{fig:pet vd mean}, accurate estimation of the parameter $V_D$ does
not really need this many iterations. However, accurate estimation of the
marginal likelihood requires considerably more samples.

\subsubsection{Estimator using the Gibbs sampling}
\label{ssub:Estimator using the Gibbs sampling}

In the particular case of the Gibbs sampling, \cite{Chib:1995em} provides an
alternative estimator based on that the identity,
\begin{equation}
  p(\data) = \frac{f(\data|\theta)\pi(\theta)}{\pi(\theta|\data)},
\end{equation}
holds for any value of $\theta$. Therefore an estimator can be obtained by
substituting $\theta$ with a specific value, say $\theta^*$, which is usually
chosen from the high probability region of the posterior distribution and
approximating the denominator using outputs from the Gibbs sampler.

Formally, assume it is possible to construct a Gibbs sampler for the
decomposition $\theta = (\theta_1,\dots,\theta_p)$. Write $\pi(\theta|\data)$
as,
\begin{equation}
  \pi(\theta|\data) = \pi(\theta_1|\data)
  \prod_{i=2}^p\pi(\theta_i|\data,\theta_{1:i-1})
\end{equation}
and given $\theta^*$, we have the estimator,
\begin{equation}
  \widehat{p(\data)}_{\gs}^N = \frac{f(\data|\theta^*)\pi(\theta^*)}
  {\pi(\theta_1^*|\data)
    \prod_{i=2}^p\pi(\theta_i^*|\data,\theta_{1:i-1}^*)}.
\end{equation}
The value of $\pi(\theta_1^*|\data)$, the marginal ordinate of
$\pi(\theta|\data)$ can be approximated with output from the Gibbs sampler.
For example,
\begin{equation}
  \hat\pi^N(\theta_1^*|\data)
  = \frac{1}{N}\sum_{i=1}^N \pi(\theta_1^*|\data,\theta_{2:p}^{(i)})
  \label{eq:est gibbs first}
\end{equation}
since
\begin{align*}
  \pi(\theta_1^*|\data)
  &= \int\pi(\theta_1^*, \theta_{2:p}|\data)\intd\theta_{2:p} \\
  &= \int\pi(\theta_1^*|\data,\theta_{2:p})
  \pi(\theta_{2:p}|\data)\intd\theta_{2:p} \\
  &= \Exp[\pi(\theta^*|\data,\theta_{2:p})|\data]
\end{align*}
where the expectation is taken with respect to the marginal distribution of
$\theta_{2:p}$ conditional on the data.

The term $\pi(\theta_i^*|\data,\theta_{1:i-1}^*)$ can be approximated based on
the identity,
\begin{align}
  \pi(\theta_i^*|\data,\theta_{1:i-1}^*)
  &= \int \pi(\theta_i^*,\theta_{i+1:p}|\data,\theta_{1:i-1}^*)
  \intd\theta_{i+1:p} \notag\\
  &= \int \pi(\theta_i^*|\data,\theta_{1:i-1}^*,\theta_{i+1:p})
  \pi(\theta_{i+1:p}|\data,\theta_{1:i-1}^*) \intd\theta_{i+1:p}
\end{align}
and using a Gibbs sampler operate on $\theta_{i:p}$ with
$\pi(\theta_{i:p}|\data,\theta_{1:i-1}^*)$ as the target distribution and an
estimator similar to that in Equation~\eqref{eq:est gibbs first}. This is
possible because of the full conditionals required to construct the Gibbs
sampler are available, otherwise the Gibbs sampler on $\theta$ cannot be
constructed.

In addition to the usual requirement of a Gibbs sampler, that all the full
conditionals can be sampled from, this method also requires that all these
densities are known including their normalizing constants, and thus can be
computed point-wise. The advantage is that this estimator does not suffer from
the instability problem as the harmonic mean estimator and its
generalizations. Only averages of full conditionals are involved in the
calculation, which are less sensitive to extreme small values.

A generalization to the generic Metropolis-Hastings algorithm was provided by
\cite{Chib:2001gq}, where the proposal distributions are required to be
known including their normalizing constants.

\subsubsection{Population \mcmc with path sampling}
\label{ssub:Population mcmc with path sampling}

For the population \mcmc, proposed in \cite{Calderhead:2009bd}, a Monte Carlo
approximation to the path sampling estimator \cite{Gelman:1998ei} can be used
for the purpose of approximating the marginal likelihood. Given a parameter
$\alpha$ which defines a family of distributions, $\{\pi_{\alpha} =
\gamma_{\alpha}/Z_{\alpha}\}_{\alpha\in[0,1]}$ which moves smoothly from
$\pi_0 = \gamma_0/Z_0$ to $\pi_1 = \gamma_1/Z_1$ as $\alpha$ increases from
zero to one, one can estimate the logarithm of the ration of their normalizing
constants via a simple integral relationship,
\begin{equation}
  \log\Round[bigg]{\frac{Z_1}{Z_0}} = \int_0^1\Exp_{\pi_{\alpha}}
  \Square[bigg]{\frac{\diff\log\gamma_{\alpha}(X)}{\diff\alpha}}
  \intd\alpha.
  \label{eq:path integral}
\end{equation}
where the inner expectation is taken with respect to $\pi_{\alpha}$.

There are various ways of constructing such a family of distributions, for
example, the sequence of distributions in Equation~\eqref{eq:power post},
$\alpha = \alpha(t/T)$. The population \mcmc provides samples that can be used
to approximate this path sampling estimator. Given samples
$\{X_0^{(i)},\dots,X_T^{(i)}\}_{i=1}^N$ from $N$ iterations of a population
\mcmc sampler, one can approximate the expectation under distribution
$\pi_{\alpha} = \pi_{\alpha(t/T)} = \pi_t$ by the empirical average of the
subchain $\{X_t^{(i)}\}_{i=1}^N$. And the integration~\eqref{eq:path integral}
can be approximated with a numerical integration scheme such as the
Trapezoidal rule. This leads to the following estimator,
\begin{equation}
  \widehat{p(\data)}_{\ps}^N = \sum_{t=1}^T
  \frac{1}{2}(\alpha_t - \alpha_{t-1})(U_t^N + U_{t-1}^N)
\end{equation}
where $U_t^N$ is the estimate of
$\diff\log\gamma_{\alpha}(X)/\diff\alpha$ evaluated at $\alpha = \alpha_t$
using samples $\{X_t^{(i)}\}_{i=1}^N$.

As shown in \cite{Calderhead:2009bd}, the use of path sampling can reduce the
variance of the estimator significantly compared to the harmonic mean
estimator and its generalizations. However, the estimator
$\widehat{p(\data)}_{\ps}^N$ is biased because of the use of numerical
integration. It is clear that the smaller the interval
$[\alpha_{t-1},\alpha_t]$ (closer the distribution $\pi_{t-1}$ and $\pi_t$),
the smaller the bias. However, this can come into conflict with the
convergence speed of the population \mcmc algorithm. As discussed earlier, in
this setting, the global moves can potentially mixes slowly. For example, for
the one-compartment \pet model and using the sequence of
distributions~\eqref{eq:power post}, the optimal placement that results in an
acceptance rate of globals close to $0.234$ has only six chains. The path
sampling estimate has a 60\% relative bias. With 30 chains and a sensible
placement, the bias can be reduced to be negligible. However, the global move
has an acceptance rate about $0.85$, which implies that the sampler is not
mixing well.

The use of path sampling for approximating the Bayes factor will be revisited
in Chapter~\ref{cha:Sequential Monte Carlo for Bayesian Computation} for
sequential Monte Carlo. More results for the \pet model and other examples can
also be found in the same chapter.

\section{Discussion}
\label{sec:Monte Carlo Discussion}

In this chapter, a few Monte Carlo algorithms have been reviewed. One of the
more important class of algorithms, Markov chain Monte Carlo has been widely
used for Bayesian modeling.

The Metropolis-Hastings algorithm provide a generic solution to a large array
of applications. There are established results for tuning the algorithm for
optimal performance. The Gibbs sampler can be more appealing when there are
decompositions of the parameter vector that lead to easy to sample full
conditionals. Both algorithms can benefit from reparameterization that leads
to more independent parameters. The difficulty of \rjmcmc is that the
cross-model move is often difficult to design. The population \mcmc algorithm
can provide robust solution for high dimensional multimodal problems where the
other algorithm may be inefficient due to the difficulty of exploring local
modes separated by small probability regions. Its performance depends on both
the design of the \mcmc algorithm that update each chain and the placement of
the sequence of distributions.

The \rjmcmc algorithm can be used for Bayesian model selection through the
simulation of the posterior model probabilities. It can be difficult to
implement though conceptually appealing. Other algorithms can be used to
approximate the marginal likelihood and thus the Bayes factor using various
estimators. The harmonic mean estimator and its generalizations can be
calculated for most \mcmc algorithms. However, they suffer the stability
issues. For the Gibbs sampling and the Metropolis-Hastings algorithm, there
exist more stable estimators. However, they require knowledge of certain
distributions that are not always available. The population \mcmc can also use
the path sampling estimator. There is a trade-off between convergence speed
and the accuracy of the estimator in term of its bias.

It should be noted that, the application of Monte Carlo methods is not limited
to integration. They have also found application in areas such as optimization
(see \cite[][chap.~5]{Robert:2004tn} and references therein). In addition,
Monte Carlo integration is also not limited to the Bayesian paradigm. For
instance, Monte Carlo integration can be used for hypothesis tests when
various asymptotic assumptions, such as normality, are not suitable. For
examples, see \cite[][sec.~3.2]{Robert:2004tn} and references therein.

There are many other Monte Carlo algorithms not reviewed in this chapter.
Notable examples are slice sampling and perfect sampling (see \cite[][chap.~8
and~13]{Robert:2004tn}). Another class of algorithms, sequential Monte Carlo
(\smc) operates by iteratively construct efficient proposal distributions for
the importance sampling. A recent develop, particle \mcmc
\cite{Andrieu:2010gc} combines the strength of \mcmc and \smc by using \smc
samplers as proposal in the Metropolis-Hastings algorithm or the Gibbs
sampling. This chapter is far from a complete review of the topic on Monte
Carlo methods. The algorithms reviewed are widely used for the purpose of of
Bayesian model comparison. They have become standard tools of statisticians
for Bayesian computation. In the next chapter, we will study the use of \smc
for this purpose in detail. Some novel algorithms will be introduced.
