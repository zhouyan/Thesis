\ifx\inthesis\undefined % In Thesis
\input{../header}
\title{Monte Carlo Methods}
\begin{document}
\maketitle
\else % In Thesis
\chapter{Monte Carlo Methods}
\label{cha:Monte Carlo Methods}
\fi % In Thesis

In its simplest form, the classical Monte Carlo method approximate the
expectation of a function $\varphi$ with respect to a distribution $\pi$,
\begin{equation}
  \Exp_{\pi}[\varphi(X)] = \int\varphi(x)\pi(x)\intd x
\end{equation}
provided that the above expectation exists, by drawing \iid samples from
$\pi$, say $\{\Xiu\}_{i=1}^N$, and approximating the expectation by the
empirical average,
\begin{equation}
  \hat\varphi^N = \frac{1}{N}\sum_{i=1}^N\varphi(\Xiu).
  \label{eq:vanilla mc}
\end{equation}
The estimator $\hat\varphi^N$ converges almost surely to
$\Exp_{\pi}[\varphi(X)]$ when $N\to\infty$. Clearly this method can only be
applied when drawing samples directly from the target distribution $\pi$ is
possible, which is often not the case in the context Bayesian model
comparison, where the target distribution are usually complex posterior
distribution only known up to a normalizing constant. In addition, the use of
samples from $\pi$ is in general not necessary optimal in the sense that the
variance of the estimator is not minimized \cite[][sec.~3.3.2]{Robert:2004tn}.
Many improved Monte Carlo estimation methods and algorithms for simulation
from complex distributions have been developed in the past few decades. In the
remaining of this chapter, some of the most important ones are reviewed. Their
use in the context of Bayesian model comparison is also analyzed.

\section{Importance sampling}
\label{sec:Importance sampling}

The \emph{importance sampling} method is based on the identity, termed
\emph{importance fundamental identity},
\begin{equation}
  \Exp_{\pi}[\varphi(X)]
  = \int\varphi(x)\pi(x)\intd x
  = \int\varphi(x)\frac{\pi(x)}{\eta(x)}\eta(x)\intd x
  = \Exp_{\eta}\Square[Big]{\varphi(X)\frac{\pi(X)}{\eta(X)}},
\end{equation}
where $\eta$ is a distribution with respect to which $\pi$ is absolutely
continuous. Thus given \iid samples $\{\Xiu\}_{i=1}^N$ from distribution
$\eta$, the expectation $\Exp_{\pi}[\varphi(X)]$ can be approximated by the
following importance sampling estimator,
\begin{equation}
  \hat\varphi_{\text{\textsc{is}}}^N
  = \frac{1}{N}\sum_{i=1}^N\varphi(\Xiu)\frac{\pi(\Xiu)}{\eta(\Xiu)}.
\end{equation}
The above estimator also converges almost surely to $\Exp_{\pi}[\varphi(X)]$
when $N\to\infty$. However its variance is not necessarily finite. In general
the variance is finite if and only if,
\cite[][sec.~3.3.2]{Robert:2004tn},
\begin{equation}
  \int(\varphi(x))^2\frac{(\pi(x))^2}{\eta(x)} < \infty.
\end{equation}
This suggests that the density $\eta$ shall have tails heavier than those of
$\pi$. To access the above inequality, evaluating of a more complex
integration than the original problem is required. Some rather restrictive
sufficient conditions for finite variance estimator was mentioned in
\cite{Geweke:1989tm}. In practice both $\pi$ and $\eta$ are often only known
up to some normalizing constants, which can be approximated with the same
samples. This leads to the estimator,
\begin{equation}
  \hat\varphi_{w\text{\textsc{is}}}^N
  = \frac{\sum_{i=1}^N\wiu\varphi(\Xiu)}{\sum_{i=1}^N\wiu}
\end{equation}
where $\wiu = \pi(\Xiu)/\eta(\Xiu)$, and is termed the \emph{weights}. This
estimator also converges almost surely to $\Exp_{\pi}[\varphi(X)]$ when
$N\to\infty$. Though this estimator has a small bias, the variance and mean
square errors are improved in some situations when compared to the vanilla
Monte Carlo methods as in equation~\eqref{eq:vanilla mc} see
\cite{Casella:1998tj} for some examples. However, either the importance
sampling or direct sampling is uniformly better than the other. The
performance depends not only on the choice of the proposal distribution
$\eta$, but also the function of interest $\varphi$, as shown in
\cite{Casella:1998tj}. As suggested in \cite[][sec.~3.3.2]{Robert:2004tn}, to
minimize the variance of the estimator, the distribution $\eta$ should be
chosen such that $|\varphi(x)|\pi(x)/\eta(x)$ is almost constant with a finite
variance. That is, it is preferable for the distribution $\eta$ to be
proportional to $|\varphi|\pi$ and have heavier tails. In practice, $\eta$ is
often chosen such that it is approximated the same shape as $\pi$ but with a
heavier tail.

The importance sampling technique is rarely used directly for estimating the
marginal likelihood which is used to compute the Bayes factor for the purpose
of Bayesian model comparison. Some techniques, such as the widely used
harmonic mean estimator and its generalizations, are somehow related. However,
they are mostly used with dependent samples from Markov chain Monte Carlo
algorithms and are therefore introduced later in the next section. The
importance sampling method however is fundamental to a more advanced family of
Monte Carlo methods, sequential Monte Carlo (\smc) which will be reviewed
later in its own chapter~\ref{cha:Sequential Monte Carlo for Bayesian Model
  Comparison}.

\section{Markov chain Monte Carlo}
\label{sec:Markov chain Monte Carlo}

Both the vanilla Monte Carlo and importance sampling methods require
simulations directly from a distribution, which is often not feasible in
realistic applications. Especially for the purpose of Bayesian modeling, the
target is often a complex posterior distribution. Estimation techniques based
on dependent samples were developed. The most important type, Markov chain
Monte Carlo (\mcmc), uses dependent samples generated by a Markov chain with
the target as a limiting distribution for estimation of desired expectations.

The basic idea is that, given a Markov chain $(X^{(1)},\dots,\Xiu,\dots)$,
whose has a limiting distribution $\pi$ over a measurable space $(E,\calE)$,
then with suitable conditions (outlined for each algorithm later), for a
$\pi$-integrable function $\varphi (\varphi
\in L^1(\pi)$
\begin{equation}
  \lim_{N\to\infty}\frac{1}{N}\sum_{i=1}^N\varphi(\Xiu) =
  \Exp_{\pi}[\varphi(X)]\qquad a.e.\ \pi
  \label{eq:mcmc convergence}
\end{equation}
And therefore samples generated by this Markov transition can be used for
estimation of various quantities in a similar fashion as with vanilla Monte
Carlo or importance sampling methods. The construction of such a
$\pi$-invariant Markov kernel leads to development of various widely used
\mcmc algorithms. In this section, some of the most important ones are
reviewed.

\subsection{Discrete time Markov chain}
\label{sub:Discrete time Markov chain}

This section briefly reviews some notions of discrete time Markov chains. Due
to its utility natural, it will be compact and only reviews results required
by later sections. For more throughout treatment of discrete time Markov
chains, the readers are referred to references therein.

\subsection{Metropolis-Hastings algorithms}
\label{sub:Metropolis-Hastings algorithms}

A Metropolis-Hastings algorithm produces a Markov chain with limiting
distribution $\pi$ with a conditional density $q(\cdot|x)$ called the
\emph{instrumental} or \emph{proposal} distribution through the following
transition. At time $t$, given sample $x^t$,
\begin{enumerate}
  \item Draw $Y^t \sim q(y|x^t)$.
  \item Take
    \begin{equation*}
      X^{t+1} =
      \begin{cases}
        Y^t, &\text{with probability } \alpha(x^t,Y^t),\\
        x^t  &\text{with probability } 1 - \alpha(x^t,Y^t).
      \end{cases}
    \end{equation*}
    where
    \begin{equation}
      \alpha(x,y) =
      \min\Curly[bigg]{\frac{\pi(y)}{\pi(x)}\frac{q(x|y)}{q(y|x)},1}.
    \end{equation}
\end{enumerate}
The probability $\alpha(x,y)$ is called the \emph{Metropolis-Hastings
  acceptance probability}. The conditions for the Markov chain produced by
this algorithm having the limiting distribution $\pi$ are quite minimal
\cite[][sec.~7.3.2]{Robert:2004tn}. Intuitively, the generated Markov chain is
aperiodic the algorithm allows events such as $\{X^{t+1} = X^t\}$. A
sufficient condition for irreducibility is that the conditional density
$q(\cdot|x)$ is positive. In other words, it allows that every set of the
$\calE$ with a positive Lebesgue measure to be reached in a single step. It
can be proved that with these two conditions, the convergence in
equation~\eqref{eq:mcmc convergence} holds, for example see
\cite[][Theorem~7.4 and Corollary~7.5]{Robert:2004tn}.

The Metropolis-Hastings algorithm is important not only because itself has
found wide applications in practice, but also it is foundation of many other
algorithms. For example the reversible jump \mcmc and population \mcmc
algorithms, reviewed later in section~\ref{sub:Reversible jump mcmc}
and~\ref{sub:Population mcmc}, can both be seen as extensions of this
algorithm, or as special cases with non-typical design of the proposal
distribution.

\subsubsection{Independent proposals}
\label{ssub:Independent proposals}

The design of efficient proposal distributions has been a difficult problem
and has attracted substantial attention in the past. The efficiency of the
proposals critically influence the performance of the resulting estimators,
etc. A proposal independent of the current state $X^t$ leads to the
independent Metropolis-Hastings algorithm. Let $\eta$ denote this proposal.
The acceptance probability becomes,
\begin{equation}
  \alpha(x,y) = \min\Curly[Big]{\frac{\pi(y)\eta(x)}{\pi(x)\eta(y)},1}.
\end{equation}
The resulting Markov chain is uniformly ergodic if the target $\pi$ is bounded
by the proposal $\eta$ up to a multiplier. In other words, there exits a
constant $M$ such that $\pi(x)\le M\eta(x)$ for all $x$ in the support of
$\pi$.

Though uniform ergodicity is a much desired property for a given algorithm,
without proper optimizing, the performance of an independent proposal is often
far from ideal in practice. The proposal $\eta$ shall be chosen such that it
maximizes the average \emph{acceptance rate} $\alpha = \Exp[\alpha(x,y)]$,
\begin{equation}
  \alpha
  = \Exp\Square[Big]{\min\Curly[Big]{\frac{\pi(Y)\eta(X)}{\pi(X)\eta(Y)},1}}
  = 2\Pr\Round[Big]{\frac{\pi(Y)}{\eta(Y)}\ge\frac{\pi(X)}{\eta(X)}},\quad
  \text{for } X\sim\pi,\ Y\sim g,
\end{equation}
provided that $\pi(X)/\eta(X)$ is absolutely continuous. This optimization is
generic in the sense that it does take the function of interest $\varphi$ into
considerations. In practice, $\eta$ shall be chosen to be close to $\pi$ as
much as possible to maximize $\alpha$. The requirement for $\pi(x)/\eta(x)$ to
be bounded also suggest that $\eta$ at least should not have a too thin tail
compared to $\pi$. Ideally it should have a slightly heavier tail than $\pi$
but not much less concentrated. In this aspect, the choice of $\eta$ is
similar to the choice of importance sampling distribution.

\subsubsection{Random walks}
\label{ssub:Random walks}

\sidenote{Shall I rename this section to ``symmetric random walk''? I did saw
  somewhere people name some non-symmetric proposals as random walk, though in
  essence they just follow the same general Metropolis-Hastings acceptance
  rule.}

The random walk Metropolis-Hastings algorithm, originally proposed in
\cite{Metropolis:1953ex}, uses proposals that are symmetric, often in form
$q(y|x) = q(\Abs{y - x})$. This leads to the acceptance probability,
\begin{equation}
  \alpha(x,y) = \min\Curly[Big]{\frac{\pi(y)}{\pi(x)},1}.
\end{equation}
This algorithm does not satisfies conditions for a uniform ergodicity.
However it is geometric ergodic under certain conditions. In
\cite{Mengersen:1996th}, a condition based on log-concavity of $\pi$ in the
tails was proposed. That is $\log\pi(x_1) - \log\pi(x_2) \ge \alpha\Abs{x_1 -
  x_2}$ for some $\alpha > 0$ and some $x_0$ such that $x_0 < x_1 < x_2$ or
$x_2 < x_1 < -x_0$.

The random walks Metropolis-Hastings algorithm is perhaps the most widely used
type of \mcmc algorithm in practice. It provides a generic working solution to
many otherwise difficult problems. As a result, its optimizations have
attracted considerable attention in researches. Intuitively the algorithm can
easily produce chains that either move too fast or too slow. In particular of
multimodal densities whose modes are separated by extreme small probability
areas, these areas clearly limited the move of the random walks. If the chains
move too fast, than it is very likely that most proposed values fall in small
probability areas and the probability of jumping from one mode to another is
arbitrarily small. This often leads to extreme small acceptance rates. On the
other hand, if the chain moves too slow, it will take long time for the chain
to explore the whole parameter space.

One of the more widely used type of proposals is the Normal distribution or
its multivariate variant. In \cite{Roberts:2001ta}, it was established that
when the dimension, say $d$, goes to infinity, and the multivariate Normal
distribution is assume to have diagonal variance matrix $I_d\sigma_d^2$, the
optimal scaling ($\sigma_d$) has a corresponding acceptance rate $0.234$.
Similar results are extended for more general target distributions.
Nonetheless, this ``optimal'' acceptance rate has been adopted widely in
practice as rule of thumb for optimizing random walk algorithms. It often
requires substantial efforts to tune an algorithm's proposal scales (such as
the variance a Normal distribution or some other measure of dispersion of the
proposal distribution) towards this optimal rate.

Alternatively, many adaptive strategies have been developed for this family of
algorithms see \cite{Andrieu:2008kh} for a recent review. The basic theme is
that a family of proposal distributions, index by some parameter, say
$\theta\in\Theta$, is considered. For example, $q(\cdot|x) = q(\cdot|x,
\theta)$. And the value of the parameter is updated along with the state $X$,
\begin{enumerate}
  \item Sample initial values $\theta^{(0)}$ and $X^{(0)}$.
  \item At iteration $i > 0$
    \begin{enumerate}
      \item Compute $\theta^{(i)} =
        \theta^{(i)}(\theta^{(0)},X^{(0)},\dots,X^{(i-1)})$
      \item Use the new proposal $q(\cdot|x^{(i-1)},\theta^{(i)})$ to sample
        the $X^{(i)}$ and accept it with the Metropolis-Hastings acceptance
        probability.
    \end{enumerate}
  \item Repeat step 2.
\end{enumerate}
It shall be noted that this scheme is not limited to random walks. There are
many methods for the updating of the parameters. See \cite{Andrieu:2008kh} for
some common practices.

One obvious problem with such adaptive scheme is that the resulting chains are
no longer Markovian. It is common practice to adapt the algorithm up to some
point $t$, and stop the adaptation and use parameter $\theta^{(t)}$ for all
iterations onwards. The first part of the generated chain is called the
\emph{burn-in period} and is usually discard afterwards and estimations will
be based on iterations after the burn-in period. There are more advanced
techniques where the adaptive scheme attain a vanishing adaptation property.
Intuitively, after a long enough period, the adaptive algorithm will only
change the parameter $\theta$ only slightly and eventually it becomes stable.
These algorithms requires more careful designs to assure the convergence of
the estimator such as \eqref{eq:mcmc convergence}.

\subsubsection{Computation of Bayes factor with \protect\mcmc samples}
\label{ssub:Computation of Bayes factor with mcmc samples}

\sidenote{The placement of this subsubsection is still a little wired}

These dependent samples from an \mcmc algorithm can be used for the purpose of
Bayesian model comparison for finite set of models through estimating the
marginal likelihood and thus the Bayes factor. The method described here is
not limited to Metropolis-Hastings algorithms. Some other algorithms reviewed
later can also use these methods. However, those algorithms often has other
methods for the purpose of approximating Bayes factor unique to them, which
can have certain advantages as we will discuss later in context.

Recall that, the marginal likelihood is written as,
\begin{equation*}
  p(\data|M_k) = \int L(\data|\theta_k,M_k)\pi(\theta_k|M_k)\intd\theta_k,
\end{equation*}
For simplicity, in this section we drop the dependency on the model $M_k$ and
simply write $p(\data) = \int L(\data|\theta)\pi(\theta)\intd\theta$.  With
samples generated by an \mcmc algorithm targeting the posterior distribution
$\pi(\theta|\data) \propto L(\data|\theta) \pi(\theta)$ available, say
$\{\supi\theta\}_{i=1}^N$, an estimator of $p(\data)$ can be obtained by the
harmonic mean \cite{Newton:1994wm},
\begin{equation}
  \widehat{p(\data)}_{\text{\textsc{hm}}}^N =
  \Round[Big]{\frac{1}{N}\sum_{i=1}^N\frac{1}{L(\data|\supi\theta)}}^{-1}
\end{equation}
Unfortunately this estimator can suffer instability problem when samples with
small likelihoods are generated. In fact this estimator does not always has a
finite variance and therefore in general does not satisfy a Central Limit
Theorem. An improvement seen in \cite{Kass:1995vb} is,
\begin{equation}
  \widehat{p(\data)}_{\text{\textsc{ghm}}}^N = \Round[Big]{
    \frac{1}{N}\sum_{i=1}^N
    \frac{\gamma(\supi\theta)}{L(\data|\supi\theta)\pi(\supi\theta)}}^{-1},
\end{equation}
where $\gamma$ is a proper density function. This is based on the identity,
\begin{equation}
  \frac{1}{p(\data)}
  = \int \frac{\gamma(\theta)}{p(\data,\theta)}
  \frac{p(\data,\theta)}{p(\data)} \intd \theta
  = \int \frac{\gamma(\theta)}{p(\data,\theta)} \pi(\theta|\data) \intd \theta
\end{equation}
It can be seen that the density $\gamma$ plays a role similar to the
importance sampling distribution. For similar reasons, high efficiency is most
likely to be achieved if $\gamma$ is roughly proportional to $L(\data|\theta)$
\cite{Kass:1995vb}. And the above equation suggests that the estimator has a
finite variance if the tails of $\gamma$ is thin enough compared to the
unnormalized posterior distribution $\pi(\theta|\data)$.
\cite{Gelfand:1994ux} suggested the use of a multivariate Normal distribution
with moments approximated from the samples as a natural choice of $\gamma$. In
addition, the variance of the estimator can also be estimated for
$1/\widehat{p(\data)}$ from the posterior samples through,
\begin{equation}
  \var\Square[Big]{\frac{1}{\widehat{p(\data)}_{\text{\textsc{ghm}}}^N}} =
  \frac{1}{N^2}\sum_{i=1}^N \Round[Big]{
    \frac{\gamma(\supi\theta)}{L(\data|\supi\theta)\pi(\supi\theta)}
    - \frac{1}{\widehat{p(\data)}_{\text{\textsc{ghm}}}^N}}^2.
\end{equation}
The above equation provides a way of monitoring the convergence of the
estimator. Though more stable than the harmonic mean estimator
$\widehat{p(\data)}_{\text{\textsc{hm}}}$, this generalized estimator still
requires considerable care in the implementation, especially the choice of the
density $\gamma$ to ensure good performance and indeed a finite variance
estimator.

\subsection{Gibbs sampling}
\label{sub:Gibbs sampling}

In a general setting, a multi-stage Gibbs sampler assumes that the random
variable $X$ can be written as $X = (X_1,\dots,X_p)$, where $X_i$'s are either
unidimensional or multidimensional. Moreover, suppose that we can simulate
from the corresponding conditional densities $\pi_1,\dots,\pi_p$, defined as,
\begin{equation}
  X_i|x_1,\dots,x_{i-1},x_{i+1},\dots,x_p
  \sim \pi_i(x_i|x_1,\dots,x_{i-1},x_{i+1},\dots,x_p)
\end{equation}
for $i = 1,\dots,p$. The associated \emph{Gibbs sampler} is given by the
following algorithm that transit $X^t$ to $X^{t+1}$. Given $X^t$ at time $t$,
by generating $X^{t+1}$ in $p$ steps. At each step $i$, $X_i^{t+1}$ is
generated from
\begin{equation}
  X_i^{t+1} \sim
  \pi_i(x_i|x_1^{t+1},\dots,x_{i-1}^{t+1},x_{i+1}^t,\dots,x_p^t).
\end{equation}
The densities $\pi_1,\dots,\pi_p$ are called the \emph{full conditionals}.

Gibbs samples have several features that make it one of the most popular \mcmc
algorithms used in practice, but also limited its use in some cases, see
\cite[][sec.~10.1.1]{Robert:2004tn}.
\begin{enumerate}
  \item A Gibbs sampler has an acceptance rate of $1$, which is quite obvious
    from the setting of this algorithm. Therefore all samples are used in
    estimations, etc.
  \item The construction of a Gibbs sampler requires prior knowledge of the
    analytical properties of the target. This largely limited its use in the
    cases of complex models.
  \item The Gibbs sampler does not apply to problems where the number of
    parameters varies.
\end{enumerate}

An important technique applied to Gibbs sampler is called \emph{completion}.
Often, the full conditionals of $\pi$ are difficult to sample from or are not
explicit at all. In this situations, a distribution, say $\eta$ with the
following property is chosen,
\begin{equation}
  \int\eta(x,y)\intd y = \int\pi(x)\intd x.
\end{equation}
Then the Gibbs sampler is constructed with the full conditionals of $\eta$
instead of $\pi$. The sub-chain of the resulting Markov chain that
corresponding to the marginal $\pi$ is then $\pi$-invariant. Many realistic
applications, such as \emph{missing data} models, can use such techniques to
construct Gibbs sampler. The theoretical justification of the Gibbs sampler
relies mostly on the Hammersley-Clifford theorem, which states that given all
the full conditionals, the joint distribution can be derived up to a
normalizing constant.

The drawbacks of the Gibbs sampler is different from those of the
Metropolis-Hastings algorithm. The composite structure of the sampler is both
the strength and the weakness of the algorithm. As seen in
\cite{Hills:1993vb}, poor parameterization can lead to slow convergences of
the resulting chain. Intuitively, the decomposition of the joint density gives
a particular coordinate system with each step only explore one of the
coordinates. It may take many cycles for the sampler to move around the
surface of the joint distribution.

\subsubsection{Relation with the Metropolis-Hastings algorithm}
\label{ssub:Relation with the Metropolis-Hastings algorithm}

The Gibbs sampler can be viewed as a special case of Metropolis-Hastings
algorithm. In particular, it is equivalent to a composition of
Metropolis-Hastings algorithms with acceptance probability uniformly equal to
$1$ \cite[][Theorem~10.13]{Robert:2004tn}.

A common difficulties of the Gibbs sampler is that some of the full
conditionals cannot be easily sampled from. In this situation, Gibbs samplers
are often combined with Metropolis-Hastings algorithms, sometimes termed
\emph{Metropolis-within-Gibbs}. Instead of sampling directly from the full
conditionals, a proposal distribution is used. And the proposed values is
accepted or rejected with the usual Metropolis-Hastings algorithm rule.

\subsubsection{Computation of Bayes factor with Gibbs sampler output}
\label{ssub:Computation of Bayes factor with Gibbs sampler output}

In addition to methods described in section~\ref{ssub:Computation of Bayes
  factor with mcmc samples}, in the particular case of Gibbs sampler,
\cite{Chib:1995em} provides an alternative approach based on that the
identity,
\begin{equation}
  p(\data) = \frac{L(\data|\theta)\pi(\theta)}{\pi(\theta|\data)}
\end{equation}
holds for any value of $\theta$. Therefore an estimator can be obtained by
substituting $\theta$ with a specific value, say $\theta^*$, which is usually
chosen from the high probability region of the posterior distribution and
approximating the denominator using outputs from the Gibbs sampler. In
addition to the usual requirement of a Gibbs sampler, that all the full
conditionals can be sampled from, this method also requires that all these
densities are known including their normalizing constants, and thus can be
computed point-wise. The advantage is that this estimator does not suffer from
the instability problem as the harmonic mean estimator and its
generalizations. A generalization to other Metropolis-Hastings algorithms was
provided by \cite{Chib:2001gq}, where only the proposal distributions are
required to be known including their normalizing constants.

\subsection{Reversible jump \protect\mcmc}
\label{sub:Reversible jump mcmc}

The reversible jump \mcmc (\rjmcmc) algorithm, introduced by
\cite{Green:1995dg}, is a technique widely used for simulations where the
dimension of the parameter space is not fixed. In the context of Bayesian
model selection, it can used for inference of the full posterior
$\pi(\theta_k,M_k|\data)$, which is defined on the space $\Theta =
\bigcup_{k\in\calK}\{M_k\}\times\Theta_k$ (see section~\ref{sec:Bayesian model
  choice}.) Situations where this can be reduced to the estimation of the
Bayes factor (section~\ref{sub:Bayes factor}), techniques reviewed so far in
this chapter can be used. However, when $\calK$ is (infinite) countable, or
for other reasons, direct inference on the full posterior distribution is
desired, \rjmcmc is the most widely used technique.

Not unlike the \mcmc algorithms reviewed before, \rjmcmc also relies on the
existence of the detailed balance condition, only the parameter space is more
complicated and hence the design the Markov transition kernel. The \rjmcmc
algorithm adapts the Metropolis-Hastings algorithm to construct such kernels.
Instead of a single type of moves defined a proposal density, a countable set
of moves are considered, say $m\in\calM$. Each type of moves is capable of
moving the current state of the Markov chain between, say $\Theta_k$ and
$\theta_{k'}$ (where in the case of $k = k'$, the move is similar to a those
in a \mcmc algorithm on a fixed dimension space). At state
$\theta_k\in\Theta_k$, a move type $m$ together with a new state
$\theta_{k'}\in\Theta_{k'}$ are proposed according to the
$q_m(\theta_{k'}|\theta_k)r_m(\theta_k)$, where $r_m(\theta_k)$ is the
probability of choosing type $m$ move when at state $\theta_k$; and
$q_m(\theta_{k'}|\theta_k)$ is the proposal kernel for the new state when a move
of type $m$ is made. The move is accepted with probability,
\begin{equation}
  \alpha(\theta_k,\theta_{k'}) =
  \min\Curly[Big]{1,
    \frac{\pi(M_{k'})\pi(\theta_{k'}|M_{k'})p(\data|\theta_{k'},M_{k'})}
    {\pi(M_k)\pi(\theta_k|M_k)p(\data|\theta_k,M_k)}
    \frac{q_m(\theta_k|\theta_{k'})r_m(\theta_{k'})}
    {q_m(\theta_{k'}|\theta_k)r_m(\theta_k)}
  }.
\end{equation}
In practice, the proposed new state $\theta_{k'}$ is often implemented by
drawing a vector of continuous random variables, say $u$, independent of
$\theta_k$ and a deterministic bijection of vector $(\theta_k,u)$ to
$\theta_{k'}$, say $\theta_{k'} = T(\theta_k,u)$. The inverse of the move from
$\theta_{k'}$ back to $\theta_k$ then uses the inverse of this transformation.
Through a simple change of variable, the conditional density
$q_m(\theta_{k'}|\theta_k)$ can be expressed in terms of the density of vector
$u$ and its density $q(u)$. The acceptance probability becomes
\begin{equation}
  \alpha(\theta_k,\theta_{k'}) =
  \min\Curly[Big]{1,
    \frac{\pi(M_{k'})\pi(\theta_{k'}|M_{k'})p(\data|\theta_{k'},M_{k'})}
    {\pi(M_k)\pi(\theta_k|M_k)p(\data|\theta_k,M_k)}
    \frac{r_m(\theta_{k'})}{r_m(\theta_k)}
    \frac{1}{q(u)}
    \Abs[Big]{\frac{\partial\theta_{k'}}{\partial(\theta_k,u)}}
  },
\end{equation}
where the last term is the determinant of the Jacobian of the transformation.
The design of efficient between-model moves is often difficult, and the mixing
of these moves largely determines the performance of the algorithm.

The main difficulties lie in the choice of cross-model proposals and the
bijection $T$. Though the mapping $T$ theoretically is quite flexible, its
creation and optimization can be quite difficult in practice. This is
specifically true when the parameter space is complicated. In some extreme
cases, creating a valid kernel is already difficult, leave alone the
optimization. For example, in multimodal models, where \rjmcmc has gain
substation attention, information available in posterior distributions of any
given model does not characterize the modes that exits only in models of
higher dimension; and thus a successful between model move between these
dimensions becomes difficult \cite{Jasra:2007id}. Inefficient proposals
results in Markov chains that are slow to explore the whole parameter space.
However the natural ideas of neighborhood and others, which proved to be
useful concepts in practice, for within model simulations, may no longer be
intuitive in the variable dimension model settings. In addition, \rjmcmc does
not characterize models with low posterior probabilities well. In some cases
it will be difficult to determine whether the low acceptance rates of between
model moves results from actual characteristics of the posterior or from a
poorly adapted proposal kernel.

Some discussion of the optimization of the cross-model moves can be found in
\cite{Green:2009tr}. Also the adaptive scheme for Metropolis-Hastings
algorithms has been extended to for \rjmcmc, for example \cite{Hastie:2005vi}.
However little other work are known for the actual performance of this kind of
improvement to \rjmcmc. \cite{Green:2001tk} discussed a method called
\emph{delayed rejection}. In this method, a rejection of a proposal does not
immediately lead to the acceptance of current state, instead a second proposal
is attempted. Their numerical results showed efficiency improvement but with
increased computation cost.

Despite all these efforts, the challenges of implementations of \rjmcmc is
still the main issue that limits its use in practice. We are going to look for
alternative ways of doing Bayesian computation and it is proposed in the next
chapter.

\subsection{Population \protect\mcmc}
\label{sub:Population mcmc}

Population-based methods have been considered in recent researches. An entire
family of such algorithms, sequential Monte Carlo, is considered in
chapter~\ref{cha:Sequential Monte Carlo for Bayesian Model Comparison}.
Another algorithm, population \mcmc, which has seen applications in the area of
Bayesian model comparison, is reviewed in this section.

Population \mcmc operates by constructing a sequence of distributions
$\{\pi_t\}_{t=0}^T$ with at least one of them being the target distribution
$\pi$. Parallel \mcmc chains are simulated for each of these distributions. In
addition, the chains interact with each other with swapping or crossover
moves, which allows the fast mixing chains to ``lend'' information to the more
slowly mixing chains. The outputs are therefore samples that approximate the
product $\prod_{t=0}^T\pi_t$ with the target distribution being a marginal.

Different choices of the sequence is possible. One commonly used in practice
is called temperating. For a target $\pi$, a sequence $\{\pi_t\}_{t=0}^T$ is
constructed such that,
\begin{equation}
  \pi_t(x) = [\pi(x)]^{\alpha(t/T)}
\end{equation}
where the mapping $\alpha:[0,1]\to[0,1]$ is monotonically increasing such that
$\alpha(1) = 1$. Other similar schemes can be constructed. For example, in the
context of Bayesian modeling where $\pi$ is the posterior distribution and
$\pi(\theta|\data) \propto \pi(\theta)L(\data|\theta)$, one can construct a
sequence
\begin{equation}
  \pi_t(\theta) = \pi(\theta)[L(\data|\theta)]^{\alpha(t/T)}
  \label{eq:power post}
\end{equation}
where the monotonically increasing mapping $\alpha$ is such that $\alpha(0) =
0$ and $\alpha(1) = 1$. Therefore the sequence moves smoothly from the prior,
which can usually be sampled easily, into the posterior.

More precisely, the algorithm target the distribution $\prod_{t=0}^T\pi_t$.
After initialization $T$ Markov chains for each of the marginals
$\{\pi_t\}_{t=0}^T$ on a common space $(E,\calE)$, at each iteration, two type
of moves are performed. One is \emph{local}, sometimes termed \emph{mutation},
that advance each chain individually using an \mcmc algorithm such as
Metropolis-Hastings algorithm or a Gibbs sampler. One can select one chain at
random in each iteration to mutate or advance or chains in parallel. The other
type of move is \emph{global}. The purpose is to allow fast mixing chains to
transfer information into slower mixing chains. There are several approaches
\cite{Jasra:2007in}. Two most widely used are the following.

\paragraph{Exchange} The exchange move select two chains at random, say $t_1$
and $t_2$, and propose to exchange the state between them. The proposed
exchange is accepted or rejected according to the Metropolis-Hastings
acceptance probability. That is, given $x_1$ and $x_2$ between the current
state of the two selected chains, the exchange is accepted with probability
\begin{equation}
  \alpha(x_1, x_2) =
  \min\Curly[Big]{\frac{\pi_1(x_2)\pi_2(x_1)}{\pi_1(x_1)\pi_2(x_2)}, 1}.
  \label{eq:exchange accept}
\end{equation}
For this to work, usually the two chains are chosen such that they are
adjacent to each other in the sense that one is chosen randomly and the other
is selected to be the one most close to it. For example, in the temperating
scheme, usually a chain $t\in{0,1,\dots,T-1}$ is chosen randomly and it is
proposed to be exchanged with chain $t+1$. The delayed rejection approach in
\cite{Green:2001tk} (see section~\ref{sub:Reversible jump mcmc}) can also be
used in the exchange moves. Thus instead of chosen adjacent chains, two chains
in some sense that are very different can also be chosen. In either case, the
key is that the chains chosen to be exchanged are chosen uniformly over all
chains.

\paragraph{Crossover} In \cite{Liang:2001dc} an idea of crossover move was
introduced. Instead of propose to exchange the whole state $x_1$ and $x_2$,
after the two chains are chosen, only parts of the two states are proposed to
be exchanged. Suppose the state $x$ can be partitioned into $x =
(x_1,\dots,x_p)$ in the same way for each chain. Then a random position, say
$l$ is chosen and the position $l$ of $x_1$ is proposed to be exchanged with
its counter-part in $x_2$. The acceptance probability is the same as
equation~\eqref{eq:exchange accept} with suitable notation changes. In
\cite{Jasra:2007in} it was found that this can be more efficient than the
exchange move.

The population \mcmc allows efficient simulation of previously difficult
problem, though at a cost of increasing computational cost. The algorithm
requires another layer of optimization in additional to the mixing speed of
each local moves -- the chosen of the sequence of distributions
$\{\pi_t\}_{t=0}^T$. If too many chains are chosen, then the information can
take many global moves to transfer from the fast mixing chains to the slower
ones. If too few chains are chosen or they are placed far apart of each other,
then the global moves are likely to have a small acceptance rate. This is a
problem similar to the random walk Metropolis-Hastings algorithm. In
\cite{Atchade:2010ha}, based on the idea of maximizing the average information
exchanged at each iteration, it was established in special cases that an
optimal placement of the distributions shall have an global acceptance rate
$0.234$.

\subsubsection{Computation of Bayes factor with population \protect\mcmc}
\label{ssub:Computation of Bayes factor with population mcmc}

The output from a population \mcmc algorithm can be used to compute the Bayes
factor in the same way as other \mcmc algorithms (see
section~\ref{ssub:Computation of Bayes factor with mcmc samples}.) One
alternative, proposed in \cite{Calderhead:2009bd} is to use path sampling
estimator \cite{Gelman:1998ei}.

Given a parameter $\alpha$ which defines a family of distributions,
$\{\pi_{\alpha} = \gamma_{\alpha}/Z_{\alpha}\}_{\alpha\in[0,1]}$ which move
smoothly from $\pi_0 = \gamma_0/Z_0$ to $\pi_1 = \gamma_1/Z_1$ as $\alpha$
increases from zero to one, one can estimate the logarithm of the ration of
their normalizing constants via a simple integral relationship,
\begin{equation}
  \log\Round[Big]{\frac{Z_1}{Z_0}} = \int_0^1\Exp_{\alpha}\Square[Big]{
    \frac{\diff\log\gamma_{\alpha}(\cdot)}{\diff\alpha}}\intd\alpha,
  \label{eq:path integral}
\end{equation}
where $\Exp_{\alpha}$ denotes the expectation under $\pi_{\alpha}$. Note that
the sequence of distributions in equation~\eqref{eq:power post} can both be
interpreted as belong to such a family of distributions with $\alpha =
\alpha(t/T)$. The population \mcmc provides samples that can be used to
approximate this path sampling estimator. Given samples
$\{supi{X_0},\dots,\supi{X_T}\}_{i=1}^N$ from $N$ iterations of a population
\mcmc sampler, one can approximate the expectation under distribution
$\pi_{\alpha} = \pi_{\alpha(t/T)} = \pi_t$ by the empirical average of the
subchain $\{\supi{X_t}\}_{i=1}^N$. And the integration~\eqref{eq:path
  integral} can be approximate with a numerical integration scheme such as
the Trapezoidal rule.

The use of path sampling for estimating of Bayes factor will be revisited in
chapter~\ref{cha:Sequential Monte Carlo for Bayesian Model Comparison}.

\section{Other development}
\label{sec:Other development}

\sidenote{For the purpose of completeness, I think a number of other
  algorithms shall be at summarized in this section. This chapter only
  reviewed a few most widely used ones. I am still gathering materials for
  this}

\ifx\inthesis\undefined
\printbibliography
\end{document}\else\relax\fi
