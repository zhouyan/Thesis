\chapter[vSMC: A C++ Library for Parallel SMC]
{\protect\vsmc: A C++ Library for Parallel SMC}
\label{cha:vSMC: A C++ Library for Parallel SMC}

The \vsmc library was developed during the research to assist the
implementation of various \smc and other algorithms, including but not limited
to the implementation of the illustrative and performance comparison examples
in previous chapters. It evolves into a sophisticated \cpp framework for
implementing \smc algorithms on both sequential and parallel hardware.

The library makes use of some modern \cpp techniques. One shall not need to be
expert on all of them to use the library. Most of the examples in this chapter
shall be self-explanatory to readers with some basic knowledge of \cpp. For
those interested, appendix~\ref{sub:Modern C++} serve as a brief introduction
to \cpp templates and callable objects, two features used extensively in the
\vsmc library.

\section{Background}
\label{sec:vSMC Background}

\subsection{Parallel computing}
\label{sub:Parallel computing}

Parallel computing is a form of computation in which many calculations are
carried out simultaneously. It operates on the principle that large problems
can be divided into independent smaller ones and can be solved concurrently
(``in parallel''). Parallelism has been practiced for many years in the form
of high performance computing. In recent years, it has also become the
dominant paradigm for desktop computing in the form of multicore processors.
However, many of today's popular statistical softwares are written with
serialization as an assumption, meaning that they do not easily take advantage
of contemporary computer architectures. (see appendix~\ref{sub:Classes of
  parallel computers} for a discussion of the commonly used parallel
computers.)

\subsubsection{Parallelism strategies}
\label{ssub:Parallelism strategies}

The best overall strategy for \emph{scalable parallelism} is \emph{data
  parallelism} \cite{datapar}. There are various definitions of data
parallelism. Narrower definitions only permit collection-oriented operations,
such as applying the same function to all elements of an array. A wider view
is that the parallelism grows (preferably linearly) as the data size or the
problem size grows. For example, parallelizing a vanilla Monte Carlo
integration algorithm belongs to this strategy. As the number of samples
increases, one can always use more parallel computing resource to run the
sampler with the same amount of time without increasing the speed of each
computing unit. Note that, here we ignored issues such as generating random
numbers in parallel, which will be discussed later. In contrast, an \mcmc
algorithm usually cannot be parallelized in a scalable way. To obtain better
statistical results, often the only way is to increase the number of
iterations, and thus no matter how much parallel computing resource are
available, the computing time will increase without increasing of the speed of
the processors. It shall be obvious that many \smc algorithms can be
parallelized using data parallelization in a scalable way.

The opposite of data parallelism is \emph{functional parallelism}, an approach
that runs different functional parts of a program in parallel. At best,
functional parallelism can improvement the performance by a constant speedup.
For example, say a program performs functions $f_1,\dots,f_k$, then at best
the computing time can be reduced by $k$-fold through parallelism. In the
remaining of this chapter, we focus on data parallelism.

For specific problems, there are various design patterns to parallelize the
computation. Interested readers can see appendix~\ref{sub:Parallel patterns}
for a discussion on this topic.

\subsubsection{Importance of parallel computing}
\label{ssub:Importance of parallel computing}

Parallel computers has been developed for decades. Several reasons have led to
increased level of parallel computing in individual, mainstream personal
computers.

The most significant one is the hardware trend. From 1973 to 2003, clock rates
of processors increased from 1 MHz to 1 GHz. However, since then there are
little improvement on this front. Now most high end workstations have
processors with clock rates at about 2.5 GHz. However, virtually all
processors produced now have multiple cores \cite{parallel}. Eight to twelve
cores configurations are common in middle to high end workstations and
personal computers often have at least two cores with quadric configurations
more and more commonly seen, while the clock rates not only remains flat, but
also has the trend of decreasing. These changes are due to various technical
difficulties in increasing the clock rates among other reasons, which we will
not elaborate further here.

Scientists are ever seeking to solve more complex problems, which often
requires more computations. To solve larger problems without use significantly
longer computing times, in the foreseeable future the only way is to use
parallelism.

Parallel computing is also much more economic in both power consumption and
processors' production than sequential computing \cite{parallel}. In reality,
it means researchers can invest the same or less amount of funding, yet get
more computing work done with the same or less time.

\subsubsection{Performance measurement}
\label{ssub:Performance measurement}

Unlike sequential computing, the performance of parallel computing is more
difficult to study. In sequential situation, the computational cost can often
be deduced from the algorithms easily. For example, a Monte Carlo algorithm
can use the total number of samples to be generated as a measure of its
computational cost. However, in the case of parallel computing, the total
amount of computation, whether measured as number arithmetic operations or
data operations, cannot reflect the cost in reality. This is due to the fact
that, today's parallel computers is much more cost efficient when more work
are parallelized \cite{parallel}. In practice, one is most concerned with the
speedup of a parallel program, defined as the ratio between running time of a
sequential program and the one of a parallel program that does the same work.

Let $P$ be the number of hardware workers, e.g., cores in a multicore
processor or nodes in a cluster, and $T_P$ be the total time of computation.
$T_1$ is usually called the \emph{work} of the program and $T_{\infty}$ is
called the \emph{span}. The speedup, defined as $S_P = T_1/T_P$, is upper
bounded,
\begin{equation}
  S_P \le \frac{T_1}{T_1/P} = P
\end{equation}
In addition, assuming that adding processors never slows down the program (it
is only the case when $P$ is modest in reality),
\begin{equation}
  S_P \le T_1/T_{\infty}
\end{equation}

Implementations on different hardwares often are interested in one of the
three quantities, $T_1$, $T_P$ and $T_{\infty}$. For sequential
implementations clearly $T_1$ is the only one of interest. For multicore and
\smp systems, $T_P$ is of interest for a particular value $P$. $T_{\infty}$ is
previously of less interest, which is only considered as an ideal situation.
However, the recent development on massive parallel computers has made it
close to a reality for many algorithms.

\subsubsection{Limitations}
\label{ssub:Limitations}

Parallel computing is not without drawbacks. Two main factors that limit its
widespread use in practice is the difficulty in reasoning of the program and
the tuning of performance.

Parallel programs are more difficult to construct \emph{correctly} than a
sequential program. Because its parallel natural, many operations may be
performed concurrently and may happen with random orders or at the same time.
However, due to reasons such as data dependency, some operations have to be
performed in a deterministic order in order to obtain meaningful results.
Informally, when two workers try to modify the same location of data, the
behavior is undefined. This is also called \emph{data race}.

Another difficulties of parallel computing is the tuning and portability of
performance. Since the development of language such as Fortran and C,
scientists have relied on them to develop portable softwares. There are two
sides of portability. One is the programming portability, meaning that the
same source code can be used to build softwares for different platforms with
little or no modifications. The other one is the performance portability,
meaning that the softwares built from the same source code for different
platforms have comparable performance. In the early days, people need to
optimize programs for each platform individually. However, with the
development of modern compiler techniques, such practices are much less seen.

In the era of parallel computing, many low level details need to be taken care
of to obtain reasonable performance. For example, while using the \openmp
programming model, which is widely used by scientists to write parallel
programs, issues such as thread affinity (associate each thread with a
particular processor) can often cause large performance differences. More
recently, devices such as \gpu{}s are even less performance portable. The same
\opencl \cite{opencl} program can have an order of magnitude difference in
performance when running on devices from different vendors even they have
similar raw computational power.

However, we believe that with the development of developer tools for parallel
computing, such issues will become less common and it will help the wider
spread of parallel computing.

The last but not least problem with parallel computing is that, not all
algorithms can be parallelized or at least not efficiently. Many \mcmc
algorithms are typical examples. And they can hardly benefit much from future
computer technology advancement. This issue can only be solved by developing
new algorithms that are more suitable for today's and future computers.

\subsection{Softwares for Monte Carlo computing}
\label{sub:Softwares for Monte Carlo computing}

Over the decades there are many softwares developed for the purpose of Monte
Carlo computing, especially for more established algorithms such as \mcmc. It
is impossible to give a complete review of even those most important ones
here. Most of them can be characterized by a two aspects,
\begin{itemize}
  \item Application area
  \item Software environments
  \item Implementation level
\end{itemize}
Some softwares are designed with general application in mind. They can be used
to solve a large array of problems. Some others target specific applications
and some of them are designed to implement a particular model.

There are the difference in software environments. Some are standalone
software. They are often the easiest to use. Others depend on a larger
software environment. For example, many numerical tools are developed using
\matlab \cite{matlab}. In recent years, the \rlang programming language
\cite{rlang} has gain substantial popularity among statisticians. These
softwares often requires at least basic knowledge of the environment (e.g.,
\matlab or \rlang) to use. There are also softwares developed for low level
languages such as \cpp, distributed in the form of libraries. They may have
an even steeper learning curve.

\sidenote{I need to coin a better term than "implementation level"} By
\emph{implementation level}, we mean how much of a given algorithm are
implemented by the software and how much are left to be implemented by the
user. This is closely related to the application area of the software. At the
lowest level, some softwares used to solve a particular problem are
implemented using some programming language such as \rlang or \cpp from ground
up. Some of them provide frameworks on top of which the user only need to fill
in some problem specific informations.

In the following, we review a few more important development for Monte Carlo
computing. Most of those that are relevant to the work in this thesis can be
categorized by whether they solve \mcmc or \smc problems.

\subsubsection{Softwares for \protect\mcmc computing}
\label{ssub:Softwares for mcmc computing}

The most noticeable of softwares for \mcmc algorithms is perhaps \bugs
\cite{bugs, bugsbook}. It provides a easy to use environment for Bayesian
modeling using the Gibbs sampler. The user need to specify the model using
\bugs model specification language, which describe the model variables in a
direct acyclic graph (\dag) and the data using a similar language. The
software analyzes the model and choose \mcmc algorithms to do the sampling. It
is a easy to use practical tool for Bayesian analysis. The output of \bugs are
usually analyzed with \rlang, using packages such as \rcoda \cite{rcoda}.

The limitation of \bugs is that the resulting algorithm may not be well tuned
and it can take a long time to get reasonable results. In particular, it is
very difficult for the user to provide insights of the algorithm design, such
as the proposal scales of random walks. In addition, it also does not allow
more flexible design of data structures which can lead to significant
inefficiency for some applications.

There also many packages for the \rlang environment that implement \mcmc
algorithms. The \rmcmcpack \cite{rmcmcpack} package provides model specific
\mcmc algorithms for a wide range of models commonly used in social and
behavioral science. The \rmcmc \cite{rmcmc} package can be used for
implementation Metropolis random walk for continuous time random vector. There
are also many more application specific packages. For example, the \rlang
project's task view on Bayesian
inference\footnote{\url{http://cran.r-project.org/web/views/Bayesian.html}}
list dozens of packages among which many of them implement \mcmc algorithms
for specific models. These packages are often very useful in their application
areas but with less generality.

Another interesting development is \pymc \cite{pymc}. It is distributed as a
module for the \python programming language. It is highly influenced by \bugs
while providing more flexibility and better performance through \python. The
\cppbugs \cite{cppbugs} is similar to \pymc but is a library for \cpp. It is
possible to archive much better performance using \cppbugs compared to \bugs
while with only a little more programming efforts. Both \pymc and \cppbugs
give users access to a general programming language in the process of
designing the algorithms. Much flexibility and performance are gained through
this compared to \bugs. On the other hand, to use them to their full potential
the users do need to have good knowledge of the programming languages.

Overall, for \mcmc algorithms, it is often not very difficult to develop
application specific software using a general purpose programming language
such as \rlang or \cpp. And widely used software tends to solve a particular
class of problems instead of provide a more general framework for
implementation of algorithms. Bayesian inference is certainly one of the more
important application area of \mcmc. And many softwares have been developed
for this purpose, with \bugs being perhaps the most influential and relative
more general in applications and other work such as the various \rlang
packages that target specific models.

\subsubsection{Softwares for \protect\smc computing}
\label{ssub:Softwares for smc computing}

Unlike \mcmc algorithms, even the simplest \smc algorithms can be difficult to
implement in general purpose programming languages for many researchers. There
is a need for softwares that help researchers to implement complex generic
\smc algorithm.

Many \smc algorithms are often more computational intense than typical \mcmc
applications. This can be partially attributed to the fact that \smc
algorithms are often used to simulate complex high dimensional distributions,
for which \mcmc algorithms often perform poorly.  The application of interest
for \smc algorithms requires highly efficient implementations.

There are relatively less development of softwares for \smc algorithms. The
\pflib \cite{pflib} toolbox for \matlab is an early development. It is
restricted to the particle filter setting.

More recently, the \smctc library \cite{smctc} was developed. It provide a
framework for implementation of generic \smc algorithms in \cpp. It possible
to implement many realistic algorithms using the library with relative ease.
It also provides very good performance. The generic framework was built with
\cpp template techniques. It allows a wide range of applications while
requires some expertise in \cpp. These are two traits also shared by the \vsmc
library.

There are also a few \rlang packages that provides implementations of \smc
algorithms. For example the \rsmc \cite{rsmc} package can be used to implement
some generic \smc algorithms. However it is more a skeleton of \smc algorithms
with much of the implementation details such as resampling needed to be
provided by the user. Overall, \rlang packages for \smc algorithms are much
less common than those for \mcmc algorithms.

As \smc family algorithms gains more attentions in research areas, such as
Bayesian inference, some application specific softwares are also developed.
The \biips \cite{biips} package aims to provide users an interface similar to
that of \bugs with \smc as the underlying algorithms for inference instead of
\mcmc. It is built with \smctc among other softwares. It can be used as a
drop-in replacement of \bugs in many applications.

Another interesting development is the \libbi library \cite{libbi}. It is
particularly suited for Bayesian state-space modeling. It provides a easy to
use interface using the \perl programming language. One does not need to know
much of the language to use \libbi's interface. However, proficiency in \perl
allows much flexibility in the design of the algorithm. This is similar to the
\pymc module for \mcmc algorithms. The library can also construct parallelized
sampler for a wide range of hardware.

\subsubsection{Parallelized Monte Carlo computing}
\label{ssub:Parallelized Monte Carlo computing}

Parallel computing can be used to accelerate Monte Carlo applications. However
due to its very sequential natural, \mcmc algorithms has seen little
development on this front. All softwares for \mcmc computing discussed before
are built with sequential implementations.

Driven by the need to simulate complex distributions efficiently and the
desire to use parallel computing to solve larger problems, many algorithms
that are particularly suitable for parallelization has been developed in the
past decade. The \smc and related algorithms are clearly one of them. The
\pmcmc algorithm can also be parallelized though less efficiently (e.g., see
results in section~\ref{sub:Nonlinear ordinary differential equations}
and~\ref{sub:Population mcmc}.) More recently the particle \mcmc algorithm
\cite{Andrieu:2010gc} is also well suited for parallel computing.

There is no lack of interest in using parallel computing for these algorithms.
For example, \cite{Lee:2010fm} studied the implementation of \smc algorithms
on massive-parallel hardware (e.g., \gpu). The results are encouraging.
However, there are yet few softwares for the purpose of implementation of
generic \smc algorithms in parallel computers. The \libbi library is a notable
exception.

There are also more fundamental work done in this area. One more important
aspect is the generating random numbers in parallel. Conventional pseudo-\rng
generate random numbers using an internal state, say $x_t$ and iterate it with
a deterministic transformation, $x_{t+1} = f(x_t)$. A data dependency exists
between $x_{t+1}$ and $x_t$, which prevent scalable parallelization. For
example, algorithms in \cite{Lee:2010fm} used to generate random numbers has a
cost greater than $O(N)$ where $N$ is the number of parallel computing units.
One solution to this problem is using state-less \rng. Informally, given a
collection of values $\{x_i\}_{i=1}^N$, the collection $\{y_i\}_{i=1}^N$ where
$y_i = f(x_i)$, appears to be random. There are no dependencies among
$\{x_i\}_{i=1}^N$. Therefore the collection $\{y_i\}_{i=1}^N$ can be generated
in parallel efficiently. The work by \cite{Salmon:2011um} provides accessible,
high performance state-less \rng. It is also used by the \vsmc library.

We believe there is a need of softwares similar to \smctc, which provides a
framework for implementation of generic \smc algorithms (in contrast to
applicable for only a class of models) yet taken full advantage of today's
parallel computers. The \vsmc library aims to fill this gap. It is less easy
to use than softwares such as \libbi or \biips. But it is possible to use to
obtain more flexibility in the design of the algorithm and better performance.

\section{The vSMC library}
\label{sec:The vSMC library}

To obtain and install the library, see detailed instructions in
\cite{vsmcjss}, which also documents the third-party dependencies and compiler
support. A \doxygen \cite{doxygen} generated reference manual can be found at
\url{http://zhouyan.github.io/vSMC/doc/html/index.html}. It is beyond the
scope of this chapter to document every feature of the \vsmc library. In many
places we will refer to this reference manual for further information.

A more systematic tutorial of the library can be found in \cite{vsmcjss} and
the reference manual. The remaining of this chapter is structured according to
the common tasks performed by generic \smc samplers. Many features of the
library are introduced in examples. Interested readers can see the tutorial
\cite{vsmcjss} and the reference manual for details.

The \vsmc library makes use of \cpp's template generic programming to
implement general \smc algorithms. The library is formed by a few major
modules. In the remaining of this chapter, unless stated otherwise, all public
classes and functions of the library resides in the namespace
\cppinline{vsmc}.

\paragraph{Core}

The highest level of abstraction of \smc samplers. Users interact with classes
defined within this module to create and manipulate general \smc samplers.
Classes in this module include \cppinline{Sampler}, \cppinline{Particle} and
others. These classes use user defined callback to perform application
specific operations, such as updating particle values and weights.

\paragraph{Symmetric Multiprocessing (\smp)}

This is the form of computing most people use everyday, including
multiprocessor workstations, multicore desktops and laptops. Classes within
this module make it possible to write generic operations which manipulate a
single particle that can be applied either sequentially or in parallel through
various parallel programming models. A method defined through classes of this
module can be used by \cppinline{Sampler} as callback objects.

\paragraph{Message Passing Interface}

\mpi is the \emph{de facto} standard for parallel programming on distributed
memory architectures. This module enables users to adapt implementations of
algorithms written for the \smp module such that the same sampler can be
parallelized using \mpi. In addition, when used with the \smp module, it
allows easy implementation of hybrid parallelization such as \mpi/\openmp.

\paragraph{\opencl}

This module is similar to the two above except it eases the parallelization
through \opencl, such as for the purpose of General Purpose \gpu Programming
(\gpgpu). \opencl is a framework for writing programs that can be execute
across heterogeneous platforms. \opencl programs can run on either \cpu or
\gpu.

\subsection{Core classes}
\label{sub:Core classes}

There are over two hundred classes, large and small, in the \vsmc library. It
is beyond the scope of this chapter to document most of them. However, there
are a few of them play central roles in implementation of \smc algorithms. In
this section, we provide an overview of them. Many of them are feature rich.
Instead of document their interfaces here, we will introduce useful features
through examples later.

\paragraph{Value collection type}

This is actually not a type defined by \vsmc, but a user defined class that
abstract the collection of values $\{X^{(i)}\}_{i=1}^N$. The library allows
much flexibility in the definition of this type. The important thing to note
here is that this class need to at least abstract the whole collection of all
values instead of a single particle. In section~\ref{sec:The value collection,
  the particles and the sampler}, we introduce a readily usable implementation
provided by the \vsmc library, on top of which users can build application
specific classes.

Most of core classes in the library are class templates with this value
collection type as their template parameter. In the following, we use the
generic name \cppinline{T} to denote this value collection type.

\paragraph{Sampler}

A \cppinline{Sampler<T>} object is used to execute various operations of an
\smc algorithm. It is used to initialize the particles and update them. It is
also used to perform resampling and importance sampling approximation. In the
body of a program, this is the usually only class that the user need to
interact with. In section~\ref{sub:Program structure}, we show how each step
of a generic \smc algorithm is mapped to the operations provided by a
\cppinline{Sampler<T>} object.

\paragraph{Particle}

A \cppinline{Sampler<T>} object contains, among other things, an object called
\cppinline{Particle<T>} that abstracts the particle system. A particle system
is formed by both the values $\{X^{(i)}\}_{i=1}^N$ and the weights
$\{W^{(i)}\}_{i=1}^N$. The former is abstracted by the user defined value
collection type \cppinline{T}. The later is abstracted by a
\cppinline{WeightSet<T>} object.

The \cppinline{Particle<T>} object also provides various methods that
manipulate the particle system, for example, it can perform the resampling
algorithm on the particle system when required by the \cppinline{Sampler<T>}
object.

\paragraph{Weights}

As said, weights in a particle system are manipulated through a sub-object of
\cppinline{Particle<T>}, \cppinline{WeightSet<T>}. In addition to common
weights manipulations, such as setting the weights directly or using the
incremental weights, it also provides ways to query properties of the
weights. For example, it can calculate the \ess and \cess values.

For most applications, the default \cppinline{WeightSet<T>} is sufficient.
However, like the value collection type, there could be special requirement of
this class. It can be replaced by user defined classes through \cpp template
metaprogramming. The details are documented in the reference manual.

\paragraph{Monitor}

Given a real-valued function $h$, the library can use \cppinline{Monitor<T>}
objects to compute the importance sampling approximation of $\Exp[h(X)]$
automatically as the sampler progresses. The function value of $h$ is allowed
to be a vector. And it is possible to use optimized linear algebra library to
accelerate the computing in that case. There are also special support for path
sampling, which requires essentially a simple importance sampling
approximation and a numerical integration.

\subsection{Program structure}
\label{sub:Program structure}

Recall the \smc algorithms discussed in section~\ref{sec:Sequential Monte
  Carlo samplers}, regardless of specific applications or algorithm settings,
in practice they can be dissembled into the following steps.
\begin{enumerate}
  \item Initialize values $\{X^{(i)}\}_{i=1}^N$ and weights
    $\{W^{(i)}\}_{i=1}^N$.
  \item For $t = 1,\dots,T$, where $T$ may not be finite (for example, a
    particle filter processing incoming data), repeat
    \begin{enumerate}
      \item Update either values $\{X^{(i)}\}_{i=1}^N$ or weights
        $\{W^{(i)}\}_{i=1}^N$ or both.
      \item Resampling.
      \item Update either values $\{X^{(i)}\}_{i=1}^N$ or weights
        $\{W^{(i)}\}_{i=1}^N$ or both.
    \end{enumerate}
\end{enumerate}
Note that steps 2.(a)-(c) are all optional, though it is unlikely that all
three of them are absent. For example, an \ais algorithm does not have the
resampling step. An \smc algorithm such as algorithm~\ref{alg:smc2} only
update the weights before the possible resampling and only update the values
after it while a particle filter might update both the values and weights at
step 2.(a). Both step 2.(a) and 2.(c) may be formed by a few sub-steps. For
example, the Markov kernel may be formed by a multi-block Metropolis random
walk.

In addition, after each iteration of step 2, we may be interested to evaluate
some importance sampling estimates. For example, the path sampling estimator,
as seen in section~\ref{sub:Path Sampling via smc2/smc3}, requires the
importance sampling estimates of $\diff\log q_{\alpha}(X)/\diff\alpha$ where
$q_{\alpha}$ is the unnormalized density function of the family of
distributions that the \smc sampler operates on. Another example is particle
filters, which often requires estimates of certain parameters at each
iteration.

For demonstration purpose, let's assume that our program has all those steps
and need to calculate both the path sampling and other importance sampling
estimates. In the \vsmc library, all these tasks are performed through the
\cppinline{Sampler} class. Below is an example of such a program,
\cppfile{snippet/program_structure.cpp}
We will explain each line of this program in detail. For now, it is sufficient
to point out that the following objects used in this program are user defined
callback that implement application specific operations.
\begin{description}
  \item[\cppinline{init_f}] Initialize the particle values.
    (section~\ref{sec:Initializing particles and parallelization})
  \item[\cppinline{move_f}] Update the particles. For example, updating the
    weights. These updates are performed before the possible resampling.
    (section~\ref{sec:Updating particles})
  \item[\cppinline{mcmc_f1} and \cppinline{mcmc_f2}] Update the particles. For
    example, moving the particles with an \mcmc kernel. These updates are
    performed after the possible resampling. (section~\ref{sec:Updating
      particles})
  \item[\cppinline{path_eval}] Evaluate the value of path sampling integrands,
    $\diff\log q_{\alpha}(X)/\diff\alpha$. (section~\ref{sec:Monitoring})
  \item[\cppinline{moments_eval}] Evaluate importance sampling estimate
    integrands, for example moments. (section~\ref{sec:Monitoring})
\end{description}

\section{The value collection, the particle system and the sampler}
\label{sec:The value collection, the particle system and the sampler}

At the core of each implementation of \smc algorithms using the \vsmc library
is the definition of the value collection type that abstracts
$\{X^{(i)}\}_{i=1}^N$. \vsmc does not restrict how the values shall be
actually stored. They can be stored in memory, spread among nodes of a
cluster, in \gpu memory or even in a database. Users can define their own
value collection type to fulfill various application specific needs. For full
details on the requirement of the value collection type, see \cite{vsmcjss}.

Given a value collection type \cppinline{T}, one can construct a sampler,
\begin{cppcode}
Sampler<T> sampler(N, Stratified, 0.5);
\end{cppcode}
The first argument is the number of particles. The second is the resampling
methods. There are six built-in resampling schemes in the library. And user
defined resampling algorithms can also be used. See the reference manual for
details. The last argument is the threshold of $\ess/N$ at each iteration,
below which a resampling will be performed. The later two parameters are
optional.

A \cppinline{Sampler<T>} object has a sub-object, \cppinline{Particle<T>},
which contains the type \cppinline{T} object along with other data such as
weights. Each can be access as the following,
\begin{cppcode}
Sampler<T> sampler(N);
sampler.particle();         // Reference to Particle<T> object
sampler.particle().value(); // Reference to type T object
\end{cppcode}

\subsection{A matrix of state values}
\label{sub:A matrix of state values}

Many typical problems' value collections can be viewed as a matrix of certain
type. For example, a simple particle filter whose state is a real-valued
vector of length $M$ can be viewed as an $N$ by $M$ matrix of type
\cppinline{double} where $N$ is the number of particles. A trans-dimensional
problem (e.g., \cite{Jasra:2008bb}) can use an $N$ by $1$ matrix whose type is
a user defined class, say \cppinline{StateType}. For this kind of problems,
a class template is provided by the library,
\begin{cppcode}
template <MatrixOrder Order, std::size_t Dim, typename StateType>
class StateMatrix;
\end{cppcode}
The first template parameter (possible value \cppinline{RowMajor} or
\cppinline{ColMajor}) specifies how the values are ordered in memory. Usually
one shall choose \cppinline{RowMajor} to optimize data access. The second
template parameter is the number of variables, an integer value no less than
$1$ or the special value \cppinline{Dynamic}, in which case
\cppinline{StateMatrix} provides a member function \cppinline{resize_dim} such
that the number of variables can be changed at runtime. The third template
parameter is the type of the state values. Each particle's state is thus a
vector of length \cppinline{Dim}, indexed from \cppinline{0} to
\cppinline{Dim - 1}. To obtain the value at position \cppinline{j} of the
vector of particle \cppinline{i} (the element at the \cppinline{i}th raw and
\cppinline{j}th column of the matrix),
\begin{cppcode}
StateBase<RowMajor, Dim, StateType> value(N);
StateType val = value.state(i, j);
\end{cppcode}
There are other ways to obtain and manipulate the values, see the reference
manual for details. Note that, one can derive from the \cppinline{StateMatrix}
class to extend its functionality, as we will see in examples later.

\subsection{A single particle}
\label{sub:A single particle}

If the value collection type \cppinline{T} satisfies certain
requirements\footnote{See the reference manual for technique details. It is
  sufficient to note here that \cppinline{StateMatrix} and any of its derived
  classes satisfy those requirements.}, then for a \cppinline{Particle<T>}
object, one can construct a \cppinline{SingleParticle<T>} object that
abstracts one of the particle from the collection. For example,
\begin{cppcode}
Particle<T> particle(N);
SingleParticle<T> sp(i, &particle);
\end{cppcode}
create a \cppinline{SingleParticle<T>} object corresponding to the particle
\cppinline{i}. There are a few member functions of
\cppinline{SingleParticle<T>} that make access to individual particles easier
than through the interface of \cppinline{Particle<T>}. For example, for each
particle, a \cppinline{Particle<T>} object construct an indepent \cppoo{} \rng
engine. For example, the following uses it to generate standard Normal random
variates,
\begin{cppcode}
std::normal_distribution<double> rnorm(0, 1);
std::vector<double> z(particle.size());
for (std::size_t i = 0; i != particle.size(); ++i)
    z[i] = rnorm(particle.rng(i));
\end{cppcode}
If we access each particle through \cppinline{SingleParticle<T>}, then we can
write
\begin{cppcode}
z[i] = rnorm(sp.rng());
\end{cppcode}
Here \cppinline{sp.rng()} is equivalent to \cppinline{particle.rng(i)}.

The functionality of a \cppinline{SingleParticle<T>} can be enhanced through
template metaprogramming. For instance, if \cppinline{T} is
\cppinline{StateMatrix} or its derived class, then \cppinline{sp.state(j)} is
equivalent to \cppinline{particle.value().state(i, j)}

\subsection{Example: The value collection of \protect\gmm}
\label{sub:Example: The value collection of gmm}

The \cppinline{StateMatrix} is a minimalistic class template. Users can derive
from it and build application specific value collection classes. Here we
demonstrate how the value collection, named \cppinline{GMM}, in the \smc[2]
algorithm for the Gaussian mixture model (\gmm; see section~\ref{sub:Gaussian
  mixture model}) is designed.

Recall that, a \gmm with $r$ components has a parameter vector of length $3r$,
$\theta_r = (\mu_{1:r}, \lambda_{1:r}, \omega_{1:r})$. In the \smc[2]
algorithm, we use the sequence of distributions $\{\pi_t\}_{t=0}^T$ taking the
form,
\begin{equation*}
  \pi_t(\theta_t) =
  \pi_0(\theta_t|\calM) p(\data|\theta_t,\calM)^{\alpha(t/T)}.
\end{equation*}
The \cppinline{GMM} class will abstract the \gmm in addition to the state of
all particle values at any given generation. Therefore, we have the following
design goals for this class,
\begin{enumerate}
  \item The data, which is associated with the model shall be stored in and
    can be accessed through this class.
  \item The calculation of the likelihood and the prior densities, which are
    characteristics of the model shall be possible through this class.
  \item The distribution specification parameter $\alpha$ and the \mcmc
    proposal scales, which are properties of a given generation of the
    particle system shall be associated with this class.
\end{enumerate}
This class is outlined as below.
\cppfile{snippet/gmm_value.cpp}
First the number of components are set through a template parameter
\cppinline{R}. Of course, it is possible to make this parameter dynamic and
changeable at runtime by using \cppinline{Dynamic} for the second template
parameter of \cppinline{StateMatrix}.

Second, the static member functions \cppinline{mu_idx}, etc., returns the
index of the $\mu_j$, etc., in each row of the \cppinline{StateMatrix}. For
example, to access $\lambda_j$ of the $i$th particle, we can use
\begin{cppcode}
particle.value().state(i, GMM<R>::lambda_idx(j));
\end{cppcode}
or with the \cppinline{SingleParticle} interface
\begin{cppcode}
sp.state(GMM<R>::lambda_idx(j));
\end{cppcode}
instead of the much more difficult to read expression,
\begin{cppcode}
sp.state(GMM<R>::ComponentNumber * 2 + j);
\end{cppcode}
It is trivial to see that the parameters are arranged as if in such a matrix,
\begin{equation*}
  \begin{pmatrix}
    \theta_r^{(1)} \\ \vdots \\ \theta_r^{(N)}
  \end{pmatrix} =
  \begin{pmatrix}
    \mu_1^{(1)},\dots,\mu_r^{(1)}, &
    \lambda_1^{(1)},\dots,\lambda_r^{(1)}, &
    \omega_1^{(1)},\dots,\omega_r^{(1)} \\
    \vdots & \vdots & \vdots \\
    \mu_1^{(N)},\dots,\mu_r^{(N)}, &
    \lambda_1^{(N)},\dots,\lambda_r^{(N)}, &
    \omega_1^{(N)},\dots,\omega_r^{(N)}
  \end{pmatrix}
\end{equation*}

Third, the setter and getter member functions such as \cppinline{mu_scale}
provide access to the proposal scales. In addition, the member function
\cppinline{alpha} will provide access to $\alpha(t/T)$.

Fourth, the \cppinline{read_data} member function, whose definition is omitted
here, provides a way to read data into the \cppinline{data_} member data.

And last, the \cppinline{log_likelihood} and \cppinline{log_prior} member
functions calculate the log-likelihood and log-prior densities for a given
particle. They accept the particle's index number as input. The actual
implementations of these functions, distributed with the library, use more
sophisticated data structures to ensure that the computation only occurs when
the parameter values are changed. From a user's perspective, one only need to
know that these member functions will return the value of likelihood and prior
densities for the current particle values when called, while the actual
computation may or may not happen when the functions are called.

\section{Initializing particles and parallelization}
\label{sec:Initializing particles and parallelization}

The particles are initialized by a user defined callback. The callable object
has the following signature,
\cppfile{snippet/init_type.cpp}
It is added to the sampler by
\begin{cppcode}
sampler.init(init_f);
\end{cppcode}
And it will be called when the following in the program
(section~\ref{sub:Program structure}) is executed,
\begin{cppcode}
sampler.initialize(param);
\end{cppcode}
where the input parameter \cppinline{param} is optional and the default is
\cppinline{NULL}. It will be passed on as the second argument of
\cppinline{init_f} with \cppinline{sampler.particle()} being the first. The
return value of \cppinline{init_f} will be recorded as the acceptance count
and can be later retrieved by,
\begin{cppcode}
sampler.accept_history(0, 0);
\end{cppcode}
The optional parameter can be used to provide additional information needed to
initialize the sampler.

If the user does not do anything special, the sampler will also initialize the
weights $\{W^{(i)}\}_{i=1}^N$ to be equal and normalized to $1/N$. In
addition, any information recorded for previous generations of the particle
system will be erased during the initialization.

\subsection{Example: Simulation of Normal distribution}
\label{sub:Example: Simulation of Normal distribution}

We show here a very simple example, simulation of Normal random variables.
And we will introduce an important feature of the library through it --
parallelization.

Suppose for an \smc algorithm with a parameter vector of length $k$, we want to
initialize each of the parameter to $\calN(\mu,\sigma^2)$, where $\calN$
denotes the Normal distribution. We can implement it as the following,
\cppfile{snippet/init_normal.cpp}
In this example, we used the second parameter \cppinline{param} to pass
informations about the Normal distribution, its mean and standard deviation.
In the body of the program (the \cppinline{main} function), we can use it as
following,
\begin{cppcode}
sampler.init(init_f);

Param param = {Mean, Sd};
smapler.initialize(param);
\end{cppcode}

\subsection{Parallelized implementation}
\label{sub:Parallelized implementation}

In the above example, we looped over all particles. The inner loop is repeated
for each particle. There are no data dependencies among particles in this
operation. It is perfectly reasonable to have the outer loop parallelized.

This kind of parallelization, not only for initializing particles, but also
for updating particles, are supported in the \vsmc library through a set of
class templates. Here we introduce the ones specific to initialization,
\begin{cppcode}
template <typename T, typename D = Virtual> class InitializeSEQ;
template <typename T, typename D = Virtual> class InitializeOMP;
template <typename T, typename D = Virtual> class InitializeTBB;
\end{cppcode}
Each of the above three implement sequential, \openmp parallelization and \tbb
parallelization, respectively. There are a few other similar classes for other
parallel programming models not listed here. We first use
\cppinline{InitializeSEQ} as an example to demonstrate how it is used. The
interface of \cppinline{InitializeSEQ} given the second template parameter
being \cppinline{Virtual}, which is also shared by others, is,
\cppfile{snippet/initialize_seq_interface.cpp}
The existence of the \emph{non-virtual} interface \cppinline{operator()} and
the form of its signature ensures that an object of its derived class can be
used just as \cppinline{init_f}. It is implemented as if,
\cppfile{snippet/initialize_seq_operator.cpp}
Different class templates listed above differ at how they implement the loop.
For example, \cppinline{InitializeOMP} uses \openmp to parallelize this loop.

The user can derive from this class and use the virtual functions to provide
application specific behaviors of this operator. For example, the simulation
of Normal random variates can now be re-implemented as,
\cppfile{snippet/init_normal_seq.cpp}
At a first glance, it took quite a few more lines than the original
implementation of \cppinline{init_f}. However, by replace
\cppinline{InitializeSEQ} with \cppinline{InitializeOMP}, without changing
anything else, the sampler will be using \openmp for parallelization during
the initialization step.

There are also other benefits of this implementation. First, if \openmp is not
available in the user's \cpp environment (e.g., using the popular \clang
\cite{clang} compiler), one can use the same implementation with other
parallel programming models. For instance, to use \tbb instead of \openmp,
only \cppinline{InitializeOMP} need to be changed to
\cppinline{InitializeTBB}.

Second, this implementation is also scalable. A few changes allows it to use
\mpi for parallelization on distributed memory computers. All that need to be
done is wrap the value collection type with the adapter class
\cppinline{StateMPI},
\begin{cppcode}
typedef StateMPI<StateMatrix<RawMajor, K, double> > T
\end{cppcode}

In summary, with almost identical implementation, we can build programs
running on single threaded sequential mode, on multicore processors with
various parallel programming models or on a distributed memory computer with
\mpi.

\section{Updating particles}
\label{sec:Updating particles}

The addition of methods that update the particles is more flexible than
initialization. There are two kinds of updating methods. One is simply called
\cppinline{move} in \vsmc, and is performed before the possible resampling at
each iteration. These moves usually perform the updating of the weights among
other tasks. The other is called \cppinline{mcmc}, and is performed after the
possible resampling. They are often \mcmc type moves. Multiple
\cppinline{move}'s or \cppinline{mcmc}'s are also allowed. In fact a \vsmc
sampler consists of a queue of \cppinline{move}'s and a queue of
\cppinline{mcmc}'s.

All these are implemented using user defined callback similar to the
\cppinline{init_f} function in the last section, with a slight different
signature,
\cppfile{snippet/move_type.cpp}
This is the same for both \cppinline{move}'s and \cppinline{mcmc}'s. The first
argument is the iteration number, counting from zero for the initialization
step. The second argument is passed by the sampler, which is
\cppinline{sampler.particle()}.

To add \cppinline{move_f} into the queue of \cppinline{move}'s, call
\begin{cppcode}
sampler.move(move_f, false);
\end{cppcode}
The second argument, a boolean value, indicate whether the new
\cppinline{move} shall be appended to the existing (possible empty) queue (if
it is set to \cppinline{false}); or the queue shall be cleared before set a
new one. The queue of the \cppinline{mcmc}'s is manipulated similar.

\subsection{Example: Updating the weights in the \smc[2] algorithm}
\label{sub:Example: Updating the weights in the SMC2 algorithm}

Recall algorithm~\ref{alg:smc2}, the updating of weights shall be performed
before possible resampling at each iteration. And the change of the weights
are calculated with the incremental weights,
\begin{align*}
  W_t^{(i)} &\propto W_{t-1}^{(i)} w_t(X_{t-1}^{(i)}, X_t^{(i)}) \\
  w_t(X_{t-1}^{(i)}, X_t^{(i)}) &=
  p(\data|\theta_{t-1}^{(i)},\calM)^{\alpha(t/T) - \alpha([t-1]/T)}.
\end{align*}
This is quite generic for different applications. All we need here is the
calculation of the log-likelihood function. It is natural to write a function
template for it,
\cppfile{snippet/smc_move.cpp}
The assumption about the value collection type \cppinline{T} is,
\begin{enumerate}
  \item It provide access to $\alpha(t/T)$ in the same way as the
    \cppinline{GMM} class in section~\ref{sub:Example: The value collection of
      gmm}.
  \item It provide access to the log-likelihood in the same way as the
    \cppinline{GMM} class.
\end{enumerate}
The first part of the function template \cppinline{smc_move} calculates
$\alpha(t/T) - \alpha([t-1]/T)$. The second part calculates the logarithm of
the incremental weights for each particle. The last part manipulates the
weights.

Weights are manipulated through a object of type \cppinline{WeightSet}. There
are other ways to manipulate them,
\begin{cppcode}
std::vector<double> weight(particle.size());
particle.weight_set().set_equal_weight();
particle.weight_set().set_weight(weight.begin());
particle.weight_set().mul_weight(weight.begin());
particle.weight_set().set_log_weight(weight.begin());
particle.weight_set().add_log_weight(weight.begin());
\end{cppcode}
The \cppinline{set_equal_weight} member function sets all weights to be equal
i.e., $1/N$. The \cppinline{set_weight} and \cppinline{set_log_weight} member
functions set the values of weights and logarithm weights, respectively. The
\cppinline{mul_weight} and \cppinline{add_log_weight} member functions
multiply the weights or add to the logarithm weights by the given values,
respectively. All these member functions accept general input iterators as
their arguments.

\subsection{Example: The \protect\mcmc move in \protect\gmm}
\label{sub:Example: The mcmc move in gmm}

In this example, we show an implementation of the \mcmc moves in the \gmm
example (section~\ref{sub:Gaussian mixture model}). We will only detail the
implementation of random walk block on $\mu_{1:r}$. The others are similar.
Recall that, we perform a Normal random walk on the mean parameters. The \mcmc
algorithm's implementation can be summarized as the following steps,
\begin{enumerate}
  \item Calculate the value of the target density for the parameter values,
    say $f$.
  \item Propose new values according to the proposal distribution. In our
    implementation, this proposal step are carried in place, meaning that the
    particle values are updated when new values are proposed.
  \item Calculate the value of the target density for the proposed parameter
    values, say $f'$.
  \item Generate uniform random variate on the $[0,1]$ interval, say $u$,
  \item Accept the proposed values if $u < f'/f$. Otherwise, restore the old
    values.
\end{enumerate}
The implementation of these five steps are straightforward,
\cppfile{snippet/gmm_mcmc_mu.cpp}
First, we derived our class from a class template called \cppinline{MoveOMP}.
It is similar to the \cppinline{InitializeSEQ} class template introduced in
section~\ref{sub:Parallelized implementation}. It provides \openmp
parallelization.

Second, we used a few new features of the \cppinline{SingleParticle} class
template. Recall that, it is created from a reference to a
\cppinline{Particle<T>} object and an index of the individual particle. The
\cppinline{Particle<T>} object can be obtained, by a constant reference,
through
\begin{cppcode}
sp.particle();
\end{cppcode}
and the index can be obtained through
\begin{cppcode}
sp.id();
\end{cppcode}
These are used in the calculation of the log-likelihood and log-prior
densities.

Otherwise, the implementation is a straightforward translation of the
mathematical representation of the algorithm. The whole algorithm has three
blocks of random walks. Say we implemented the other two similarly as
\cppinline{GMM_MCMC_Lamba} and \cppinline{GMM_MCMCM_Omega}, then in the body
of the program we can add them to the sampler by,
\cppfile{snippet/gmm_add_mcmc.cpp}
Note that the \cppinline{mcmc} member function call return a reference the
sampler itself. Therefore we can chain these calls. When we call,
\begin{cppcode}
sampler.iterate(IterNum);
\end{cppcode}
the sampler will iterate \cppinline{IterNum} steps and at each step, all three
of these random walks will be applied to the particle system.

\section{Monitoring}
\label{sec:Monitoring}

Before initializing the sampler or after a certain time point, one can add
monitors to the sampler. The concept is similar to \bugs's \cppinline{monitor}
statement, except it does not monitor the individual values but rather the
importance sampling estimates. Consider approximating $\Exp[h(X)]$, where
$h(X) = (h_1(X),\dots,h_m(X))$ is an $m$-vector function. The importance
sampling estimate can be obtained by $AW$ where $A$ is an $N$ by $m$ matrix
where $A(i,j) = h_j(X^{(i)})$ and $W = (W^{(i)},\dots,W^{(N)})^T$ is the
$N$-vector of normalized weights. To compute this importance sampling
estimate, one need to define the following evaluation function (or other kinds
of callable objects),
\cppfile{snippet/monitor_eval.cpp}
and add it to the sampler by calling,
\begin{cppcode}
sampler.monitor("variable.name", m, monitor_eval);
\end{cppcode}
When the function \cppinline{monitor_eval} is called, \cppinline{iter} is the
iteration number of the sampler, \cppinline{m} is the same value as the one
the user passed to \cppinline{Sampler<T>::monitor}; and thus one does not need
global variable or other similar techniques to access this value. The output
pointer \cppinline{res} points to an $N \times m$ output array of row major
order. That is, after the calling of the function,
\cppinline{res[i * dim + j]} shall be $h_j(X^{(i)})$.

Implementation of the path sampling estimator (section~\ref{sub:Path Sampling
  via smc2/smc3}) can be viewed as a special kind of monitor. In addition to
the evaluation of $h(X^{(i)})$, where $h$ is $\diff\log
q_{\alpha}(X)/\diff\alpha$ in this special case, the interval length of the
numerical integration as in equation~\eqref{eq:path_est} also need to be
obtained. The \vsmc library provide a special support for path sampling. First
one need to define a function, say \cppinline{path_eval} with the following
signature,
\cppfile{snippet/path_eval.cpp}
It is not unlike the \cppinline{monitor_eval} function above. It only differs
by,
\begin{enumerate}
  \item The output array \cppinline{res} is always of length $N$, and thus
    there is no argument to pass the dimension of the monitor
  \item It returns a value, which should be $\alpha_t$.
\end{enumerate}
To use it, one can add it to the sampler by,
\begin{cppcode}
sampler.path_sampling(path_eval);
\end{cppcode}
And the estimate can be obtained by
\begin{cppcode}
sampler.path_sampling();
\end{cppcode}

\subsection{Example: Path sampling with the \smc[2] algorithm}
\label{sub:Example: Path sampling with the SMC2 algorithm}

In the case of algorithm~\ref{alg:smc2}, the path sampling integrands is
simply the log-likelihood, and $\alpha_t = \alpha(t/T)$. Therefore the
implementation of a generic \cppinline{path_eval} is straightforward. Again,
we assume that value collection type $T$ provides access to $\alpha(t/T)$ and
the log-likelihood in the same way as the \cppinline{GMM} class. We can
implement the function template as the following,
\cppfile{snippet/path_eval_f.cpp}

Other \smc algorithms such algorithm~\ref{alg:smc3} may have different path
sampling estimator expression. But the implementation is similar.

\subsection{Example: Adaptive specification of proposal scales}
\label{sub:Example: Adaptive specification of proposal scales}

Now we demonstrate the implementation of a slightly more complex monitor and
use it for the adaptive specification of proposal scales. Consider the \gmm
example, as outlined in section~\ref{sub:Adaptive specification of proposals},
we can use the moments of the parameters to set the proposal scales
adaptively.

First, we need to create a monitor that record the first two raw moments of
each parameter. We can make this problem more general as estimating the first
$M$ raw moments. We use a parallelized monitor, \cppinline{MonitorEvalOMP} for
the implementation of the evaluation function. It is not unlike the
\cppinline{MoveOMP} class template introduced earlier. The main difference is
that now we need to define the following member function,
\begin{cppcode}
void monitor_state(std::size_t, std::size_t,
        ConstSingleParticle<GMM<R> > csp, double *res)
\end{cppcode}
where the first argument is the iteration number and the second is the
dimension of the monitor. The third, a \cppinline{ConstSingleParticle} object,
is similar to \cppinline{SingleParticle} except that now one does not have
write access to the particles. In other words, one cannot change the particle
values through it. The last, the output parameter \cppinline{res} is of length
the dimension of the monitor. The function call need only to store the values
of $h(X^{(i)})$ for a single particle. The complete implementation is given
as below,
\cppfile{snippet/gmm_moments.cpp}
In addition to the \cppinline{monitor_state} member function, we also provide
a few utilities in this class. First, similar to the \cppinline{GMM} class, we
use functions to give the index of a given order of moment for a specific
parameter of a certain components inside the output parameter \cppinline{res}.
This makes the implementation more readable. Second, we also provide
functions, whose definitions are trivial and not shown here, that calculate
the proposal scales for each random walk given an array of moments estimates.
We can add this monitor to the sampler by,
\cppfile{snippet/gmm_moments_add.cpp}
In the above, we choose only to estimate the first two raw moments.

Now we only need a method to set the proposal scales using this monitor. Since
this should be set before the updating of weights and possible resampling, we
can construct a \cppinline{move} for this job.
\cppfile{snippet/gmm_adaptive_scale.cpp}
And in the \cppinline{main} function, we change the move queue to
\begin{cppcode}
sampler.move(GMM_AdaptiveScale(&sampler), false);
sampler.move(smc_move<GMM<R> >, true);
\end{cppcode}
Note that, we initialize the move with a pointer of the sampler itself, and
use this pointer to access the record in the monitor named
\cppinline{"moments"}. There are many ways to retrieve the importance sampling
estimates from a monitor. The one we used here
\begin{cppcode}
res[i] = sampler_ptr_->monitor("moments").record(j);
\end{cppcode}
return the importance sampling estimate of $\Exp[h_j(X)]$ for the latest
generation of the particle system.

\section{Performance of parallelization}
\label{sec:Performance of parallelization}

One of the main motivation behind the creation of \vsmc is to ease the
parallelization with different programming models. The same implementation can
be used to built different samplers based on what kind of parallel programming
model is supported on the users' platforms. In this section we compare the
performance of various \smp parallel programming models and \opencl
parallelization. We use the \gmm with \smc[2] algorithm as shown in
section~\ref{sub:Gaussian mixture model}. Many major parts of its
implementation have been shown through this chapter. For a complete
documentation on its implementation with \vsmc, see \cite{vsmcjss}.

\subsection{Using the \protect\smp module}
\label{sub:Using the SMP module}

We consider five different implementations supported by \icpc~2013:
sequential, \tbb, \cilk, \openmp and \cppoo{} \cppinline{<thread>}. The
program is built and run on a Ubuntu~12.10 workstation with an Xeon~W3550
(3.06GHz, 4 cores, 8 hardware threads through hyper-threading) \cpu. A four
components model and $100$ iterations with a prior annealing scheme is used
for all implementations. A range of numbers of particles are tested, from
$2^3$ to $2^{17}$.

For different number of particles, the wall clock time and speedup are shown
in Figure~\ref{fig:bench-smp-perf}. For $10^4$ or more particles, the
differences are minimal among all the programming models. They all have
roughly 550\% speedup. With smaller number of particles, \vsmc's \cppoo
parallelization is less efficient than other industry strength programming
models. However, with $1000$ or more particles, which is less than typical
applications, the difference is not very significant.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{fig/bench-smp-time-running}
  \includegraphics[width=\linewidth]{fig/bench-smp-speedup-running}
  \caption{Performance of \cpp implementations of Bayesian modeling for
    Gaussian mixture model (Linux; Xeon W3550, 3.06GHz, 4 cores, 8 threads).}
  \label{fig:bench-smp-perf}
\end{figure}

\subsection{Using the \protect\opencl module}
\label{sub:Using the OpenCL module}

The implementation of the same algorithm using \opencl is quite similar to
those using the \smp module.

\opencl implementations are also compared on the same workstation, which also
has an NVIDIA Quadro 2000 graphic card. \opencl programs can be compiled to
run on both \cpu and \gpu. For \cpu implementation, there are \iocl
\cite{iocl} and \aocl \cite{aocl} platforms. We use the \tbb implementation as
a baseline for comparison. The same \opencl implementation are used for all
the \cpu and \gpu runtimes.  Therefore they are not particularly optimized for
any of them. For the \gpu implementation, in addition to double precision, we
also tested a single precision configuration. Unlike modern \cpu, which have
the same performance for double and single precision floating point operations
(unless \simd instructions are used, which can have at most a speedup by a
factor of 2), \gpu penalize double precision performance heavily.

For different number of particles, the wall clock time and speed up are
plotted in Figure~\ref{fig:bench-ocl-perf}. With smaller number of particles,
the \opencl implementations have a high overhead when compared to the \tbb
implementation. With a large number of particles, \aocl has a similar
performance as the \tbb implementation. \iocl is about 40\% faster than the
\tbb implementation. This is due to more efficient vectorization and compiler
optimizations. The double precision performance of the NVIDIA \gpu has a 220\%
speedup and the single precision performance has near 1600\% speedup. As a
rough reference for the expected performance gain, the \cpu has a theoretical
peak performance of 24.48 GFLOPS. The \gpu has a theoretical peak performance
of 60 GFLOPS in double precision and 480 GFLOPS in single precision. This
represents 245\% and 1960\% speedup compared to the \cpu, respectively.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{fig/bench-ocl-time-running}
  \includegraphics[width=\linewidth]{fig/bench-ocl-speedup-running}
  \caption{Performance of \opencl implementations of Bayesian modeling for
    Gaussian mixture model (Linux; Xeon W3550 \gpu, 3.06GHz, 4 cores, 8
    threads; NVIDIA Quadro 2000).}
  \label{fig:bench-ocl-perf}
\end{figure}

\subsection{Performance and productivity}
\label{sub:Performance and productivity}

Performance along is not enough for a software to be useful. The productivity,
the efforts need to develop new algorithms, should also be taken into
considerations. Due the low level natural of \cpp, it certain takes more
efforts to develop an algorithm using \vsmc than, say \libbi or \biips.
However, \smc does provide a some advantages. First, some application of \smc
algorithms may not fit into the framework of those softwares. The framework of
\smc is general enough for them to be implemented with relative ease.

Second, one can choose various parallel programming models while using the
same implementation, as we have seen in section~\ref{sub:Parallelized
  implementation}. This can be particularly useful in a few scenarios,
\begin{enumerate}
  \item Many parallel programming models do not coexist in the same program
    well. Some of them such as \tbb and \cilk has explicit support for
    \openmp. Much less so can be said for others. Often, some other part of
    the program may also be parallelized with a particular programming model
    familiar to the user. In this case, using \vsmc one can often freely
    choose the same one for the \smc algorithm's parallelization. See
    \cite{vsmcjss} for all the programming models supported by the library.
  \item Often an algorithm is first developed on a desktop or laptop with only
    multicore processors. Later it may be deployed to larger computers to
    process bigger data. With \vsmc it is possible to use the implementation
    on a \smp system, with little modifications, for the larger computer. In
    \cite{vsmcjss} there is a full fledged example showing the use \mpi.
\end{enumerate}

Overall, we found the productivity of \vsmc is at a similar level of \smctc.
For example, the particle filter example in \cite{smctc} can be implemented in
\vsmc using roughly the same number of lines of code. Consider that there is a
significant performance gain through parallelization, as seen in
section~\ref{sub:Using the SMP module}, we believe the effort of using a \cpp
library is adequate.

The library also support \opencl parallelization. The performance is
impressive as seen in section~\ref{sub:Using the OpenCL module}. It is widely
believed that \opencl programming is tedious and hard. Limited by the scope of
this paper, the \opencl implementation (distributed with the \vsmc source) is
not documented in this chapter. Overall the \opencl implementation has about
800 lines including both host and device code. It is not an enormous increase
in effort when compared to the 500 lines \smp implementation. Less than
doubling the code base but gaining more than 15 times performance speedup, we
consider the programming effort is relatively small. Moreover, the \gpu used
in the examples is relatively lower end and outdated. With better hardware,
the same implementation has the potential to gain hundreds of times
performance speedup.

In addition, the \opencl language is essentially a variant of the C
programming language. For the intended users of \vsmc, those with some
knowledge of \cpp, writing \opencl kernels (the part of the program that are
executed on the device, such as \gpu{}s) is not a difficult task. What often
makes \opencl programming difficult is the management of the devices. It
involves the understanding an array of layers of the underlying hardware.
There are examples\footnote{For example,
  \url{https://developer.apple.com/library/mac/samplecode/OpenCL_FFT/Introduction/Intro.html}}
where a major part of the program is irrelevant to the algorithm itself. This
is not the case of using \vsmc. The library provides facilities to manage
\opencl platforms and devices as well as common operations. The
implementations of \smc algorithms using \opencl, compared to using the \smp
module, only requires a marginal addition of efforts to manage the \opencl
platform.

\section{Appendix}
\label{sec:vSMC Appendix}

\subsection{Classes of parallel computers}
\label{sub:Classes of parallel computers}

There are a few types of parallel computers. Here we introduce the four types
of hardware parallelism that are most commonly seen. Parallel computers can be
nested. In a multicore \cpu, each core can perform instruction level
parallelism. On the other hand, a distributed system can be formed by multiple
multicore \cpu{}s.

\subsubsection{Instruction level}
\label{ssub:Instruction level}

Modern \cpu{}s all implement the so called \simd instructions, short for
\emph{single instruction, multiple data}. The \cpu can execute a single
instruction on different data in a single cycle. However, unlike the higher
level parallelism discussed later, \simd often has strict requirement on the
arrangement of the data. In addition, the implementation often requires using
low level assembly language or intrinsics functions.

Though \vsmc does not directly implement this level of parallelism, it can be
used by the user nonetheless. In addition, many operations within \vsmc can be
performed using libraries that are implemented with \simd parallelization,
such as \mkl. Also note that, most modern \cpp compilers perform \simd
optimizations on simple loop and some of them, such as \clang \cite{clang}
performs \simd optimizations for non-loop structures. This kind of
optimization is also called \emph{vectorization}.

\subsubsection{Multicore processors and symmetric multiprocessing}
\label{ssub:Multicore processors and symmetric multiprocessing}

In the late 1990s, computer \cpu{}s are advanced by increasing the clock
speed. However, this strategy soon hit some bottlenecks, mainly the control of
heat and power. The industry started to develop multicore processors. Each
\cpu has several cores, each running at a modest clock rate. By executing
different threads on different cores, the \cpu can process the same amount of
work with less time without increasing the clock rate.

When a computer has multiple \cpu{}s and each of them has the same speed to
access the memory, the system is often called \emph{symmetric multiprocessing}
(\smp). Most higher end workstations are \smp systems. The programming tools
are usually the same for \smp and multicore processors.

The \vsmc library support various \smp programming models. In addition, \vsmc
allows the same user implementation source code to be compiled into different
parallel samplers using different programming models.

\subsubsection{Distributed computing}
\label{ssub:Distributed computing}

Distributed computing usually refers to the form of computing where both
memory and computing processors are spread among computing nodes. It can take
different forms, such as grids and clusters. The \emph{de facto} programming
model for distributed computing is \mpi. This is also supported by \vsmc. In
addition, the library also allows easy integration of \mpi and various \smp
programming models.

\subsubsection{Massive parallel computing}
\label{ssub:Massive parallel computing}

In recent years, there is a new trend of using specialized massive parallel
devices, such as \gpu{}s for scientific computing. Modern \gpu{}s often have
hundreds or thousands co-processors. The main difference between \gpu and \cpu
is that, \cpu has more logic control units, and thus is more suited for
general programs. In contrast, \gpu are better at applying the same arithmetic
operations on a collection of data. It performs the best if each computing
unit are executing \emph{exactly the same} instructions. In addition, it is
often much more efficient if there are a large amount data to be processed.
Another significant feature of these devices is that they provide much higher
\emph{local} memory bandwidth and can use various technologies to reduce local
data latency than traditional \cpu.

Massive parallel computing is extremely suitable for the \smc algorithms,
which can have a large number of particles, while each of then need to be
updated using the same \mcmc kernel.

There are two major programming models for general purpose \gpu programming
(\gpgpu), Nvidia's \cuda framework and the \opencl \cite{opencl} standard. The
\vsmc library provides direct support for the \opencl programming model.

\subsection{Parallel patterns}
\label{sub:Parallel patterns}

In a more micro level, parallelism can be implemented with different patterns.
The term \emph{pattern} in computer science, introduced and popularized by
\cite{software:GoF}, is a way of codifying best practices for software
engineering. We found patterns are more useful to statisticians for reasoning
the parallel structure of a given algorithm. This section is not an exhaustive
discussion of parallel patterns. Instead, we choose some of the most commonly
seen in practice, in particular those relevant to Monte Carlo algorithms.

\subsubsection{Map}
\label{ssub:Map}

This is perhaps the simplest form of parallelism. A function, called
\emph{elemental function}, is replicated for each element of a data collection
concurrently. The elemental function must have no side-effects in order for
the map to be implementable in parallel while achieving deterministic results.
In particular, it cannot modify global data that other instances of that
function depend on.

In \smc algorithm, the updating of particle values is clearly implementable
using a map pattern. The operation of the kernel $K(x_{t-1},x_t)$ depends only
on the history of the particle that it will be used to update, but not other
particles at a given generation.

\subsubsection{Fork-join}
\label{ssub:Fork-join}

This pattern lets control flows fork into multiple parallel flows that rejoin
later. The major difference between fork-join and map is that fork-joint does
not necessarily apply the same function on different data. Instead, usually
different functions are applied to different or the same data. There are
different programming models that implement this pattern. The \openmp
\cppinline{parallel region} fork control into multiple threads that all
execute the \emph{same} statements and use other constructs to determine which
thread does what. The \cilk \cite{cilk} \cppinline{spawn} fork a new thread to
execute the calling function on a new thread and it is later joined with the
callee.

The fork-join pattern are often used by programming models to implement other
patterns and is widely used in practice itself. One example is numerical
integrations, especially for adaptive schemes. Whenever a new segment of the
integral interval is chosen, the program can fork a new thread to compute the
results. And after all segments are computed, the program can join all threads
and sum up the final result.

\subsubsection{Reduction}
\label{ssub:Reduction}

This pattern uses an associative operator to combine every element in a
collection into a single element. Given the associativity of the operator,
many different orderings are possible and hence multiple threads can be used
to parallelize the computation. This is most often used for parallelization of
computations such as summations.

For example, the computation of \ess, \cess, normalizing of weights, etc., are
all parallelized using the reduction pattern within \vsmc.

\subsubsection{Pipeline}
\label{ssub:Pipeline}

A pipeline connects tasks in a \emph{producer-consumer} relationship. A few
computation units are active at the same time. The first one consume the data,
and produce new data to be used by the second, and so on. As the data flows
into the pipeline, each unit has its own work to do and thus computations are
carried out in parallel while the data dependencies are correctly maintained.

There are several applications of pipeline in Monte Carlo computing. For
example, an \mcmc algorithm often need to compute various convergence
statistics, say $h(X_{0:t})$. Often, this statistic can be written as
$h(X_{0:t}) = h(X_t, h(X_{0:{t-1}}))$. Instead of compute it after all
iterations, one can use one thread to update the \mcmc chain and another one
to compute the statistics, using the pipeline pattern. In this case, the
Markov kernel that update the states is the \emph{producer} and the thread
that update the statistics is the \emph{consumer}.

\subsection{Modern C++}
\label{sub:Modern C++}

The \cpp programming language \cite{cpp03} was first created to support
object-oriented programming (\oop) on top of the C programming language
\cite{evolutioncpp}.  The features, such as templates, come to the language
fairly late. However, it was found that the \cpp template feature provides a
complete sub-language \cite{cpptemplateturing}. This leads to various new
metaprogramming techniques. Many of them are documented in \cite{moderncpp}
and characterize the modern usage of \cpp. In this section, we introduce two
of these techniques. They are widely used inside the \vsmc library and the
contents here shall ease the reading of the following sections for those less
familiar with them. However, we assume the reader has at least some working
knowledge of \cpp, including concepts such as \oop.

\subsubsection{Templates}
\label{ssub:Templates}

\cpp is a static strong type language. It requires the user to declare
variables, functions, and most other kinds of entities using specific types.
However, a lot of code looks the same for different types, especially for
implementation of algorithms. The \cpp template technique allows one to write
generic code to solve a class of problems, while the involved types can be
seamlessly replaced at compile time.

Templates are useful for a few reasons. It reduces duplication of the same
code for multiple types. Though conventional \oop also supports polymorphism
behaviors, they rely on runtime decisions. In contrast, templates rely on
compile time decisions and are more type safe. Templates emphasize that the
same operation can be applied to many types. It allows unlimited extension of
existing functionality. It is possible to have any combination of the allowed
operations on a certain type and the allowed types of a certain operations.
More specifically, given a collection of operations and a collection of types,
each operation may support any subset of the types and each type can support
any subset of the operations. Such combinatorial behavior is difficult to
implement using conventional \oop, where a collection of types have a common
interface that provides a fixed set of operations.

There are two main types of templates in \cpp, function template and class
template.

\paragraph{Function template}

The following lines define a simple function template,
\cppfile{snippet/function_template_def.cpp}
This template definition specifies a \emph{family} of functions that returns
the maximum of two values, which are passed as function parameters
\cppinline{a} and \cppinline{b}. The type of these parameters is left open as
\emph{template parameter} \cppinline{T}.

In this template definition, the assumption about \cppinline{T} is that the
operator \cppinline{<} is properly defined. It does not matter whether
\cppinline{T} is a fundamental type or a class type with this operator defined
by the user. The actual types are deduced at compile time when the function
template is used.

\paragraph{Class template}

Class template is similar to function template, yet somehow simpler. A class
template define a family of classes. For example,
\cppfile{snippet/class_template_def.cpp}
defines a \cppinline{Stack} class template. For simplicity, some edge cases
and exceptional situations such as calling \cppinline{top} on an empty
\cppinline{Stack} is not handled here. This class template can be used as the
following,
\cppfile{snippet/class_template_usage.cpp}
As we can see, to use a class template, one \emph{explicitly} supply the type
of the template parameter. Unlike function template, there is no template
parameter deduction here.

\subsubsection{Callable objects}
\label{ssub:Callable objects}

\vsmc is a framework for constructing generic \smc samplers. It relies on the
user to write callback functions to perform application specific operations,
such as updating particles. In this section, we introduce the few forms of
callback that are supported by the library. Collectively, they are also called
\emph{callable objects}, meaning that they support the function calling syntax
though they may not be functions.

A callable object, say \cppinline{callable}, is similar to a function in the
sense that it has a return type and a parameter list as its signature. It can
be used with the syntax,
\begin{cppcode}
callable( /* arguments */ );
\end{cppcode}
However, the object may or may not be a function. There are three ways to
define a callable objects, function pointer, functor and \cppoo lambda
expression. Function pointer is the main way of passing callback in C. The
other two are introduced later.

The library also use type erasures, introduced later in this section, to
enforce certain interfaces. The benefits of techniques introduced below
increases the productivity and flexibility of the library compared to
conventional techniques of passing callback through function pointer.

\paragraph{Functors}

Consider the simple problem, sort a vector $\{x_i\}_{i=1}^N$ according to the
values $y_i = f(x_i)$. We may define a function template to solve this
problem,
\cppfile{snippet/sort_f.cpp}
The function template \cppinline{sort_f} expects input and output as pointers.
In addition, it expects a callable object \cppinline{f}, which accepts a
variable of type \cppinline{double} as its input and return a number that can
be assigned to a variable of type \cppinline{double}.

One way to define such a callable object is to use \emph{functor}, a class
type with \cppinline{operator()} properly defined. For example,
\begin{cppcode}
struct F
{
    double operator() (double x) const { return x * x; }
};
sort_f(input, output, F());
\end{cppcode}
Here we created this object in the function call of \cppinline{sort_f}. It can
also be used as,
\begin{cppcode}
F f;
double y = f(3); // y <- 9
\end{cppcode}

\paragraph{Lambda expressions}

Another way, introduced in \cppoo, is a new feature called \emph{lambda
  expression}. It is also called \emph{local function} or \emph{closure} in
other programming languages. It allows us to define callable object \emph{on
  site}. Here is an example,
\begin{cppcode}
sort_f(input, output, [] (double x) { return x * x; });
\end{cppcode}
The full declaration of a lambda expression is as the following,
\begin{cppcode}
[ /* capture */ ] ( /* parameters */ ) /* mutable */
/* exception specification */
/* attribute specification */
-> /* return type */ { /* body * };
\end{cppcode}
The \cppinline{/* mutable */} part can be either empty or the keyword
\cppinline{mutable}, which allows the body to modify captured parameters
(explained soon). The exception specification is similar to a normal function
and the attribute specification is a new feature for all functions in \cppoo,
that specifies things like parameter passing conventions among other things,
which we will not go into details. The part \cppinline{-> /* return type */}
specifies the return type of the lambda expression. If omitted, it is deduced
from the body. And if the body does contain any \cppinline{return} statement,
it is deduced to be \cppinline{void}. If the expression takes no arguments,
the parameter list can also be omitted.

The \cppinline{/* capture */} specifies which symbols visible at the scope of
the definition of the lambda expression will be visible inside the body. There
are a few forms,
\begin{description}
  \item[\cppinline{[a, &b]}] captures \cppinline{a} by value and \cppinline{b}
    by reference.
  \item[\cppinline{[this]}] captures the \cppinline{this} pointer by value.
  \item[\cppinline{[=]}] captures all automatic variables \emph{used} in the
    body by value.
  \item[\cppinline{[&]}] captures all automatic variables \emph{used} in the
    body by reference.
  \item[\cppinline{[]}] captures nothing.
\end{description}

\paragraph{Type erasures}

In the example above, the function template \cppinline{sort_f} does not
actually enforce the signature of the function or functor, in contrast to the
definition of it that takes a function pointer as an argument. For example,
the following is perfect valid \cpp,
\begin{cppcode}
int h (int x) { return x * x };
sort_f(input, output, &h);
\end{cppcode}
while it may not be what one wants. The use of \cppinline{h} with
\cppinline{sort_f} is perhaps an typo. The type erasure in \cppoo{},
\cppinline{std::function}, provides a solution to this problem. A \emph{type
  erasure} can convert various types of objects into a single type. Below is a
basic usage of \cppinline{std::function},
\begin{cppcode}
#include <functional> // the header that defines std::function

std::function<double (double)> f;
F f_obj;
f = f_obj;  // Correct
f = &h;     // ERROR: h does not has the required signature.
\end{cppcode}
Now we can redefine the function \cppinline{sort_f} as,
\cppfile{snippet/sort_f_erasure.cpp}
The \vsmc library makes extensive use of the type erasure to enforce certain
callback interfaces. When \cppoo features are not available, the \boost
library provides the same functionality through \cppinline{boost::function}.
See \cite{vsmcjss} for details of how \vsmc choose between \cppoo and \boost
libraries.
