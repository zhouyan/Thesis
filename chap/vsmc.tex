\chapter[vSMC: A C++ Library for Parallel SMC]
{\protect\vsmc: A C++ Library for Parallel SMC}
\label{cha:vSMC: A C++ Library for Parallel SMC}

The \vsmc library was developed during the research to assist the
implementation of various \smc and other algorithms, including but not limited
to the implementation of the illustrative and performance comparison examples
in previous chapters. It evolves into a sophisticated \cpp framework for
implementing \smc algorithms on both sequential and parallel hardware.

The library makes use of some modern \cpp techniques. One shall not need to be
expert on all of them to use the library. Most of the examples in this chapter
shall be self-explanatory to readers with some basic knowledge of \cpp. For
those interested, appendix~\ref{sub:Modern C++} serve as a brief introduction
to \cpp templates and callable objects, two features used extensively in the
\vsmc library.

\section{Background}
\label{sec:vSMC Background}

\subsection{Monte Carlo computing}
\label{sub:Monte Carlo computing}

\subsection{Parallel computing}
\label{sub:Parallel computing}

\section{The vSMC library}
\label{sec:The vSMC library}

The \vsmc library makes use of \cpp's template generic programming to
implement general \smc algorithms. The library is formed by a few major
modules.
\begin{description}
  \item[Core] The highest level of abstraction of \smc samplers. Users
    interact with classes defined within this module to create and manipulate
    general \smc samplers. Classes in this module include \cppinline{Sampler},
    \cppinline{Particle} and others. These classes use user defined callback
    to perform application specific operations, such as updating particle
    values and weights.
  \item[Symmetric Multiprocessing (\smp)] This is the form of computing most
    people use everyday, including multiprocessor workstations, multicore
    desktops and laptops. Classes within this module make it possible to write
    generic operations which manipulate a single particle that can be applied
    either sequentially or in parallel through various parallel programming
    models. A method defined through classes of this module can be used by
    \cppinline{Sampler} as callback objects.
  \item[Message Passing Interface] \mpi is the \emph{de facto} standard for
    parallel programming on distributed memory architectures. This module
    enables users to adapt implementations of algorithms written for the \smp
    module such that the same sampler can be parallelized using \mpi. In
    addition, when used with the \smp module, it allows easy implementation of
    hybrid parallelization such as \mpi/\openmp.
  \item[\opencl] This module is similar to the two above except it eases the
    parallelization through \opencl, such as for the purpose of General
    Purpose \gpu Programming (\gpgpu). \opencl is a framework for writing
    programs that can be execute across heterogeneous platforms. \opencl
    programs can run on either \cpu or \gpu.
\end{description}
To obtain and install the library, see detailed instructions in
\cite{vsmcjss}, which also documents the third-party dependencies and compiler
support. A \doxygen \cite{doxygen} generated reference manual can be found at
\url{http://zhouyan.github.io/vSMC/doc/html/index.html}. It is beyond the
scope of this chapter to document every feature of the \vsmc library. In many
places we will refer to this reference manual for further information.

A more systematic tutorial of the library can be found in \cite{vsmcjss} and
the reference manual. The remaining of this chapter is structured according to
the common tasks performed by generic \smc samplers. Many features of the
library are introduced in examples. Interested readers can see the tutorial
\cite{vsmcjss} and the reference manual for details.

\subsection{Core classes}
\label{sub:Core classes}

There are over two hundred classes, large and small, in the \vsmc library. It
is beyond the scope of this chapter to document most of them. However, there
are a few of them play central role in implementation of \smc algorithms. In
this section, we provide an overview of them. Many of them are feature rich.
Instead of document their interfaces here, we will introduce useful features
through examples later.

\paragraph{Value collection type}

This is actually not a type defined by \vsmc, but a user defined class that
abstract the collection of values $\{X^{(i)}\}_{i=1}^N$. The library allows
much flexibility in the definition of this type. The important thing to note
here is that this class need to at least abstract the whole collection of all
values instead of a single particle. In section~\ref{sec:The value collection,
  the particles and the sampler}, we introduce a readily usable implementation
provided by the \vsmc library, on top of which users can build application
specific classes.

Most of core classes in the library are class templates with this value
collection type as their template parameter. In the following, we use the
generic name \cppinline{T} to denote this value collection type.

\paragraph{Sampler}

A \cppinline{Sampler<T>} object is used to execute various operations of an
\smc algorithm. It is used to initialize the particles and update them. It is
also used to perform resampling and importance sampling estimates of
parameters. In the body of a program, this is the usually only class that the
user need to interact with. In section~\ref{sub:Program structure}, we show
how each step of a generic \smc algorithm is mapped to the operations provided
by a \cppinline{Sampler<T>} object.

\paragraph{Particle}

A \cppinline{Sampler<T>} object contains, among other things, an object called
\cppinline{Particle<T>} that abstracts the particle system. A particle system
is formed by both the values $\{X^{(i)}\}_{i=1}^N$ and the weights
$\{W^{(i)}\}_{i=1}^N$. The former is abstracted by the user defined value
collection type \cppinline{T}. The later is abstracted by a
\cppinline{WeightSet<T>} object.

The \cppinline{Particle<T>} object also provides various methods that
manipulate the particle system, for example, it can perform the resampling
algorithm on the particle system when required by the \cppinline{Sampler<T>}
object.

\paragraph{Weights}

As said, a weights are manipulated through a sub-object of
\cppinline{Particle<T>}, \cppinline{WeightSet<T>}. In addition to common
weights manipulations, such as setting the weights directly or use the
incremental weights, it also provides ways to extract properties of the
weights. For example, it can calculate the \ess and \cess values.

For most applications, the default \cppinline{WeightSet<T>} is sufficient.
However, like the value collection type, there could be special requirement of
this class. It can be replaced by user defined classes through \cpp template
metaprogramming. The details are documented in the reference manual.

\paragraph{Monitor}

Given a real-valued function $h$, the library can use \cppinline{Monitor<T>}
objects to computes the importance sampling approximation of $\Exp[h(X)]$
automatically as the sampler progresses. The function value of $h$ is allowed
to be a vector. And it is possible to use optimized linear algebra library to
accelerate the computing in that case.

There are also special support for path sampling, which requires essentially a
simple importance sampling approximation and a numerical integration.

\subsection{Program structure}
\label{sub:Program structure}

Recall the \smc algorithms discussed in section~\ref{sec:Sequential Monte
  Carlo samplers}, regardless of specific applications or algorithm settings,
from an implementation perspective, they can be dissembled into the following
steps.
\begin{enumerate}
  \item Initialize values $\{X^{(i)}\}_{i=1}^N$ and weights
    $\{W^{(i)}\}_{i=1}^N$.
  \item For $t = 1,\dots,T$, where $T$ may not be finite (for example, a
    particle filter processing incoming data), repeat
    \begin{enumerate}
      \item Update either values $\{X^{(i)}\}_{i=1}^N$ or weights
        $\{W^{(i)}\}_{i=1}^N$ or both.
      \item Resampling.
      \item Update either values $\{X^{(i)}\}_{i=1}^N$ or weights
        $\{W^{(i)}\}_{i=1}^N$ or both.
    \end{enumerate}
\end{enumerate}
Note that steps 2.(a)-(c) are all optional, though it is unlikely that all
three of them are absent. For example, an \ais algorithm does not have the
resampling step. An \smc algorithm such as algorithm~\ref{alg:smc2} only
update the weights before the possible resampling and only update the values
after it while a particle filter might update both the values and weights at
step 2.(a). Both step 2.(a) and 2.(c) may be formed by a few sub-steps. For
example, the Markov kernel may be formed by a multi-block Metropolis random
walk.

In addition, after each iteration of step 2, we may be interested to evaluate
some importance sampling estimates. For example, the path sampling estimator,
as seen in section~\ref{sub:Path Sampling via smc2/smc3}, requires the
importance sampling estimates of $\diff\log q_{\alpha}(X)/\diff\alpha$ where
$q_{\alpha}$ is the unnormalized density function of the family of
distributions that the \smc sampler operates on. Another example is particle
filters, which often requires estimates of certain parameters at each
iteration.

For demonstration purpose, let's assume that our program has all those steps
and need to calculate both the path sampling and other importance sampling
estimates. In the \vsmc library, all these tasks are performed through the
\cppinline{Sampler} class. Below is an example of such a program,
\cppfile{snippet/program_structure.cpp}
We will explain each line of this program in detail. For now, it is sufficient
to point out that the following objects used in this program are user defined
callbacks that implement application specific operations.
\begin{description}
  \item[\cppinline{init_f}] Initialize the particle values.
    (section~\ref{sec:Initializing particles and parallelization})
  \item[\cppinline{move_f}] Update the particles. For example, updating the
    weights. These updates are performed before the possible resampling.
    (section~\ref{sec:Updating particles})
  \item[\cppinline{mcmc_f1} and \cppinline{mcmc_f2}] Update the particles. For
    example, moving the particles with an \mcmc kernel. These updates are
    performed after the possible resampling. (section~\ref{sec:Updating
      particles})
  \item[\cppinline{path_eval}] Evaluate the value of path sampling integrands,
    $\diff\log q_{\alpha}(X)/\diff\alpha$. (section~\ref{sec:Monitoring})
  \item[\cppinline{moments_eval}] Evaluate importance sampling estimate
    integrands, for example moments. (section~\ref{sec:Monitoring})
\end{description}

\section{The value collection, the particles and the sampler}
\label{sec:The value collection, the particles and the sampler}

At the core of each implementation of \smc algorithms using the \vsmc library
is the definition of the value collection type that abstracts
$\{X^{(i)}\}_{i=1}^N$. \vsmc does not restrict how the values shall be
actually stored. They can be stored in memory, spread among nodes of a
cluster, in \gpu memory or even in a database. Users can define their own
value collection type to fulfill various application specific needs. For full
details on the requirement of the value collection type, see \cite{vsmcjss}.

Given a value collection type \cppinline{T}, one can construct a sampler,
\begin{cppcode}
Sampler<T> sampler(N, Stratified, 0.5);
\end{cppcode}
The first argument is the number of particles. The second is the resampling
methods. There are six built-in resampling schemes in the library. And user
defined resampling algorithms can also be used. See the reference manual for
details. The last argument is the threshold of $\ess/N$ at each iteration,
below which a resampling will be performed. The later two parameters are
optional.

A \cppinline{Sampler<T>} object has a sub-object, \cppinline{Particle<T>},
which contains the type \cppinline{T} object along with other data such as
weights. Each can be access as the following,
\begin{cppcode}
Sampler<T> sampler(N);
sampler.particle();         // Reference to Particle<T> object
sampler.particle().value(); // Reference to type T object
\end{cppcode}

\subsection{A matrix of state values}
\label{sub:A matrix of state values}

Many typical problems' value collections can be viewed as a matrix of certain
type. For example, a simple particle filter whose state is a real-valued
vector of length $M$ can be viewed as an $N$ by $M$ matrix of type
\cppinline{double} where $N$ is the number of particles. A trans-dimensional
(e.g., \cite{Jasra:2008bb}) problem can use an $N$ by $1$ matrix whose type is
a user defined class, say \cppinline{StateType}. For this kind of problems,
\vsmc provide a class
template
\begin{cppcode}
template <MatrixOrder Order, std::size_t Dim, typename StateType>
class StateMatrix;
\end{cppcode}
The first template parameter (possible value \cppinline{RowMajor} or
\cppinline{ColMajor}) specifies how the values are ordered in memory. Usually
one shall choose \cppinline{RowMajor} to optimize data access. The second
template parameter is the number of variables, an integer value no less than
$1$ or the special value \cppinline{Dynamic}, in which case
\cppinline{StateMatrix} provides a member function \cppinline{resize_dim} such
that the number of variables can be changed at runtime. The third template
parameter is the type of the state values. Each particle's state is thus a
vector of length \cppinline{Dim}, indexed from \cppinline{0} to
\cppinline{Dim - 1}. To obtain the value at position \cppinline{j} of the
vector of particle \cppinline{i} (the element at the \cppinline{i}th raw and
\cppinline{j}th column of the matrix),
\begin{cppcode}
StateBase<RowMajor, Dim, StateType> value(N);
StateType val = value.state(i, j);
\end{cppcode}
There are other ways to obtain and manipulate the values, see the reference
manual for details. Note that, one can derive from the \cppinline{StateMatrix}
class to extend its functionality, as we will see in examples later.

\subsection{A single particle}
\label{sub:A single particle}

If the value collection type \cppinline{T} satisfies certain
requirements\footnote{See the reference manual for technique details. It is
  sufficient to note here that \cppinline{StateMatrix} and any of its derived
  classes satisfy those requirements.}, then for a \cppinline{Particle<T>}
object, one can construct a \cppinline{SingleParticle<T>} object that
abstracts one of the particle from the collection. For example,
\begin{cppcode}
Particle<T> particle(N);
SingleParticle<T> sp(i, &particle);
\end{cppcode}
create a \cppinline{SingleParticle<T>} object corresponding to the particle
\cppinline{i}. There are a few member functions of
\cppinline{SingleParticle<T>} that make access to individual particles easier
than through the interface of \cppinline{Particle<T>}. For example, for each
particle, a \cppinline{Particle<T>} object construct an indepent \cppoo{} \rng
engine. For example, the following uses it to generate standard Normal random
variates,
\begin{cppcode}
std::normal_distribution<double> rnorm(0, 1);
std::vector<double> z(particle.size());
for (std::size_t i = 0; i != particle.size(); ++i)
    z[i] = rnorm(particle.rng(i));
\end{cppcode}
If we access each particle through \cppinline{SingleParticle<T>}, then we can
write
\begin{cppcode}
z[i] = rnorm(sp.rng());
\end{cppcode}
Here \cppinline{sp.rng()} is equivalent to \cppinline{particle.rng(i)}.

The functionality of a \cppinline{SingleParticle<T>} can be enhanced through
template metaprogramming. For instance, if \cppinline{T} is
\cppinline{StateMatrix} or its derived class, then \cppinline{sp.state(j)} is
equivalent to \cppinline{particle.value().state(i, j)}

\subsection{Example: The value collection of \protect\gmm}
\label{sub:Example: The value collection of gmm}

The \cppinline{StateMatrix} is a minimalistic class template. Users can derive
from it and build application specific value collection classes. Here we
demonstrate how the value collection, named \cppinline{GMM}, in the \smc[2]
algorithm for the Gaussian mixture model (\gmm; see section~\ref{sub:Gaussian
  mixture model}) is designed.

Recall that, a \gmm with $r$ components has a parameter vector of length $3r$,
$\theta_r = (\mu_{1:r}, \lambda_{1:r}, \omega_{1:r})$. In the \smc[2]
algorithm, we use the sequence of distributions $\{\pi_t\}_{t=0}^T$ taking the
form,
\begin{equation*}
  \pi_t(\theta_t) =
  \pi_0(\theta_t|\calM) p(\data|\theta_t,\calM)^{\alpha(t/T)}.
\end{equation*}
The \cppinline{GMM} class will abstract the \gmm in addition to the state of
all particle values at any given generation. Therefore, we have the following
design goals for this class,
\begin{enumerate}
  \item The data, which is associated with the model shall be stored in and
    can be accessed through this class.
  \item The calculation of the likelihood and the prior densities, which are
    characteristics of the model shall be possible through this class.
  \item The distribution specification parameter $\alpha$ and the \mcmc
    proposal scales, which are properties of a given generation of the
    particle system shall be associated with this class.
\end{enumerate}
This class is outlined as below.
\cppfile{snippet/gmm_value.cpp}
First the number of components are set through a template parameter
\cppinline{R}. Of course, it is possible to make this parameter dynamic and
changeable at runtime by using \cppinline{Dynamic} for the second template
parameter of \cppinline{StateMatrix}.

Second, the static member functions \cppinline{mu_idx}, etc., returns the
index of the $\mu_j$, etc., in each row of the \cppinline{StateMatrix}. For
example, to access $\lambda_j$ of the $i$th particle, we can use
\begin{cppcode}
particle.value().state(i, GMM<R>::lambda_idx(j));
\end{cppcode}
or with the \cppinline{SingleParticle} interface
\begin{cppcode}
sp.state(GMM<R>::lambda_idx(j));
\end{cppcode}
instead of the much more difficult to read expression,
\begin{cppcode}
sp.state(GMM<R>::ComponentNumber * 2 + j);
\end{cppcode}

Third, the setter and getter member functions such as \cppinline{mu_scale}
provide access to the proposal scales. In addition, the member function
\cppinline{alpha} will provide access to $\alpha(t/T)$.

Fourth, the \cppinline{read_data} member function, whose definition is omitted
here, provides a way to read data into the \cppinline{data_} member data.

And last, the \cppinline{log_likelihood} and \cppinline{log_prior} member
functions calculate the log-likelihood and log-prior densities for a given
particle. They accept the particle's index number as input. The actual
implementations of these functions, distributed with the library, use more
sophisticated data structures to ensure that the computation only occurs when
the parameter values are changed. From a user's perspective, one only need to
know that these member functions will return the value of likelihood and prior
densities for the current particle values when called, while the actual
computation may or may not happen when the functions are called.

\section{Initializing particles and parallelization}
\label{sec:Initializing particles and parallelization}

The particles are initialized by a user defined callback. The callable object
has the following signature,
\cppfile{snippet/init_type.cpp}
It is added to the sampler by
\begin{cppcode}
sampler.init(init_f);
\end{cppcode}
And it will be called when the following in the program
(section~\ref{sub:Program structure}) is executed,
\begin{cppcode}
sampler.initialize(param);
\end{cppcode}
where the input parameter \cppinline{param} is optional and the default is
\cppinline{NULL}. It will be passed on as the second argument of
\cppinline{init_f} with \cppinline{sampler.particle()} being the first. The
return value of \cppinline{init_f} will be recorded as the acceptance count
and can be later retrieved by,
\begin{cppcode}
sampler.accept_history(0, 0);
\end{cppcode}
The optional parameter can be used to provide additional information needed to
initialize the sampler.

If the user does not do anything special, the sampler will also initialize the
weights $\{W^{(i)}\}_{i=1}^N$ to be equal and normalized to $1/N$. In
addition, any information recorded for previous generations of the particle
system will be erased during the initialization.

\subsection{Example: Simulation of Normal distribution}
\label{sub:Example: Simulation of Normal distribution}

We show here a very simple example, simulation of Normal random variables.
And we will introduce an important feature of the library through it --
parallelization.

Suppose for an \smc algorithm with a parameter vector of length $k$, we want to
initialize each of the parameter to $\calN(\mu,\sigma^2)$, where $\calN$
denotes the Normal distribution. We can implement it as the following,
\cppfile{snippet/init_normal.cpp}
In this example, we used the second parameter \cppinline{param} to pass
informations about the Normal distribution, its mean and standard deviation.
In the body of the program (the \cppinline{main} function), we can use it as
following,
\begin{cppcode}
sampler.init(init_f);

Param param = {Mean, Sd};
smapler.initialize(param);
\end{cppcode}

\subsection{Parallelized implementation}
\label{sub:Parallelized implementation}

In the above example, we looped over all particles. The inner loop is repeated
for each particle. There are no data dependencies among particles in this
operation. It is perfectly reasonable to have the outer loop parallelized.

This kind of parallelization, not only for initializing particles, but also
for updating particles, are supported in the \vsmc library through a set of
class templates. Here we introduce the ones specific to initialization,
\begin{cppcode}
template <typename T, typename D = Virtual> class InitializeSEQ;
template <typename T, typename D = Virtual> class InitializeOMP;
template <typename T, typename D = Virtual> class InitializeTBB;
\end{cppcode}
Each of the above three implement sequential, \openmp parallelization and \tbb
parallelization, respectively. There are a few other similar classes for other
parallel programming models not listed here. We first use
\cppinline{InitializeSEQ} as an example to demonstrate how it is used. The
interface of \cppinline{InitializeSEQ} given the second template parameter
being \cppinline{Virtual}, which is also shared by others, is,
\cppfile{snippet/initialize_seq_interface.cpp}
The existence of the \emph{non-virtual} interface \cppinline{operator()} and
the form of its signature ensures that an object of its derived class can be
used just as \cppinline{init_f}. It is implemented as if,
\cppfile{snippet/initialize_seq_operator.cpp}
Different class templates listed above differ at how they implement the loop.
For example, \cppinline{InitializeOMP} uses \openmp to parallelize this loop.

The user can derive from this class and use the virtual functions to provide
application specific behaviors of this operator. For example, the simulation
of Normal random variates can now be re-implemented as,
\cppfile{snippet/init_normal_seq.cpp}
At a first glance, it took quite a few more lines than the original
implementation of \cppinline{init_f}. However, by replace
\cppinline{InitializeSEQ} with \cppinline{InitializeOMP}, without changing
anything else, the sampler will be using \openmp for parallelization during
the initialization step.

\section{Updating particles}
\label{sec:Updating particles}

The addition of methods that update the particles is more flexible than
initialization. There are two kinds of updating methods. One is simply called
\cppinline{move} in \vsmc, and is performed before the possible resampling at
each iteration. These moves usually perform the updating of the weights among
other tasks. The other is called \cppinline{mcmc}, and is performed after the
possible resampling. They are often \mcmc type moves. Multiple
\cppinline{move}'s or \cppinline{mcmc}'s are also allowed. In fact a \vsmc
sampler consists of a queue of \cppinline{move}'s and a queue of
\cppinline{mcmc}'s.

All these are implemented using user defined callbacks similar to the
\cppinline{init_f} function in the last section, with a slight different
signature,
\cppfile{snippet/move_type.cpp}
This is the same for both \cppinline{move}'s and \cppinline{mcmc}'s. The first
argument is the iteration number, counting from zero for the initialization
step. The second is \cppinline{sampler.particle()}.

To add a \cppinline{move_f} into the queue of \cppinline{move}, call
\begin{cppcode}
sampler.move(move_f, false);
\end{cppcode}
The second argument, a boolean value, indicate whether the new
\cppinline{move} shall be appended to the existing (possible empty) queue (if
it is set to \cppinline{false}); or the queue shall be cleared before set a
new one. The queue of the \cppinline{mcmc}'s is manipulated similar.

\subsection{Example: Updating the weights in the \smc[2] algorithm}
\label{sub:Example: Updating the weights in the SMC2 algorithm}

Recall algorithm~\ref{alg:smc2}, the updating of weights shall be performed
before possible resampling at each iteration. And the change of the weights
are calculated with the incremental weights,
\begin{align*}
  W_t^{(i)} &\propto W_{t-1}^{(i)} w_t(X_{t-1}^{(i)}, X_t^{(i)}) \\
  w_t(X_{t-1}^{(i)}, X_t^{(i)}) &=
  p(\data|\theta_{t-1}^{(i)},\calM)^{\alpha(t/T) - \alpha([t-1]/T)}.
\end{align*}
This is quite generic for different applications. All we need here is the
calculation of the log-likelihood function. It is natural to write a function
template for it,
\cppfile{snippet/smc_move.cpp}
The assumption about the value collection type \cppinline{T} is,
\begin{enumerate}
  \item It provide access to $\alpha(t/T)$ in the same way as the
    \cppinline{GMM} class in section~\ref{sub:Example: The value collection of
      gmm}.
  \item It provide access to the log-likelihood in the same way as the
    \cppinline{GMM} class.
\end{enumerate}
The first part of the function template \cppinline{smc_move} calculates
$\alpha(t/T) - \alpha([t-1]/T)$. The second part calculates the logarithm of
the incremental weights for each particle. The last part manipulates the
weights.

Weights are manipulated through a object of type \cppinline{WeightSet}. There
are other ways to manipulate them,
\begin{cppcode}
std::vector<double> weight(particle.size());
particle.weight_set().set_equal_weight();
particle.weight_set().set_weight(weight.begin());
particle.weight_set().mul_weight(weight.begin());
particle.weight_set().set_log_weight(weight.begin());
particle.weight_set().add_log_weight(weight.begin());
\end{cppcode}
The \cppinline{set_equal_weight} member function sets all weights to be equal.
Similarly, the \cppinline{set_weight} and \cppinline{set_log_weight} member
functions set the values of weights and logarithm weights, respectively. And
the \cppinline{mul_weight} and \cppinline{add_log_weight} member functions
multiply the weights or add to the logarithm weights by the given values,
respectively. All these member functions accept general input iterators as
their arguments.

\subsection{Example: The \protect\mcmc move in \protect\gmm}
\label{sub:Example: The mcmc move in gmm}

In this example, we show an implementation of the \mcmc moves in the \gmm
example (section~\ref{sub:Gaussian mixture model}). We will only detail the
implementation of random walk block on $\mu_{1:r}$. The others are similar.
Recall that, we perform a Normal random walk on the mean parameters. The \mcmc
algorithm's implementation can be summaries as the following steps,
\begin{enumerate}
  \item Calculate the value of the target density for the parameter values,
    say $f$.
  \item Propose new values according to the proposal distribution. In our
    implementation, this proposal step are carried in place, meaning that the
    particle values are manipulated when new values are proposed.
  \item Calculate the value of the target density proposed parameter values,
    say $f'$.
  \item Generate uniform random variate on the $[0,1]$ interval, say $u$,
  \item Accept the proposed values if $u < f'/f$. Otherwise, restore the old
    values.
\end{enumerate}
The implementation of these five steps are straightforward,
\cppfile{snippet/gmm_mcmc_mu.cpp}
First, we derived our class from a class template called \cppinline{MoveOMP}.
It is similar to the \cppinline{InitializeSEQ} class template introduced in
section~\ref{sub:Parallelized implementation}. It provides \openmp
parallelization.

Second, we used a few new features of the \cppinline{SingleParticle} class
template. Recall that, it is created from a reference to a
\cppinline{Particle<T>} object and an index of the individual particle. The
\cppinline{Particle<T>} object can be obtained, by a constant reference,
through
\begin{cppcode}
sp.particle();
\end{cppcode}
and the index can be obtained through
\begin{cppcode}
sp.id();
\end{cppcode}
These are used in the calculation of the log-likelihood and log-prior
densities.

Otherwise, the implementation is a straightforward translation of the
mathematical representation of the algorithm. The whole algorithm has three
blocks of random walks. Say we implemented the other two similarly as
\cppinline{GMM_MCMC_Lamba} and \cppinline{GMM_MCMCM_Omega}, then in the body
of the program we can add them to the sampler by,
\cppfile{snippet/gmm_add_mcmc.cpp}
Note that the \cppinline{mcmc} member function call return a reference the
sampler itself. Therefore we can chain these calls. When we call,
\begin{cppcode}
sampler.iterate(IterNum);
\end{cppcode}
the sampler will iterate \cppinline{IterNum} steps and at each step, all three
of these random walks will be applied to the particle system.

\section{Monitoring}
\label{sec:Monitoring}

Before initializing the sampler or after a certain time point, one can add
monitors to the sampler. The concept is similar to \bugs's \cppinline{monitor}
statement, except it does not monitor the individual values but rather the
importance sampling estimates. Consider approximating $\Exp[h(X)]$, where
$h(X) = (h_1(X),\dots,h_m(X))$ is an $m$-vector function. The importance
sampling estimate can be obtained by $AW$ where $A$ is an $N$ by $m$ matrix
where $A(i,j) = h_j(X^{(i)})$ and $W = (W^{(i)},\dots,W^{(N)})^T$ is the
$N$-vector of normalized weights. To compute this importance sampling
estimate, one need to define the following evaluation function (or other kinds
of callable objects),
\cppfile{snippet/monitor_eval.cpp}
and add it to the sampler by calling,
\begin{cppcode}
sampler.monitor("variable.name", m, monitor_eval);
\end{cppcode}
When the function \cppinline{monitor_eval} is called, \cppinline{iter} is the
iteration number of the sampler, \cppinline{m} is the same value as the one
the user passed to \cppinline{Sampler<T>::monitor}; and thus one does not need
global variable or other similar techniques to access this value. The output
pointer \cppinline{res} points to an $N \times m$ output array of row major
order. That is, after the calling of the function,
\cppinline{res[i * dim + j]} shall be $h_j(X^{(i)})$.

Implementation of the path sampling estimator (section~\ref{sub:Path Sampling
  via smc2/smc3}) can be viewed as a special kind of monitor. In addition to
the evaluation of $h(X^{(i)})$, where $h$ is $\diff\log
q_{\alpha}(X)/\diff\alpha$ in this special case, the interval length of the
numerical integration as in equation~\eqref{eq:path_est} also need to be
obtained. The \vsmc library provide a special support for path sampling. First
one need to define a function, say \cppinline{path_eval} with the following
signature,
\cppfile{snippet/path_eval.cpp}
It is not unlike the \cppinline{monitor_eval} function above. It only differs
by,
\begin{enumerate}
  \item The output array \cppinline{res} is always of length $N$, and thus
    there is no argument to pass the dimension of the monitor
  \item It returns a value, which should be $\alpha_t$.
\end{enumerate}
To use it, one can add it to the sampler by,
\begin{cppcode}
sampler.path_sampling(path_eval);
\end{cppcode}
And the estimate can be obtained by
\begin{cppcode}
sampler.path_sampling();
\end{cppcode}

\subsection{Example: Path sampling with the \smc[2] algorithm}
\label{sub:Example: Path sampling with the SMC2 algorithm}

In the case of algorithm~\ref{alg:smc2}, the path sampling integrands is
simply the log-likelihood, and $\alpha_t = \alpha(t/T)$. Therefore the
implementation of a generic \cppinline{path_eval} is straightforward. Again,
we assume that value collection type $T$ provides access to $\alpha(t/T)$ and
the log-likelihood in the same way as the \cppinline{GMM} class. We can
implement the function template as the following,
\cppfile{snippet/path_eval_f.cpp}

Other \smc algorithms such algorithm~\ref{alg:smc3} may have different path
sampling estimator expression. But the implementation is similar.

\subsection{Example: Adaptive specification of proposal scales}
\label{sub:Example: Adaptive specification of proposal scales}

Now we demonstrate the implementation of a slightly more complex monitor and
use it for the adaptive specification of proposal scales. Consider the \gmm
example, as outlined in section~\ref{sub:Adaptive specification of proposals},
we can use the moments of the parameters to set the proposal scales
adaptively.

First, we need to create a monitor that record the first two raw moments of
each parameter. We can make this problem more general as estimating the first
$M$ raw moments. We use a parallelized monitor, \cppinline{MonitorEvalOMP} for
the implementation of the evaluation function. It is not unlike the
\cppinline{MoveOMP} class template introduced earlier. The main difference is
that now we need to define the following member function,
\begin{cppcode}
void monitor_state(std::size_t, std::size_t,
        ConstSingleParticle<GMM<R> > csp, double *res)
\end{cppcode}
where the first argument is the iteration number and the second is the
dimension of the monitor. The third, a \cppinline{ConstSingleParticle} object,
is similar to \cppinline{SingleParticle} except that now one does not have
write access to the particles. In other words, one cannot change the particle
values through it. The last, the output parameter \cppinline{res} is of length
the dimension of the monitor. The function call need only to store the values
of $h(X^{(i)})$ for a single particle. The complete implementation is given
as below,
\cppfile{snippet/gmm_moments.cpp}
In addition to the \cppinline{monitor_state} member function, we also provide
a few utilities in this class. First, similar to the \cppinline{GMM} class, we
use functions to give the index of a given order of moment for a specific
parameter of a certain components inside the output parameter \cppinline{res}.
This makes the implementation more readable. Second, we also provide
functions, whose definitions are trivial and not shown here, that calculate
the proposal scales for each random walk given an array of moments estimates.
We can add this monitor to the sampler by,
\cppfile{snippet/gmm_moments_add.cpp}
In the above, we choose only to estimate the first two raw moments.

Now we only need a method to set the proposal scales using this monitor. Since
this should be set before the updating of weights and possible resampling, we
can construct a \cppinline{move} for this job.
\cppfile{snippet/gmm_adaptive_scale.cpp}
And in the \cppinline{main} function, we change the move queue to
\begin{cppcode}
sampler.move(GMM_AdaptiveScale(&sampler), false);
sampler.move(smc_move<GMM<R> >, true);
\end{cppcode}
Note that, we initialize the move with a pointer of the sampler itself, and
use this pointer to access the record in the monitor named
\cppinline{"moments"}. There are many ways to retrieve the importance sampling
estimates from a monitor. The one we used here
\begin{cppcode}
res[i] = sampler_ptr_->monitor("moments").record(i);
\end{cppcode}
return the importance sampling estimate of $\Exp[h_j(X)]$ for the latest
generation of the particle system.

\section{Performance of parallelization}
\label{sec:Performance of parallelization}

One of the main motivation behind the creation of \vsmc is to ease the
parallelization with different programming models. The same implementation can
be used to built different samplers based on what kind of parallel programming
model is supported on the users' platforms. In this section we compare the
performance of various \smp parallel programming models and \opencl
parallelization. We use the \gmm with \smc[2] algorithm as shown in
section~\ref{sub:Gaussian mixture model}. Many major parts of its
implementation have been shown through this chapter. For a complete
documentation on its implementation with \vsmc, see \cite{vsmcjss}.

\subsection{Using the \protect\smp module}
\label{sub:Using the SMP module}

We consider five different implementations supported by \icpc~2013:
sequential, \tbb, \cilk, \openmp and \cppoo{} \cppinline{<thread>}. The
program is built and run on a Ubuntu~12.10 workstation with an Xeon~W3550
(3.06GHz, 4 cores, 8 hardware threads through hyper-threading) \cpu. A four
components model and $100$ iterations with a prior annealing scheme is used
for all implementations. A range of numbers of particles are tested, from
$2^3$ to $2^{17}$.

For different number of particles, the wall clock time and speedup are shown
in Figure~\ref{fig:bench-smp-perf}. For $10^4$ or more particles, the
differences are minimal among all the programming models. They all have
roughly 550\% speedup. With smaller number of particles, \vsmc's \cppoo
parallelization is less efficient than other industry strength programming
models. However, with $1000$ or more particles, which is less than typical
applications, the difference is not very significant.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{fig/bench-smp-time-running}
  \includegraphics[width=\linewidth]{fig/bench-smp-speedup-running}
  \caption{Performance of \cpp implementations of Bayesian modeling for
    Gaussian mixture model (Linux; Xeon W3550, 3.06GHz, 4 cores, 8 threads).}
  \label{fig:bench-smp-perf}
\end{figure}

\subsection{Using the \protect\opencl module}
\label{sub:Using the OpenCL module}

The implementation of the same algorithm using \opencl is quite similar to
those using the \smp module.

\opencl implementations are also compared on the same workstation, which also
has an NVIDIA Quadro 2000 graphic card. \opencl programs can be compiled to
run on both \cpu and \gpu. For \cpu implementation, there are \iocl
\cite{iocl} and \aocl \cite{aocl} platforms. We use the \tbb implementation as
a baseline for comparison. The same \opencl implementation are used for all
the \cpu and \gpu runtimes.  Therefore they are not particularly optimized for
any of them. For the \gpu implementation, in addition to double precision, we
also tested a single precision configuration. Unlike modern \cpu, which have
the same performance for double and single precision floating point operations
(unless \simd instructions are used, which can have at most a speedup by a
factor of 2), \gpu penalize double precision performance heavily.

For different number of particles, the wall clock time and speed up are
plotted in Figure~\ref{fig:bench-ocl-perf}. With smaller number of particles,
the \opencl implementations have a high overhead when compared to the \tbb
implementation. With a large number of particles, \aocl has a similar
performance as the \tbb implementation. \iocl is about 40\% faster than the
\tbb implementation. This is due to more efficient vectorization and compiler
optimizations. The double precision performance of the NVIDIA \gpu has a 220\%
speedup and the single precision performance has near 1600\% speedup. As a
rough reference for the expected performance gain, the \cpu has a theoretical
peak performance of 24.48 GFLOPS. The \gpu has a theoretical peak performance
of 60 GFLOPS in double precision and 480 GFLOPS in single precision. This
represents 245\% and 1960\% speedup compared to the \cpu, respectively.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{fig/bench-ocl-time-running}
  \includegraphics[width=\linewidth]{fig/bench-ocl-speedup-running}
  \caption{Performance of \opencl implementations of Bayesian modeling for
    Gaussian mixture model (Linux; Xeon W3550 \gpu, 3.06GHz, 4 cores, 8
    threads; NVIDIA Quadro 2000).}
  \label{fig:bench-ocl-perf}
\end{figure}

\subsection{Programming efforts comparison}
\label{sub:Programming efforts comparison}



It is widely believed that \opencl programming is tedious and hard. However,
\vsmc provides facilities to manage \opencl platforms and devices as well as
common operations. Limited by the scope of this paper, the \opencl
implementation (distributed with the \vsmc source) is not documented in this
paper. Overall the \opencl implementation has about 800 lines including both
host and device code. It is not an enormous increase in effort when compared
to the 500 lines \smp implementation. Less than doubling the code base but
gaining more than 15 times performance speedup, we consider the programming
effort is relatively small.

\section{Appendix}
\label{sec:vSMC Appendix}

\subsection{Classes of parallel computers}
\label{sub:Classes of parallel computers}

There are a few types of parallel computers. Here we introduce the four types
of hardware parallelism that are most commonly seen. Parallel computers can be
nested. In a multicore \cpu, each core can perform instruction level
parallelism. On the other hand, a distributed system can be formed by multiple
multicore \cpu{}s.

\subsubsection{Instruction level}
\label{ssub:Instruction level}

Modern \cpu{}s all implement the so called \simd instructions, short for
\emph{single instruction, multiple data}. The \cpu can execute a single
instruction on different data in a single cycle. However, unlike the higher
level parallelism discussed later, \simd often has strict requirement on the
arrangement of the data. In addition, the implementation often requires using
low level assembly language or intrinsics functions.

Though \vsmc does not directly implement this level of parallelism, it can be
used by the user nonetheless. In addition, many operations within \vsmc can be
performed using libraries that are implemented with \simd parallelization,
such as \mkl. Also note that, most modern \cpp compilers perform \simd
optimizations on simple loop and some of them, such as \clang \cite{clang}
performs \simd optimizations for non-loop structures. This kind of
optimization is also called \emph{vectorization}.

\subsubsection{Multicore processors and symmetric multiprocessing}
\label{ssub:Multicore processors and symmetric multiprocessing}

In the late 1990s, computer \cpu{}s are advanced by increasing the clock
speed. However, this strategy soon hit some bottlenecks, mainly the control of
heat and power. The industry started to develop multicore processors. Each
\cpu has several cores, each running at a modest clock rate. By executing
different threads on different cores, the \cpu can process the same amount of
work with less time without increasing the clock rate.

When a computer has multiple \cpu{}s and each of them has the same speed to
access the memory, the system is often called \emph{symmetric multiprocessing}
(\smp). Most higher end workstations are \smp systems. The programming tools
are usually the same for \smp and multicore processors.

The \vsmc library support various \smp programming models. In addition, \vsmc
allows the same user implementation source code to be compiled into different
parallel samplers using different programming models.

\subsubsection{Distributed computing}
\label{ssub:Distributed computing}

Distributed computing usually refers to the form of computing where both
memory and computing processors are spread among computing nodes. It can take
different forms, such as grids and clusters. The \emph{de facto} programming
model for distributed computing is \mpi. This is also supported by \vsmc. In
addition, the library also allows easy integration of \mpi and various \smp
programming models.

\subsubsection{Massive parallel computing}
\label{ssub:Massive parallel computing}

In recent years, there is a new trend of using specialized massive parallel
devices, such as \gpu{}s for scientific computing. Modern \gpu{}s often have
hundreds or thousands co-processors. The main difference between \gpu and \cpu
is that, \cpu has more logic control units, and thus is more suited for
general programs. In contrast, \gpu are better at applying the same arithmetic
operations on a collection of data. It performs the best if each computing
unit are executing \emph{exactly the same} instructions. In addition, it is
often much more efficient if there are a large amount data to be processed.
Another significant feature of these devices is that they provide much higher
\emph{local} memory bandwidth and can use various technologies to reduce local
data latency than traditional \cpu.

Massive parallel computing is extremely suitable for the \smc algorithms,
which can have a large number of particles, while each of then need to be
updated using the same \mcmc kernel.

There are two major programming models for general purpose \gpu programming
(\gpgpu), Nvidia's \cuda framework and the \opencl \cite{opencl} standard. The
\vsmc library provides direct support for the \opencl programming model.

\subsection{Modern C++}
\label{sub:Modern C++}

The \cpp programming language \cite{cpp03} was first created to support
object-oriented programming (\oop) on top of the C programming language
\cite{evolutioncpp}.  The features, such as templates, come to the language
fairly late. However, it was found that the \cpp template feature provides a
complete sub-language \cite{cpptemplateturing}. This leads to various new
metaprogramming techniques. Many of them are documented in \cite{moderncpp}
and characterize the modern usage of \cpp. In this section, we introduce two
of these techniques. They are widely used inside the \vsmc library and the
contents here shall ease the reading of the following sections for those less
familiar with them. However, we assume the reader has at least some working
knowledge of \cpp, including concepts such as \oop.

\subsubsection{Templates}
\label{ssub:Templates}

\cpp is a static strong type language. It requires the user to declare
variables, functions, and most other kinds of entities using specific types.
However, a lot of code looks the same for different types, especially for
implementation of algorithms. The \cpp template technique allows one to write
generic code to solve a class of problems, while the involved types can be
seamlessly replaced at compile time.

Templates are useful for a few reasons. It reduces duplication of the same
code for multiple types. Though conventional \oop also supports polymorphism
behaviors, they rely on runtime decisions. In contrast, templates rely on
compile time decisions and are more type safe. Templates emphasize that the
same operation can be applied to many types. It allows unlimited extension of
existing functionality. It is possible to have any combination of the allowed
operations on a certain type and the allowed types of a certain operations.
More specifically, given a collection of operations and a collection of types,
each operation may support any subset of the types and each type can support
any subset of the operations. Such combinatorial behavior is difficult to
implement using conventional \oop, where a collection of types have a common
interface that provides a fixed set of operations.

There are two main types of templates in \cpp, function template and class
template.

\paragraph{Function template}

The following lines define a simple function template,
\cppfile{snippet/function_template_def.cpp}
This template definition specifies a \emph{family} of functions that returns
the maximum of two values, which are passed as function parameters
\cppinline{a} and \cppinline{b}. The type of these parameters is left open as
\emph{template parameter} \cppinline{T}.

In this template definition, the assumption about \cppinline{T} is that the
operator \cppinline{<} is properly defined. It does not matter whether
\cppinline{T} is a fundamental type or a class type with this operator defined
by the user. The actual types are deduced at compile time when the function
template is used.

\paragraph{Class template}

Class template is similar to function template, yet somehow simpler. A class
template define a family of classes. For example,
\cppfile{snippet/class_template_def.cpp}
defines a \cppinline{Stack} class template. For simplicity, some edge cases
and exceptional situations such as calling \cppinline{top} on an empty
\cppinline{Stack} is not handled here. This class template can be used as the
following,
\cppfile{snippet/class_template_usage.cpp}
As we can see, to use a class template, one \emph{explicitly} supply the type
of the template parameter. Unlike function template, there is no template
parameter deduction here.

\subsubsection{Callable objects}
\label{ssub:Callable objects}

\vsmc is a framework for constructing generic \smc samplers. It relies on the
user to write callback functions to perform application specific operations,
such as updating particles. In this section, we introduce the few forms of
callbacks that are supported by the library. Collectively, they are also
called \emph{callable objects}, meaning that they support the function calling
syntax though they may not be functions.

A callable object, say \cppinline{callable}, is similar to a function in the
sense that it has a return type and a parameter list as its signature. It can
be used with the syntax,
\begin{cppcode}
callable( /* arguments */ );
\end{cppcode}
However, the object may or may not be a function. There are three ways to
define a callable objects, function pointer, functor and \cppoo lambda
expression. Function pointer is the main way of passing callbacks in C. The
other two are introduced later.

The library also use type erasures, introduced later in this section, to
enforce certain interfaces. The benefits of techniques introduced below
increases the productivity and flexibility of the library compared to
conventional techniques of passing callbacks through function pointer.

\paragraph{Functors}

Consider the simple problem, sort a vector $\{x_i\}_{i=1}^N$ according to the
values $y_i = f(x_i)$. We may define a function template to solve this
problem,
\cppfile{snippet/sort_f.cpp}
The function template \cppinline{sort_f} expects input and output as pointers.
In addition, it expects a callable object \cppinline{f}, which accepts a
variable of type \cppinline{double} as its input and return a number that can
be assigned to a variable of type \cppinline{double}.

One way to define such a callable object is to use \emph{functor}, a class
type with \cppinline{operator()} properly defined. For example,
\begin{cppcode}
struct F
{
    double operator() (double x) const { return x * x; }
};
sort_f(input, output, F());
\end{cppcode}
Here we created this object in the function call of \cppinline{sort_f}. It can
also be used as,
\begin{cppcode}
F f;
double y = f(3); // y <- 9
\end{cppcode}

\paragraph{Lambda expressions}

Another way, introduced in \cppoo, is a new feature called \emph{lambda
  expression}. It is also called \emph{local function} or \emph{closure} in
other programming languages. It allows us to define callable object \emph{on
  site}. Here is an example,
\begin{cppcode}
sort_f(input, output, [] (double x) { return x * x; });
\end{cppcode}
The full declaration of a lambda expression is as the following,
\begin{cppcode}
[ /* capture */ ] ( /* parameters */ ) /* mutable */
/* exception specification */
/* attribute specification */
-> /* return type */ { /* body * };
\end{cppcode}
The \cppinline{/* mutable */} part can be either empty or the keyword
\cppinline{mutable}, which allows the body to modify captured parameters
(explained soon). The exception specification is similar to a normal function
and the attribute specification is a new feature for all functions in \cppoo,
that specifies things like parameter passing conventions among other things,
which we will not go into details. The part \cppinline{-> /* return type */}
specifies the return type of the lambda expression. If omitted, it is deduced
from the body. And if the body does contain any \cppinline{return} statement,
it is deduced to be \cppinline{void}. If the expression takes no arguments,
the parameter list can also be omitted.

The \cppinline{/* capture */} specifies which symbols visible at the scope of
the definition of the lambda expression will be visible inside the body. There
are a few forms,
\begin{description}
  \item[\cppinline{[a, &b]}] captures \cppinline{a} by value and \cppinline{b}
    by reference.
  \item[\cppinline{[this]}] captures the \cppinline{this} pointer by value.
  \item[\cppinline{[=]}] captures all automatic variables \emph{used} in the
    body by value.
  \item[\cppinline{[&]}] captures all automatic variables \emph{used} in the
    body by reference.
  \item[\cppinline{[]}] captures nothing.
\end{description}

\paragraph{Type erasures}

In the example above, the function template \cppinline{sort_f} does not
actually enforce the signature of the function or functor, in contrast to the
definition of it that takes a function pointer as an argument. For example,
the following is perfect valid \cpp,
\begin{cppcode}
int h (int x) { return x * x };
sort_f(input, output, &h);
\end{cppcode}
while it may not be what one wants. The use of \cppinline{h} with
\cppinline{sort_f} is perhaps an typo. The type erasure in \cppoo{},
\cppinline{std::function}, provides a solution to this problem. A \emph{type
  erasure} can convert various types of objects into a single type. Below is a
basic usage of \cppinline{std::function},
\begin{cppcode}
#include <functional> // the header that defines std::function

std::function<double (double)> f;
F f_obj;
f = f_obj;  // Correct
f = &h;     // ERROR: h does not has the required signature.
\end{cppcode}
Now we can redefine the function \cppinline{sort_f} as,
\cppfile{snippet/sort_f_erasure.cpp}
The \vsmc library makes extensive use of the type erasure to enforce certain
callback interfaces. When \cppoo features are not available, the \boost
library provides the same functionality through \cppinline{boost::function}.
See \cite{vsmcjss} for details of how \vsmc choose between \cppoo and \boost
libraries.
